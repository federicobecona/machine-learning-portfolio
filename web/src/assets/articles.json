[
    {
        "unidad":"Preparación de datos",
        "unit":"Data preparation",
        "titulo":"Normalización sobre dataset Wine en RapidMiner",
        "title":"Normalization over Wine dataset in RapidMiner",
        "descripcion":"En RapidMiner se realizó una normalización sobre el dataset Wine y se comparó la performance de aplicar Naive Bayes usando el dataset normalizado y no normalizado con un split con el 70% de ejemplos para training y 30% para test en ambos casos.",
        "description":"In RapidMiner a normalization was performed on the Wine dataset and the performance of applying Naive Bayes was compared using the normalized and non-normalized dataset with a split with 70% of examples for training and 30% for testing in both cases .",
        "contenido":"p>>- Se creó un proceso en RapidMiner con dos canales, uno que utiliza el dataset normalizado con transforamción Z y uno que no.=,=p>>- En ambos canales se realizó un split con 70% de datos para entrenamiento y 30% para test. =,=p>>- Para cada caso se entrenó un modelo de Naive Bayes para clasificación por lo cual hubo que convertir el atriburto Wine de numérico a polinómico.=,=p>>- Se le midió performance del modelo.=,=br=,=h3>>Resultados=,=img>>UT2-PD1-3.jpg>>100%>>400=,=br=,=a>>Descargar modelo>>UT2 PD1 ej2.rmp>>d=,=br=,=br=,=p>>Caso normalizado=,=img>>UT2-PD1-4.jpg>>100%>>200=,=br=,=br=,=p>>Caso sin normalizar=,=img>>UT2-PD1-5.jpg>>100%>>200=,=br=,=br=,=p>>Se obtuvo un apenas poco de mayor presición para el caso del dataset normalizado lo cual se debe al algoritmo utilizado. Naive Bayes calcula frecuencias para hallar las clases de predicción y al estandarizar los datos, las frecuencias se mantienen de una forma similar.",
        "content":"p>>- A process was created in RapidMiner with two channels, one that uses the normalized dataset with Z transformation and one that does not.=,=p>>- A split was performed on both channels with 70% of data for training and 30% for tests. =,=p>>- For each case, a Naive Bayes model was trained for classification, for which the Wine attribute had to be converted from numeric to polynomial.=,=p>>- The performance of the model was measured.=,= br=,=h3>>Results=,=img>>UT2-PD1-3.jpg>>100%>>400=,=br=,=a>>Download model>>UT2 PD1 ej2.rmp>>d =,=br=,=br=,=p>>Normalized case=,=img>>UT2-PD1-4.jpg>>100%>>200=,=br=,=br=,=p>> Non-normalized case=,=img>>UT2-PD1-5.jpg>>100%>>200=,=br=,=br=,=p>>A little better precision was obtained for the dataset case normalized which is due to the algorithm used. Naive Bayes computes frequencies to find the prediction classes, and by standardizing the data, the frequencies remain similar."
    },
    {   
        "unidad":"Preparación de datos",
        "unit":"Data preparation",
        "titulo":"Tutoriales de preparación de datos en RapidMiner",
        "title":"Data preparation tutorials in RapidMiner",
        "descripcion":"En este ejercicio se hicieron tutoriales de missing values, normalización, detección de outliers, modelado, scoring, split y validación, cross validation y visual model comparison para la herramienta RapidMiner.",
        "description":"In this exercise, modeling, scoring, split and validation, cross validation and visual model comparison tutorials were made for the RapidMiner tool.",
        "contenido":"h3>> Tutorial Handling Missing Values =,=p>>=,=img>>UT2-PD1-1.jpg>>100%>>200=,=br=,=a>>Descargar archivo>>Tutorial Handling Missing Values.rmp>>d =,=br=,=br=,=h3>> Tutorial Normalization and Outlier detection =,=img>>UT2-PD1-2.jpg>>100%>>150=,=br=,=a>>Descargar archivo>>Tutorial Normalization and Outlier detection.rmp>>d=,=br=,=br=,=h3>> Modeling =,=img>>UT2-PD2-1.jpg>>100%>>300 =,=br=,=a>>Descargar archivo>>Tutorial Modeling.rmp>>d=,=br=,=br=,=br=,=h3>> Scoring =,=img>>UT2-PD2-2.jpg>>100%>>250 =,=br=,=a>>Descargar archivo>>Tutorial Scoring.rmp>>d=,=br=,=br=,=br=,=h3>> Test Splits and Validation =,=img>>UT2-PD2-3.jpg>>100%>>175 =,=br=,=a>>Descargar archivo>>Tutorial Test Splits and Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Cross Validation =,=img>>UT2-PD2-4-1.jpg>>100%>>200=,=br=,=br=,=img>>UT2-PD2-4-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>> Tutorial Cross Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Visual Model Comparison =,=img>>UT2-PD2-5-1.jpg>>100%>>125=,=br=,=br=,=img>>UT2-PD2-5-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>>Tutorial Visual Model Comparison.rmp>>d",
        "content":"h3>> Tutorial Handling Missing Values =,=p>>=,=img>>UT2-PD1-1.jpg>>100%>>200=,=br=,=a>>Descargar archivo>>Tutorial Handling Missing Values.rmp>>d =,=br=,=br=,=h3>> Tutorial Normalization and Outlier detection =,=img>>UT2-PD1-2.jpg>>100%>>150=,=br=,=a>>Descargar archivo>>Tutorial Normalization and Outlier detection.rmp>>d=,=br=,=br=,=h3>> Modeling =,=img>>UT2-PD2-1.jpg>>100%>>300 =,=br=,=a>>Descargar archivo>>Tutorial Modeling.rmp>>d=,=br=,=br=,=br=,=h3>> Scoring =,=img>>UT2-PD2-2.jpg>>100%>>250 =,=br=,=a>>Descargar archivo>>Tutorial Scoring.rmp>>d=,=br=,=br=,=br=,=h3>> Test Splits and Validation =,=img>>UT2-PD2-3.jpg>>100%>>175 =,=br=,=a>>Descargar archivo>>Tutorial Test Splits and Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Cross Validation =,=img>>UT2-PD2-4-1.jpg>>100%>>200=,=br=,=br=,=img>>UT2-PD2-4-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>> Tutorial Cross Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Visual Model Comparison =,=img>>UT2-PD2-5-1.jpg>>100%>>125=,=br=,=br=,=img>>UT2-PD2-5-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>>Tutorial Visual Model Comparison.rmp>>d"
    },
    {
        "unidad":"Preparación de datos",
        "unit":"Data preparation",
        "titulo":"Pre procesamiento de datos en Wine con Python",
        "title":"Data pre-processing in Wine with Python",
        "descripcion":"Para este caso se aplican técnicas de pre procesamiento como normalización, estandarización y split en datos de training y test. Luego se obtienen estadísticas para el dataset Wine utiliando funciones de Python con SciKitLearn y la librería Pandas.",
        "description":"For this case, pre-processing techniques such as normalization, standardization and split are applied to training and test data. Statistics for the Wine dataset are then obtained using Python functions with SciKitLearn and the Pandas library.",
        "contenido":"p>>- Se descargó el dataset Wine.=,=p>>- Se imprimieron las columnas de las primeras 10 filas.=,=p>>- Se convirtieron los valores numéricos de string a float.=,=p>>- Se obtuvieron los valores mínimos y máximos para cada columna.=,=p>>- Se obtuvo la media y la desviación estándar de los valores de cada columna.=,=p>>- Se normalizaron y se estandarizaron los valores del dataset original.=,=p>>- Se dividió el dataset en conjuntos de entrenamiento y testing.=,=br=,=h3>>Código=,=code>>.from csv import reader=,=br=,=code>>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy=,=br=,=code>>.=,=br=,=code>>.columns = ['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\u00a0    dataset = list()=,=br=,=code>>.\u00a0    with open(filename, 'r') as file:=,=br=,=code>>.\u00a0\u00a0        csv_reader = reader(file)=,=br=,=code>>.\u00a0\u00a0        for row in csv_reader:=,=br=,=code>>.\u00a0\u00a0\u00a0            if not row:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0                continue=,=br=,=code>>.\u00a0\u00a0\u00a0            dataset.append(row)=,=br=,=code>>.\u00a0\u00a0        return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        row[column] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\u00a0    minmax = list()=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        value_min = min(col_values)=,=br=,=code>>.\u00a0\u00a0        value_max = max(col_values)=,=br=,=code>>.\u00a0\u00a0        minmax.append([value_min, value_max])=,=br=,=code>>.\u00a0    return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\u00a0    means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        means[i] = sum(col_values) / float(len(dataset))=,=br=,=code>>.\u00a0    return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means):=,=br=,=code>>.\u00a0    stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\u00a0\u00a0        stdevs[i] = sum(variance)=,=br=,=code>>.\u00a0\u00a0        stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]=,=br=,=code>>.\u00a0    return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])=,=br=,=code>>.=,=br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\u00a0    train = list()=,=br=,=code>>.\u00a0    train_size = split * len(dataset)=,=br=,=code>>.\u00a0    dataset_copy = list(dataset)=,=br=,=code>>.\u00a0    while len(train) < train_size:=,=br=,=code>>.\u00a0\u00a0        index = randrange(len(dataset_copy))=,=br=,=code>>.\u00a0\u00a0        train.append(dataset_copy.pop(index))=,=br=,=code>>.\u00a0    return train, dataset_copy=,=br=,=code>>.=,=br=,=code>>.input_file = \"wine.csv\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print(\"\n- Primeras 10 filas:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(dataset[i])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(columns)+1):=,=br=,=code>>.\u00a0    str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset con floats:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(strToFloatDataset[i])=,=br=,=code>>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\"\n- Estadísticas: \")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>>.\u00a0    print(\"*\"+columns[i] + \": min \" + str(minmax[i][0]) + \",  max \" + str(minmax[i][1]) + \", media \" + str(means[i]) + \", desviación estándar \" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.standarized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standarized_dataset, means, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset estandarizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(standarized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset normalizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(normalized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(train[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.\u00a0for i in range(10):    =,=br=,=code>>.\u00a0    print(test[i])=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD3-1.jpg>>100%>>300=,=img>>UT2-PD3-2.jpg>>100%>>300=,=img>>UT2-PD3-3.jpg>>100%>>300=,=img>>UT2-PD3-4.jpg>>100%>>600=,=img>>UT2-PD3-5.jpg>>100%>>600",
        "content":"p>>- The Wine dataset was downloaded.=,=p>>- The columns of the first 10 rows were printed.=,=p>>- The numeric values ​​were converted from string to float.=,=p>> - The minimum and maximum values ​​were obtained for each column.=,=p>>- The mean and standard deviation of the values ​​of each column were obtained.=,=p>>- The dataset values ​​were normalized and standardized original.=,=p>>- The dataset was divided into training and testing sets.=,=br=,=h3>>Code=,=code>>.from csv import reader=,=br=,=code >>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy =,=br=,=code>>.=,=br=,=code>>.columns = ['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\u00a0 dataset = list()=,=br=,=code>>.\u00a0 with open(filename, 'r') as file:=,=br=,=code>>.\u00a0\u00a0 csv_reader = reader(file)=,=br=,=code>>.\u00a0\u00a0 for row in csv_reader:=,=br=,=code>>.\u00a0\u00a0\u00a0 if not row:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 continue=,=br=,=code>>.\u00a0\u00a0\u00a0 dataset.append(row)=,=br=,=code>>.\u00a0\u00a0 return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\u00a0 for row in dataset:=,=br=,=code>>.\u00a0\u00a0 row[column ] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\u00a0 minmax = list()=,=br=,=code>>.\u00a0 for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0 col_values ​​= [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0 value_min = min(col_values)=,=br=,=code>>.\u00a0\u00a0 value_max = max(col_values)=,=br=,=code>>.\u00a0\u00a0 minmax.append([value_min, value_max])=,=br=,=code>>.\u00a0 return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\u00a0 means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0 for i in range(len(dataset[0])):=,=br =,=code>>.\u00a0\u00a0 col_values ​​= [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0 means[i] = sum(col_values) / float( len(dataset))=,=br=,=code>>.\u00a0 return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means): =,=br=,=code>>.\u00a0 stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0 for i in range(len (dataset[0])):=,=br=,=code>>.\u00a0\u00a0 variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\u00a0\u00a0 stdevs[i] = sum(variance)=,=br=,=code>>.\u00a0\u00a0 stdevs = [sqrt(x/(float(len(dataset)-1 ))) for x in stdevs]=,=br=,=code>>.\u00a0 return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\u00a0 for row in dataset:=,=br=,=code>>.\u00a0\u00a0 for i in range(len(row)):=,= br=,=code>>.\u00a0\u00a0\u00a0 row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0] )=,=br=,=code>>.=, =br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\u00a0 for row in dataset:=, =br=,=code>>.\u00a0\u00a0 for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0 row[i] = (row[i ] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\u00a0 train = list()=,=br=,=code>>.\u00a0 train_size = split * len(dataset)=,=br=,=code>>.\u00a0 dataset_copy = list( dataset)=,=br=,=code>>.\u00a0 while len(train) < train_size:=,=br=,=code>>.\u00a0\u00a0 index = randrange(len(dataset_copy))=,= br=,=code>>.\u00a0\u00a0 train.append(dataset_copy.pop(index))=,=br=,=code>>.\u00a0 return train, dataset_copy=,=br=,=code>> .=,=br=,=code>>.input_file = \"wine.csv\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print (\"\n- First 10 rows:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0 print(dataset[i ])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(co lumns)+1):=,=br=,=code>>.\u00a0 str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\"\n- First 10 dataset rows with floats: \")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0 print(strToFloatDataset[i])=,=br=,=code >>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\"\n- Statistics: \")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>> .\u00a0 print(\"*\"+columns[i] + \": min \" + str(minmax[i][0]) + \", max \" + str(minmax[i][1] ) + \", mean \" + str(means[i]) + \", standard deviation \" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy( strToFloatDataset)=,=br=,=code>>.standardized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>> .normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standardized_dataset, m eans, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\"\n- First 10 rows standardized dataset:\") =,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0 print(standardized_dataset[i])=,=br=,=code>>. print(\"\n- First 10 rows normalized dataset:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0 print( normalized_dataset[i])=,=br=,=code>>.print(\"\n- First 10 rows train dataset:\")=,=br=,=code>>.for i in range(10 ):=,=br=,=code>>.\u00a0 print(train[i])=,=br=,=code>>.print(\"\n- First 10 rows train dataset:\") =,=br=,=code>>.\u00a0for i in range(10): =,=br=,=code>>.\u00a0 print(test[i])=,=br=,=br=,=h3>>Results=,=img>>UT2-PD3-1.jpg>>100%>>300=,=img>>UT2-PD3-2.jpg>>100%>>300=,=img>>UT2-PD3-3.jpg>>100%>>300=,=img>>UT2-PD3-4.jpg>>100%>>600=,=img>>UT2-PD3-5.jpg>>100%>>600"
    },
    {
        "unidad":"Preparación de datos",
        "unit":"Data preparation",
        "titulo":"Data pre processing y cálculo de probabilidades de sucesos sobre el dataset Titanic con Python",
        "title":"Data pre-processing and calculation of probabilities of events on the Titanic dataset with Python",
        "descripcion":"Sobre el dataset Titanic se realizó un tratamiento de missing values con reemplazo, remoción, sustitución por categoría única y selección de atributos. Posteriormente se realizaron consultas probabilísticas sobre los datos.",
        "description":"On the Titanic dataset, a missing value treatment was carried out with replacement, removal, substitution by single category and selection of attributes. Subsequently, probabilistic queries were made on the data.",
        "contenido":"p>> - Se importaron librerías y paquetes.=,=p>> - Se cargó el dataset y se los mostró.=,=p>> - Se gestionaron los datos faltantes.=,=p>> - Se graficaron los datos según Age vs Survived y Fare vs Survived.=,=code>>.import pandas as pd=,=br=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.print(\"-----DATASET------\")=,=br=,=code>>.print(dataset.to_string())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----MISSSING VALUES------\")=,=br=,=code>>.print(\"*Antes de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.#Se reemplazan los missing values de edad por la media=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan las columnas body y cabin por altos porcentajes =,=br=,=code>>.# de missing values siendo estos 90.7% y 77% respectivamente=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\"], axis = 1, inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan la fila sin valor para fare y las dos filas=,=br=,=code>>.# Sin valor para embarked=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=['embarked'], how='all', inplace=True)=,=br=,=code>>.=,=br=,=code>>.#Se asigna una categoría única para los missing values de boay y home.dest=,=br=,=code>>.dataset[\"boat\"].fillna('U', inplace = True)=,=br=,=code>>.dataset[\"home.dest\"].fillna('U', inplace = True)=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"*Despues de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----GRAFICAS------\")=,=br=,=code>>.print(\"- x = Age, y = Survived\")=,=br=,=code>>.print(\"- x = Fare, y = Survived\")=,=br=,=code>>.colors = (\"red\", \"blue\")=,=br=,=code>>.plt.scatter(dataset['age'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.plt.scatter(dataset['fare'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-1.jpg>>30%>>100=,=br=,=img>>UT2-PD4-2.jpg>>50%>>300=,=img>>UT2-PD4-3.jpg>>50%>>300=,=br=,=br=,=p>>- Se calculó la probabilidad de que una persona sobreviva dados su sexo y clase de pasajero para luego aplicarlo a una serie de valores concretos.=,=p>> - Se calculó la probabilidad de que un niño de 10 años o menos de 3ra clase sobreviva.=,=code>>.import pandas as pd=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.def probSgivenGandC(G, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probGC = sizeGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSGC = sizeSGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSGC/probGC=,=br=,=code>>.print(\"\n-Probabilidades:\")=,=br=,=code>>.print(\"*female, class 1: \" + str(probSgivenGandC(\"female\",1)))=,=br=,=code>>.print(\"*female, class 2: \" + str(probSgivenGandC(\"female\",2)))=,=br=,=code>>.print(\"*female, class 3: \" + str(probSgivenGandC(\"female\",3)))=,=br=,=code>>.print(\"*male, class 1: \" + str(probSgivenGandC(\"male\",1)))=,=br=,=code>>.print(\"*male, class 2: \" + str(probSgivenGandC(\"male\",2)))=,=br=,=code>>.print(\"*male, class 3: \" + str(probSgivenGandC(\"male\",3)))=,=br=,=code>>.=,=br=,=code>>.def probSgivenMaxAandC(A, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probMaxAC = sizeMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSMaxAC = sizeSMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSMaxAC/probMaxAC=,=br=,=code>>.print(\"*Less than 3yo, class 3: \" + str(probSgivenMaxAandC(10,3)))=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-4.jpg>>60%>>200",
        "content":"p>> - Libraries and packages were imported.=,=p>> - The dataset was loaded and displayed.=,=p>> - Missing data was managed.=,=p>> - Data was plotted according to Age vs Survived and Fare vs Survived.=,=code>>.import pandas as pd=,=br=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt =,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.print(\"-----DATASET------\")=,=br=,=code>>.print(dataset.to_string())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>. print(\"-----MISSSING VALUES------\")=,=br=,=code>>.print(\"*Before data pre-processing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.#Missing age values ​​are replaced by mean=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=code>>.=,=br=,=code>>.#The body and cabin columns are eliminated due to high percentages =,=br=,=code>>.# de missing va lues being these 90.7% and 77% respectively=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\"], axis = 1, inplace = True)=,=br=,=code>>.=,=br=,=code>>.#The row with no value for fare and the two rows are deleted=,=br=,=code>>.# With no value for embarked= ,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=[ 'embarked'], how='all', inplace=True)=,=br=,=code>>.=,=br=,=code>>.#A unique category is assigned for the missing values ​​of boay and home.dest=,=br=,=code>>.dataset[\"boat\"].fillna('U', inplace = True)=,=br=,=code>>.dataset[\"home. dest\"].fillna('U', inplace = True)=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>> .print(\"*After data pre-processing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>. =,=br=,=code>>.print()=,=br=,=code>>.print(\"-----GRAPHICS------\")=,=br=,=code>>.print(\"- x = Age, y = Survived\")=,=br=,=code>>.print(\"- x = Fare, y = Survived\")=,=br =,=code>>.colors = (\"red\", \"blue\")=,=br=,=code>>.plt.scatter(dataset['age'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.plt.scatter(dataset['fare'], dataset['survived'], s=10, c=dataset ['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=br=,=h3>>Results=,=img>>UT2-PD4-1.jpg>>30%>>100=,=br=,=img>>UT2-PD4-2.jpg>>50%>>300=,=img>>UT2-PD4-3.jpg>>50%>>300=,=br=,=br=,=p>>- The probability of a person surviving given their gender and passenger class was calculated and then applied to a series of concrete values.=,=p>> - Calculated the probability of a 3rd class child aged 10 or younger surviving.=,=code>>.import pandas as pd=,=br=,=code>>. =,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.def probSgivenGandC(G, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 sizeGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 sizeSGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 probGC = sizeGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 probSGC = sizeSGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 return probSGC/probGC=,=br=,=code>>.print(\"\n-Probabilities:\")=,=br=,=code>>.print (\"*female, class 1: \" + str(probSgivenGandC(\"female\",1)))=,=br=,=code>>.print(\"*female, class 2: \" + str(probSgivenGandC(\"female\",2)))=,=br=,=code>>.print(\"*female, class 3: \" + str(probSgivenGandC(\"female\",3) ))=,=br=,=code>>.print(\"*male, class 1: \" + str(probSgivenGandC(\"male\",1)))=,=br=,=code>> .print(\"*male, class 2: \" + str(probSgivenGandC(\"male\",2)))=,=br=,=code>>.print(\"*male, class 3: \" + str(probSgivenGandC(\"male\",3)))=,=br=,=code>>.=,=br=,=code>>.def probSgivenMaxAandC(A, C):=,=br =,=code>>.\u00a0\u00a0\u00a0\u00a0 size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 sizeMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 sizeSMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 probMaxAC = sizeMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 probSMaxAC = sizeSMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0 return probSMaxAC/ probMaxAC=,=br=,=code>>.print(\"*Less than 3yo, class 3: \" + str(probSgivenMaxAandC(10,3)))=,=br=,=br=,=h3>>Results=,=img>>UT2-PD4-4.jpg>>60%>>200"    
    },
    {
        "unidad":"Preparación de datos",
        "unit":"Data preparation",
        "titulo":"Selección de atributos para dataset Sonar",
        "title":"Selection of attributes for Sonar dataset",
        "descripcion":"Ejercicio con selección de atributos en RapidMiner sobre el dataset title.",
        "description":"Exercise with attribute selection in RapidMiner on the title dataset.",
        "contenido":"p>>En la herramienta RapidMiner usaremos el dataset title y compararemos los resultados de performance al aplicar un simple algoritmo de Naive Bayes como línea base, Forward Selection con Naive Bayes y Backward Elimination con Naive Bayes empleando cross validation con 5 folds, muestreo estratificado y la misma seed para los tres casos.=,=h4>>Modelo=,=img>>aa4.jpg>>80%>>600=,=br=,=a>>Descargar modelo>>UT4-TA9.rmp>>d=,=br=,=br=,=h4>>Gráficas=,=p>>Se observa una tendencia en las frecuencias capturadas por el title. Vemos una amplitud y varianza clara en ambas clases. El problema que se observa a primera vista, es que las frecuencias se solapan demasiado, haciendo muy difícil su diferenciación.=,=img>>UT4-PD9-1.jpg>>80%>>400=,=br=,=br=,=img>>UT4-PD9-2.jpg>>80%>>400=,=br=,=br=,=h4>>Resultados=,=br=,=h6>>Línea base Naive Bayes=,=img>>UT4-PD9-3.jpg>>80%>>130=,=br=,=br=,=h6>>Forward Selection=,=img>>UT4-PD9-4.jpg>>80%>>130=,=p>>Mediante este algoritmo de feature selection se redujeron los atributos utilizados y la precisión de predicciones para la clase mina mejoró sustancialmente, esto quiere decir que usando solo los atributos relevantes, que representan una mayor variación en la información la precisión aumentó. Los atributos elegidos fueron los 12, 15, 17 y 18.=,=h6>>Backward Elimination=,=img>>UT4-PD9-5.jpg>>80%>>130=,=p>>Este algoritmo de feature selection eliminó 8 atributos y con ello se mejoró la performance respecto a la línea base al igual que en el caso anterior.",
        "content":"p>>In the RapidMiner tool we will use the title dataset and compare the performance results by applying a simple Naive Bayes algorithm as a baseline, Forward Selection with Naive Bayes and Backward Elimination with Naive Bayes using cross validation with 5 folds, stratified sampling and the same seed for all three cases.=,=h4>>Model=,=img>>aa4.jpg>>80%>>600=,=br=,=a>>Download model>>UT4-TA9.rmp>>d=,=br=,=br=,=h4>>Graphs=,=p>>A trend is observed in the frequencies captured by the title. We see a clear amplitude and variance in both classes. The problem that can be seen at first sight is that the frequencies overlap too much, making their differentiation very difficult.=,=img>>UT4-PD9-1.jpg>>80%>>400=,=br=,= br=,=img>>UT4-PD9-2.jpg>>80%>>400=,=br=,=br=,=h4>>Results=,=br=,=h6>>Naive Bayes Baseline =,=img>>UT4-PD9-3.jpg>>80%>>130=,=br=,=br=,=h6>>Forward Selection=,=img>>UT4-PD9-4.jpg>>80%>>130=,=p>>Through this feature selection algorithm, the attributes used were reduced and the accuracy of predictions for the mine class improved substantially, this means that using only the relevant attributes, which represent a greater variation in the information the precision increased. The chosen attributes were 12, 15, 17 and 18.=,=h6>>Backward Elimination=,=img>>UT4-PD9-5.jpg>>80%>>130=,=p>>This algorithm of feature selection removed 8 attributes and thereby improved performance compared to the baseline as in the previous case."
    },
    {
        "unidad":"Algoritmos lineales",
        "unit":"Linear algorithms",
        "titulo":"Regresión lineal simple en hoja de cálculo",
        "title":"Simple Linear Regression in spreadsheet",
        "descripcion":"Utilizando una planilla electrónica se generó un modelo de regresión linear simple calculando los coeficientes en el primer ejercicio y por descenso de gradiente en el segundo ejercicio. Para ambos casos se usaron los modelos para hacer predicciones.",
        "description":"Using an electronic spreadsheet, a simple linear regression model was generated by calculating the coefficients in the first exercise and by gradient descent in the second exercise. For both cases the models were used to make predictions.",
        "contenido":"p>>La regresión lineal es el algoritmo de machine learning mejor entendido consistiendo en una representación lineal entre las entradas x y la salida y para calcular predicciones en casos supervisados de regresión.=,=p>>Cuando hay una sola variable de entrada se llama regresión lineal simple teniendo una forma de tipo y = B0 + B1 x X y cuando hay más lleva el nombre de regresión lineal múltiple siendo por ejemplo y = B0 + B1 x X1 + B2 x X2 para el caso de dos predictores.=,=p>>El entrenamiento de modelos se basa en hallar los coeficientes que mejor aproximen una relación lineal entre las entradas y la salida y suele consistir en utilizar ecuaciones estadísticas para el caso simple y mínimos cuadrodos ordinarios (ordinary least squares) o descenso de gradiente si hay múltiples predictores. El descenso de gradiente optimiza los coeficientes de la relación lineal iterando sobre los datos de entrenamiento para minimizar el error del modelo e ir aproximándo los parámetros cada vez mejor según un parámetro de aprendizaje alfa. Los coeficientes comienzan valiendo 0 en la primer iteración.=,=p>>Para realizar predicciones con un modelo de regresión lineal basta con resolver la ecuación con los datos de entrada utilizando los coeficientes hallados en el entrenamiento.=,=p>>Es importante mencionar que se requieren muchos pasos para preparar los datos en el caso de la regresión lineal convertir todas las variables nominales a numéricas en caso de tenerlas, valorar si existe una relación lineal entre las entradas y la salida, remover las variables de entrada correlacionadas entre sí, observar si hay distribuciones gaussianas entre las variables y considerar utilizar transformaciones logarítmicas o box cox y reescalar las entradas por estandarización o normalización. =,=p>>Para mostrar estos conceptos de forma practica comenzaremos hallando los coeficientes estadísticamente para el caso de una regresión lineal simple con los siguientes valores de entrada.=,=img>>aa8.jpg>>100%>>500=,=br=,=br=,=p>>- Se utilizarán las fórmulas:=,=img>>UT3-PD1-1.jpg>>70%>>100=,=br=,=br=,=img>>UT3-PD1-2.jpg>>70%>>50=,=br=,=br=,=p>>Los valores obtenidos fueron B0 = 1.466667 y B1 = 0.342857.=,=p>>Por otra parte, se realizó un método alternativo con la siguiente ecuación para calcular B1 y B0 a partir de este al igual que antes llegando a los mismos valores.=,=img>>UT3-PD1-4.jpg>>70%>250 =,=p>> =,=p>> -Se aplicó el modelo a los valores de entrenamiento.=,=img>>aa9.jpg>>25%>>200=,=br=,=br=,=p>>-Se estimó el error de predicción RMSE llegando a un valor de 1.101 con esta ecuación:=,=img>>UT3-PD1-3.jpg>>30%>>100=,=br=,=br=,=p>> -Se añadió una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo hallado a los valores de la anterior. A partir de estas se hizo un gráfico para mostrar la recta de ajuste.=,=img>>aa10.jpg>>75%>>600=,=br=,=br=,=p>>Ahora nos enfocaremos en mostrar como se realiza descenso de gradiente para un ejemplo practico también de regresión lineal simple usando los mismos datos para X e Y que en el caso anterior.=,=p>>-Se estimaron los coeficientes del modelo simple de regresión lineal Y = B0 + B1 x X realizando 24 iteraciones. En un principio los coeficientes B0 y B1 comienzan con 0 y luego para cada época estos se modifican según los datos de la época anterior de esta manera:=,=p>> B0 = B0 de la época anterior - alfa * error de predicción época anterior=,=p>>B1 = B1 de la época anterior - alfa * error de predicción época anterior=,=p>>Usando alfa = 0.01 tras todas las iteraciones se llegö a los valores B0 = 0.21781 y B1 = 0.6382.=,=p>>-Se aplicó el modelo hallado a los valores de entrada obteniendo las siguientes cifras de predicción.=,=img>>aa11.jpg>>25%>>200.=,=p>>-Calculando el error medio cuadrático RMSE empleando la misma fórmula que en el caso anterior se llegó a RMSE = 1.230204 =,=p>>-Se realizó el gráfico error de predicción contra iteraciones.=,=img>>aa12.jpg>>60%>>400=,=br=,=br=,=p>>- Nuevamente se añadió una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo a las entradas.=,=img>>aa13.jpg>>75%>>600=,=br=,=br=,=p>>Comparando los dos métodos para llegar al modelo de regresión lineal simple se observa que en el primero se obtuvo un menor error RMSE y que la segunda tiene una pendiente mayor al anterior.=,=p>>La hoja de trabajo donde se realizaron los dos ejercicios explicados anteriormente y en la que se encuentran los resultados correspondientes puede descargarla en el siguiente enlace =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD1.xlsx>>d",
        "content":"p>>Linear regression is the best understood machine learning algorithm consisting of a linear representation between the inputs x and the output y to compute predictions in supervised regression cases.=,=p>>When there is only one input variable it is called simple linear regression having a form of type y = B0 + B1 x X and when there are more it is called multiple linear regression being for example y = B0 + B1 x X1 + B2 x X2 for the case of two predictors.=,= p>>Model training is based on finding the coefficients that best approximate a linear relationship between the inputs and the output and usually consists of using statistical equations for the simple case and ordinary least squares or gradient descent if there are multiple predictors. Gradient descent optimizes the coefficients of the linear relationship by iterating over the training data to minimize the model error and approximate the parameters better and better according to a learning parameter alpha. The coefficients start with a value of 0 in the first iteration.=,=p>>To make predictions with a linear regression model, it is enough to solve the equation with the input data using the coefficients found in the training.=,=p>>It is It is important to mention that many steps are required to prepare the data in the case of linear regression, convert all the nominal variables to numeric ones if they exist, assess if there is a linear relationship between the inputs and the output, remove the input variables correlated between yes, observe if there are Gaussian distributions between the variables and consider using logarithmic or box cox transformations and rescaling the inputs by standardization or normalization. =,=p>>To show these concepts in a practical way we will start by finding the coefficients statistically for the case of a simple linear regression with the following input values.=,=img>>aa8.jpg>>100%>>500=,=br=,=br=,=p>>- The formulas will be used:=,=img>>UT3-PD1-1.jpg>>70%>>100=,=br=,=br=,= img>>UT3-PD1-2.jpg>>70%>>50=,=br=,=br=,=p>>The values ​​obtained were B0 = 1.466667 and B1 = 0.342857.=,=p>>By On the other hand, an alternative method was carried out with the following equation to calculate B1 and B0 from this as before, arriving at the same values.=,=img>>UT3-PD1-4.jpg>>70%>250 =,=p>> =,=p>> -The model was applied to the training values.=,=img>>aa9.jpg>>25%>>200=,=br=,=br=,= p>>-The RMSE prediction error was estimated reaching a value of 1.101 with this equation:=,=img>>UT3-PD1-3.jpg>>30%>>100=,=br=,=br=,=p>> -A column was added with values ​​of x between 0 and 8 with step 0.1 and another with the result of applying the model found to the values ​​of the previous one. From these, a graph was made to show the fit line.=,=img>>aa10.jpg>>75%>>600=,=br=,=br=,=p>>Now we will focus on showing how gradient descent is performed for a practical example also of simple linear regression using the same data for X and Y as in the previous case.=,=p>>-The coefficients of the simple linear regression model Y = B0 + were estimated B1 x X performing 24 iterations. Initially the coefficients B0 and B1 start with 0 and then for each epoch these are modified according to the data of the previous epoch in this way:=,=p>> B0 = B0 of the previous epoch - alpha * epoch prediction error previous=,=p>>B1 = B1 of the previous epoch - alpha * prediction error previous epoch=,=p>>Using alpha = 0.01 after all the iterations, the values ​​B0 = 0.21781 and B1 = 0.6382 were reached.=,=p>>-The model found was applied to the input values, obtaining the following prediction figures.=,=img>>aa11.jpg>>25%>>200.=,=p>>-Calculating the error mean square RMSE using the same formula as in the previous case, RMSE = 1.230204 was reached =,=p>>-The prediction error graph was made against iterations.=,=img>>aa12.jpg>>60%>> 400=,=br=,=br=,=p>>- Again, a column with values ​​of x between 0 and 8 was added with step 0.1 and another with the result of applying the model to the inputs.=,=img>>aa13.jpg>>75%>>600=,=br=,=br=,=p>>Comparing the two methods to arrive to the simple linear regression model, it is observed that in the first one a lower RMSE error was obtained and that the second has a greater slope than the previous one.=,=p>>The worksheet where the two exercises explained above were carried out and in the corresponding results can be downloaded at the following link =,=a>>Download worksheet>>Linear algorithms PD1.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "unit":"Linear algorithms",
        "titulo":"Regresión logística en planilla electrónica",
        "title":"Logistic Regression in spreadsheet",
        "descripcion":"En una planilla electrónica se visualizará una función logística y se genararán modelos de regresión logística con descenso de gradiente estocástico.",
        "description":"A logistic function will be visualized in an electronic spreadsheet and logistic regression models will be generated with stochastic gradient descent.",
        "contenido":"p>>La regesíón logística es un algoritmo que resuelve problemas supervisados de clasificación binaria en base a la función logística la cual tiene la siguiente forma y gráfica.=,=img>>aaa1.jpg>>250>>100=,=br=,=br=,=img>>aaa2.jpg>>75%>>375.=,=p>>En el caso de la regresión lineal se utilza la siguiente función de tipo logística..=,=img>>a1.jpg>>250>>100=,=br=,=br=,=p>>A partir de esta se entrena hallando los valores de los coeficientes con descenso de gradiente estocástico y tras ello se realizan predicciones reemplazando los valoes en la función obtenida. Siempre los resultados obtenidos estarán en el rango (0,1) por lo que definiendo un cierto valor en ese rango se determinan dos subrangos en los que a partir del resultado obtenido de la función evaluada con ciertos datos de entrada entrada se los clasifica como de una clase u otra.=,=p>>Para aplicar regresión logística es conveniente remover outliers, tener distribuciones gaussianas para lo que puede contribuir realizar transformar logarítmicas o box cox y quitar entradas correlacionadas.=,=p>>En esta ocasiön veremos un ejercicio práctico en el que se generará un modelo de regresión logística con el que realizar predicciones a partir del siguiente conjunto de datos.=,=img>>a2.jpg>>30%>>400=,=br=,=br=,=p>>Se estimaron los coeficientes del modelo de regresión logística Y = B0+B1xX1+B2xX2 con el algoritmo de descenso de gradiente estocástico utilizando un coeficiente alpha = 0.3 y repitiendo el proceso para 10 épocas lo que equivale a hacer 10 iteraciones sobre cada ejemplo del dataset.=,=p>>Para ver cómo van mejoran las predicciones al aumentar las épocas se realizaron las gráficas de exactitud vs épocas y RMSE VS épocas generándose las siguientes visualizaciones.=,=img>>a3.jpg>>100%>>300=,=br=,=br=,=p>>La hoja de trabajo donde se realizaron los procedimientos anteriores puede ser descargada en el siguiente enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD2.xlsx>>d",
        "content":"p>>Logistic regestion is an algorithm that solves supervised binary classification problems based on to the logistic function which has the following form and graph.=,=img>>aaa1.jpg>>250>>100=,=br=,=br=,=img>>aaa2.jpg>>75%>>375.=,=p>>In the case of linear regression, the following logistic type function is used..=,=img>>a1.jpg>>250>>100=,=br=,=br=,=p>>From this, it is trained by finding the values ​​of the coefficients with stochastic gradient descent and after that, predictions are made by replacing the values ​​in the function obtained. The results obtained will always be in the range (0,1), so by defining a certain value in that range, two subranges are determined in which, based on the result obtained from the function evaluated with certain input data, they are classified as of one class or another.=,=p>>To apply logistic regression it is convenient to remove outliers, to have Gaussian distributions for which it can contribute to carry out logarithmic or box cox transforms and remove correlated entries.=,=p>>This time we will see a practical exercise in which a logistic regression model will be generated with which to make predictions from the following data set.=,=img>>a2.jpg>>30%>>400=,=br=,=br=,=p>>The coefficients of the logistic regression model Y = B0+B1xX1+B2xX2 were estimated with the stochastic gradient descent algorithm using an alpha coefficient = 0.3 and repeating the process for 10 epochs, which is equivalent to doing 10 iterations on each example of the dataset.=,=p>>To see how the predictions improve as the epochs increase, the graphs of accuracy vs. epochs and RMSE vs. epochs were made, generating the following visualizations.=,=img>>a3.jpg>>100%>>300=,=br=,=br=,=p>>The worksheet where the previous procedures were carried out can be downloaded at the following link: =,=a>>Download worksheet>>Linear algorithms PD2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "unit":"Linear algorithms",
        "titulo":"LDA en hoja de cálculo",
        "title":"LDA in spreadsheet",
        "descripcion":"Empleando una planilla electrónica se generará un modelo de análisis de discriminante lineal LDA y con él se hará la predicción de clase para un conjunto de datos con un atributo y una salida con dos clases.",
        "description":"Using an electronic spreadsheet, an LDA linear discriminant analysis model will be generated and with it the class prediction will be made for a data set with one attribute and an output with two classes.",
        "contenido":"p>>El algoritmo de Linear Discriminant analysis se utiliza para problemas de clasificación sin tener la restricción de ser solo para variables nominales como ocurria en Logistic Regression. Este modelo casume que los datos tienen distribución gaussiana y que los atributos tienen la misma varianza.=,=p>>Asumiendo los puntos anteriores LDA estima la media y la varianza para cada clase utilizando las siguientes ecuaciónes.=,=img>>a4.jpg>>30%>>90=,=img>>a5.jpg>>40%>>90=,=p>>LDA hace predicciones estimando las probabilidades de que un input pertenezca a una clase utilizando el discriminante de las diferentes clases cuya teoría proviene del teorema de Bayes y su fórmula para la clase k se encuentra a continuación.=,=img>>a6.jpg>>60%>>100=,=p>>Siendo P(k) el ratio de instancias con esa clase en el dataset.=,=p>>Finalmente, para un ejemplo se le designa la clase cuyo discriminante de un valor más alto.=,=p>>A partir de la teoría anterior se procede a realizar un ejemplo practico que se muestra contiguamente.=,=p>>- Se insertaron los valores de un dataset con variables numéricas X para la entrada e Y para la salida. =,=p>> - Se graficaron los datos separando según las clases de salida.=,=img>>a7.jpg>>60%>>400=,=br=,=br=,=p>> - Se calcularon, siendo n el número de clases, las medias para cada clase k y la varianza de x con las ecuaciones que se mostraron anteriormente.=,=p>> - Se calculó la predicción de clases usando: =,=img>>UT3-PD3-4.jpg>>100%>200 =,=p>> - Se utilizó la ecuación para calcular los discriminantes de x para ambas clases Y = 0 e Y = 1 y finalmente se realizaron las predicciones de clase para los datos comparando los discriminantes. La exactitud alcanzada por el modelo generado fue calculada dando un valor de 0.65.=,=p>>La hoja de trabajo donde se hizo la práctica puede descargarse en este enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD3.xlsx>>d",
        "content":"p>>The Linear Discriminant analysis algorithm is used for classification problems without the restriction of being only for nominal variables as was the case in Logistic Regression. This model assumes that the data have a Gaussian distribution and that the attributes have the same variance.=,=p>>Assuming the previous points, LDA estimates the mean and variance for each class using the following equations.=,=img>>a4.jpg>>30%>>90=,=img>>a5.jpg>>40%>>90=,=p>>LDA makes predictions by estimating the probabilities that an input belongs to a class using the discriminant of the different classes whose theory comes from Bayes theorem and its formula for class k is found below.=,=img>>a6.jpg>>60%>>100=,=p>>where P(k) is the ratio of instances with that class in the dataset.=,=p>>Finally, for an example, the class whose discriminant value is higher is designated.=,=p>>Based on the previous theory, a practical example shown next.=,=p>>- The values ​​of a dataset with numeric variables X for input and Y for output were inserted. =,=p>> - The data was plotted separating according to the output classes.=,=img>>a7.jpg>>60%>>400=,=br=,=br=,=p>> - It calculated, where n is the number of classes, the means for each class k and the variance of x with the equations shown above.=,=p>> - Class prediction was calculated using: =,=img>>UT3-PD3-4.jpg>>100%>200 =,=p>> - The equation was used to calculate the discriminants of x for both classes Y = 0 and Y = 1 and finally class predictions were made for the data comparing the discriminants. The accuracy achieved by the generated model was calculated giving a value of 0.65.=,=p>>The worksheet where the practice was done can be downloaded at this link: =,=a>>Download worksheet>>Linear algorithms PD3.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "unit":"Linear algorithms",
        "titulo":"Regresión lineal con descenso de gradiente",
        "title":"Comparación de LDA con Python y RapidMiner para el dataset Sports",
        "descripcion":"En este ejercicio se usa una planilla electrónica para minimizar una función siguiendo los gradientes de la función de costo y luego armar un modelo de regresión lineal.",
        "description":"Se realizan ejercicios con modelos de LDA para en Python y RapidMiner y se comparan los resultados obtenidos.",
        "contenido":"p>>- Se representó en la planilla un dataset con una variable de entrada X y una de salida Y.=,=p>> - Se graficaron los datos. =,=img>>UT3-PD4-1.jpg>>100%>400=,=p>> - Se realizó el procedimiento de descenso de gradiente con alpha=0.01 en 24 iteraciones hallando valores para los coeficientes B0 y B1 que en una regresión lineal siguen la relación Y = B1xX + B0, la predicción para dicho modelo y el error de predicción. =,=p>> - Se graficó el error de predicción vs iteraciones. =,=img>>UT3-PD4-2.jpg>>100%>400=,=p>> - Se calculó el error medio cuadrático RMSE. =,=p>> - Se generaron nuevos valores de entrada para X entre 0 y 8 con un paso 0.1 y se predijo su valor de Y a partir de un modelo de regresión lineal usando los coeficientes hallados al finalizar el procedimento mencionado anteriormente. El resultado fue graficado. =,=img>>UT3-PD4-3.jpg>>100%>400 =,=p>> - Se analizaron los datos de entrada desde la óptica de los requerimientos para aplicar un método de regresión lineal.=,=br=,=h3>>Resultados=,=p>> La planilla de trabajo donde se realizaron los pasos descriptos anteriormente se encuentra accesible para la descarga con el enlace que se encuentra a continuación:=,=a>>Descargar plantilla>>Algoritmos lineales PD4.xlsx>>d",
        "content":"p>>- A dataset with an input variable X and an output variable Y was represented on the spreadsheet.=,=p>> - The data was graphed. =,=img>>UT3-PD4-1.jpg>>100%>400=,=p>> - The gradient descent procedure was performed with alpha=0.01 in 24 iterations, finding values ​​for the coefficients B0 and B1 that in a linear regression they follow the relation Y = B1xX + B0, the prediction for said model and the prediction error. =,=p>> - The prediction error vs iterations was plotted. =,=img>>UT3-PD4-2.jpg>>100%>400=,=p>> - RMSE mean square error was calculated. =,=p>> - New input values ​​for X between 0 and 8 were generated with a step of 0.1 and its Y value was predicted from a linear regression model using the coefficients found at the end of the procedure mentioned above. The result was graphed. =,=img>>UT3-PD4-3.jpg>>100%>400 =,=p>> - The input data was analyzed from the perspective of the requirements to apply a linear regression method.=,=br =,=h3>>Results=,=p>> The worksheet where the steps described above were carried out is accessible for download with the link below:=,=a>>Download template>>Algorithms linear PD4.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "unit":"Linear algorithms",
        "titulo":"Comparación de LDA con Python y RapidMiner para el dataset Sports",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificación a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.",
        "contenido":"h2>>Ejercicio 1=,=p>>- Se descargó un dataset de prueba Sample.csv.=,=p>>- Se leyó el archivo CSV y se graficaron los datos utilizando pyplot de matplotlib.=,=p>>- Se dividió el conjunto de datos en una parte de entrenamiento y otra de prueba.=,=p>>- Se creó y entrenó un modelo LDA.=,=p>>- Se predijeron las clases para los datos de prueba y se compararon los resultados.=,=p>>- Se imprimió el reporte de clasificación y la matriz de confusión=,=code>>.=,=br=,=code>>.input_file = \"sample.csv\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br=,=code>>.=,=br=,=code>>.colors = (\"orange\", \"blue\")=,=br=,=code>>.plt.scatter(df['x'], df['y'], s=300, c=df['label'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.=,=br=,=code>>.X = df[['x', 'y']].values=,=br=,=code>>.y = df['label'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state=0, shuffle=True)   =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\"Predicted vs Expected\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br=,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br=,=h3>>Resultados=,=img>>UT3-PD5-1.jpg>>100%>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br=,=br=,=h2>>Ejercicio 2=,=p>>- Se descargó el dataset sports_Training.csv.=,=p>>- Se eliminaron las filas con atributo CapacidadDecision menores de 3 y mayores a 100. =,=p>>- Se transformaron los atributos string a números. =,=p>> - Se utilizó el modelo para clasificar los datos del archivo sports_Scoring.csv.=,=p>>- Se realizó el procedimiento equivalente en la herramienta RapidMiner.=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = ['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']=,=br=,=code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training['CapacidadDecision'] >= 3) & (data_training['CapacidadDecision'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring['CapacidadDecision'] >= 3) & (data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Esquema proceso RapidMiner=,=img>>UT3-PD5-3.jpg>>100%>>300=,=br=,=a>>Descargar proceso>>Algoritmos Lineales PD5.rmp>>d=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d",
        "content":"h2>>Exercise 1=,=p>>- Downloaded a test dataset Sample.csv.=,=p>>- Read the CSV file and plotted the data using pyplot from matplotlib .=,=p>>- The data set was divided into a training part and a test part.=,=p>>- An LDA model was created and trained.=,=p>>- The classes were predicted for the test data and the results were compared.=,=p>>- The classification report and the confusion matrix were printed=,=code>>.=,=br=,=code>>.input_file = \"sample.csv\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br =,=code e>>.=,=br=,=code>>.colors = (\"orange\", \"blue\")=,=br=,=code>>.plt.scatter(df['x' ], df['y'], s=300, c=df['label'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()= ,=br=,=code>>.=,=br=,=code>>.X = df[['x', 'y']].values=,=br=,=code>>.y = df['label'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state= 0, shuffle=True) =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit( train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\"Predicted vs Expected\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br =,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br= ,=h3>>Results=,=img>>UT3-PD5-1.jpg>>100%>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br= ,=br=,=h2>>Exercise 2=,=p>>- The sports_Training.csv dataset was downloaded.=,=p>>- S The rows with the CapacityDecision attribute less than 3 and greater than 100 were eliminated. =,=p>>- The string attributes were transformed into numbers. =,=p>> - The model was used to classify the data in the file sports_Scoring.csv.=,=p>>- The equivalent procedure was carried out in the RapidMiner tool.=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=, =br=,=code>>.labels = ['Age','Strength','Speed','Injuries','Vision','Endurance','Agility','DecisionAbility']=,=br=, =code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br= ,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code> >.=,=br=,=code>>.data_training = data_training[(data_training['DecisionCapacity'] >= 3) & (data_training['DecisionCapacity'] <= 100)]=,=br=,=code> >.data_scoring = data_scoring[(data_scoring['DecisionCapacity'] >= 3) & (data_scoring['DecisionCapacity'] <= 100)]= ,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels] )=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['PrimarySport'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring [labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({'Prediction': y_pred})= ,=br=,=code>>.writer = pd.ExcelWriter('Prediction.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1' )=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>RapidMiner process scheme=,=img>>UT3-PD5-3.jpg>>100 %>>300=,=br=,=a>>Download process>>Linear Algorithms PD5.rmp>>d=,=br=,=br=,=h3>>Comparison of results between Scikitlearn and RapidMiner=,= p>>98.81% of values ​​were obtained of prediction the same for the models made with Sckitlearn and RapidMiner.=,=a>>Download comparative template>>Comparison Rapid Miner Sci Kit Learn.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "unit":"Linear algorithms",
        "titulo":"Titanic",
        "title":"Titanic dataset",
        "descripcion":"Se realizará un análisis del problema y de los datos del dataset Titanic para a partir de este crear un modelo de regresión logística con la herramienta RapidMiner y otro equivalente con SciKitLearn, medir sus performances y compararlas.",
        "description":"In this case, a data preparation study is done for the Titanic dataset and equivalent logistic regression models are created for RapidMiner and SciKitLearn.",
        "contenido":"h2>>Problema=,=p>>A partir de datos respecto al hundimiento de Titanic se pretende predecir si una persona con determinadas características sobrevivió o no por lo cual es un problema supervisado de clasificación binaria. Para este se propone una solución con un algoritmo lineal de regresión logística utilizando CrossValidation para el testeo de su performance.=,=h2>>Análisis de datos=,=p>>El dataset cuenta con 13 atributos de entrada más la variable objetivo \"survived\" para los cuales se cuenta con 1310 observaciones.=,=h4>>Atributos=,=p>>- pclass: Clase del pasajero. Tipo integer.=,=p>>- name. Nombre del pasajero. Tipo string.=,=p>>- sex. Sexo del viajante. Tipo string.=,=p>>- age. Edad de la persona. Tipo real. Hay 263 datos faltantes para esta variable.=,=p>>- sibsp: Número de hermanos a bordo. Tipo entero.=,=p>>- parch: Número de padres a bordo. Tipo entero.=,=p>>- ticket: Código del ticket del pasajero. Tipo string.=,=p>>- fare: Tarifa pagada. Tipo real. Hay un dato faltante para esta entrada.=,=p>>- cabin. Tipo string. Cabina del viajero. Tipo string. Tiene 1014 missing values. =,=p>>- embarked: Puerto en el que embarcó. Tipo string. Faltan dos de estos datos.=,=p>>- boat: Código del bote salvavida que usó la persona. Tipo string. Cuenta con 823 missing values.=,=p>>- body: Número de cuerpo si falleció. Tipo integer. Cuenta con 1188 missing values. =,=p>>- home.dest: Lugar de destino. Tipo string. Tiene 564 missing values.=,=p>>- survived: Si sobrevivió o no. Tipo integer.=,=br=,=h3>>Preparación de datos=,=br=,=h5>>Missing Values=,=p>>Entre las entradas hay variables con alto porcentaje de datos faltantes por lo que no se contó con ellas siendo estas body con un 90.7% y cabin con un 77.4%, para la variable age se reemplazaron los valores faltantes por el promedio, se eliminaron las filas con missing values para fare y embarked y se le asignó una categoría única para los missing values de boat y home.dest. Luego de todo este proceso se eliminaron todos los missing values.=,=h5>>Outliers=,=p>>El algoritmo de regresión logística es altamente influenciable a los outliers y por lo tanto tras observar los histogramas de los datos para los distintos parámetros, que se encuentran a continuación, se eliminaron aquellos con una alguna/s de las siguientes condiciones age>=70, sibsp>=6, parch>=6 o fare>=350.=,=img>>caso-2-2.jpg>>100%>>150=,=img>>caso-2-3.jpg>>100%>>150=,=br=,=img>>caso-2-4.jpg>>100%>>150=,=img>>caso-2-5.jpg>>100%>>150=,=h5>>Correlación=,=p>>Observando la matriz de correlación se observó que hay una correlación grande entre home.dest y pclass con 0.887 y otra bastante significativa con un 0.612 entre fare y pclass de forma inversamente proporcional. Por lo tanto se decidió quitar pclass.=,=img>>caso-2-6.jpg>>100%>>200=,=br=,=br=,=h5>>Selección de atributos=,=p>>Se descartaron las variables name y ticket por ser ids.=,=h3>>Modelo=,=p>>Se realizó el procedimiento de data pre-processing descripto anteriormente y luego se utilizó un algoritmo de regresión logística con CrossValidation de 10 folds y muestreo estratificado para el testeo.=,=h5>>Diseño de RapidMiner=,=img>>caso-2-7.jpg>>100%>>400=,=p>>A continuación se mostrará la configuración bloque a bloque del esquema de RapidMiner y se presentará a la vez el código del modelo idéntico creado en SciKitLearn.=,=h5>>Retrieve Titanic=,=img>>caso-2-rm-1.jpg>>100%>>75.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=br=,=h5>>Select Attributes=,=img>>caso-2-rm-2.jpg>>100%>>400.=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\",\"name\",\"pclass\",\"ticket\"], axis = 1, inplace = True)=,=br=,=br=,=h5>>Primer replace Missing Values=,=img>>caso-2-rm-3.jpg>>100%>>175=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=br=,=h5>>Segundo replace Missing Values=,=img>>caso-2-rm-4.jpg>>100%>>400=,=br=,=code>>.dataset[\"boat\"].fillna('0', inplace = True)=,=br=,=code>>.dataset[\"home.dest\"].fillna('0', inplace = True)=,=br=,=br=,=h5>>Filter Examples=,=img>>caso-2-rm-5.jpg>>100%>>400.=,=br=,=code>>.dataset = dataset[dataset['sibsp'] <= 5]=,=br=,=code>>.dataset = dataset[dataset['parch'] <= 5]=,=br=,=code>>.dataset = dataset[dataset['age'] <= 70]=,=br=,=code>>.dataset = dataset[dataset['fare'] <= 350]=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=['embarked'], how='all', inplace=True)=,=br=,=br=,=h5>>Nominal to Numerical=,=img>>caso-2-rm-6.jpg>>100%>>400..=,=br=,=code>>.le = LabelEncoder()=,=br=,=code>>.le.fit(dataset[\"sex\"])=,=br=,=code>>.encoded_sex_training = le.transform(dataset[\"sex\"])=,=br=,=code>>.dataset[\"sex\"] = encoded_sex_training=,=br=,=code>>.le.fit(dataset[\"embarked\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"embarked\"])=,=br=,=code>>.dataset[\"embarked\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"home.dest\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"home.dest\"])=,=br=,=code>>.dataset[\"home.dest\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"boat\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"boat\"])=,=br=,=code>>.dataset[\"boat\"] = encoded_embarked_training=,=br=,=br=,=h5>>Cross Validation=,=img>>caso-2-rm-7.jpg>>100%>>150.=,=br=,=code>>.X = dataset.drop(labels=[\"survived\"], axis=1)=,=br=,=code>>.y = dataset[\"survived\"]=,=br=,=code>>.X, y = make_classification(n_samples=1283, n_features=10)=,=br=,=code>>.cv = StratifiedKFold(n_splits=10, random_state=None, shuffle=True)=,=br=,=code>>.model = LogisticRegression()=,=br=,=code>>.scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)=,=br=,=code>>.print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))=,=br=,=br=,=h4>>Resultados=,=p>>Se obtuvo un muy alto nivel de presición en RapidMiner con un 96.88% +/- 1.1%. =,=img>>caso-2-9.jpg>>100%>>200=,=br=,=br=,=img>>caso-2-10.jpg>>30%>>25=,=p>> En cuanto al modelo de SciKitLearn se obtuvo una presición de 94.5% +/- 1.2%. De esta forma se puede destacar que se obtuvieron presiciones similares para de los modelos idénticos de regresión logística aplicados al dataset Titanic realizados en las dos herramientas.=,=br=,=a>>Descargar modelo RapidMiner>>Titanic Logistic Regression.rmp>>d=,=br=,=a>>Descargar código SciKitLearn>>Titanic Logistic Regression.py>>d",
        "content":"h2>>Problem=,=p>>Based on data regarding the sinking of the Titanic, the aim is to predict whether a person with certain characteristics survived or not, which is why it is a supervised binary classification problem. For this, a solution is proposed with a linear logistic regression algorithm using CrossValidation to test its performance.=,=h2>>Data analysis=,=p>>The dataset has 13 input attributes plus the target variable \"survived\" for which there are 1310 observations.=,=h4>>Attributes=,=p>>- pclass: Passenger class. Type integer.=,=p>>- name. Name of the passenger. Type string.=,=p>>-sex. Sex of the traveler. Type string.=,=p>>-age. Age of the person. royal guy. There are 263 missing data for this variable.=,=p>>- sibsp: Number of brothers on board. Integer type.=,=p>>- patch: Number of parents on board. Integer type.=,=p>>- ticket: Passenger ticket code. Type string.=,=p>>- fare: Rate paid. royal guy. There is a missing data for this entry.=,=p>>- cabin. String type. Traveler's cabin. String type. It has 1014 missing values. =,=p>>- embarked: Port where you embarked. String type. Two of these data are missing.=,=p>>- boat: Code of the lifeboat used by the person. String type. It has 823 missing values.=,=p>>- body: Body number if deceased. Integer type. It has 1188 missing values. =,=p>>- home.dest: Place of destination. String type. It has 564 missing values.=,=p>>- survived: If it survived or not. Type integer.=,=br=,=h3>>Data preparation=,=br=,=h5>>Missing Values=,=p>>Among the inputs there are variables with a high percentage of missing data, so it is not it had them being these body with 90.7% and cabin with 77.4%, for the age variable the missing values ​​were replaced by the average, the rows with missing values ​​for fare embarked were eliminated and a unique category was assigned for the missing values ​​of boat and home.dest. After all this process, all the missing values ​​were eliminated.=,=h5>>Outliers=,=p>>The logistic regression algorithm is highly influenced by the outliers and therefore, after observing the histograms of the data for the different parameters, found below, those with one/s of the following conditions age>=70, sibsp>=6, parch>=6 or fare>=350.=,=img>>caso-2-2.jpg>>100%>>150=,=img>>caso-2-3.jpg>>100%>>150=,=br=,=img>>caso-2-4.jpg>>100%>>150=,=img>>caso-2-5.jpg>>100%>>150=,=h5>>Correlation=,=p>>Observing the correlation matrix, it was observed that there is a large correlation between home.dest and pclass with 0.887 and another quite significant one with 0.612 between fare and pclass in an inversely proportional way. Therefore it was decided to remove pclass.=,=img>>caso-2-6.jpg>>100%>>200=,=br=,=br=,=h5>>Selection attributes=,=p>>The name and ticket variables were discarded because they were ids.=,=h3>>Model=,=p>>The data pre-processing procedure described above was performed and then a logistic regression algorithm with 10-fold CrossValidation was used and stratified sampling for testing.=,=h5>>RapidMiner design=,=img>>caso-2-7.jpg>>100%>>400=,=p>>The block configuration will be shown below RapidMiner schematic block and the identical model code created in SciKitLearn will be presented at the same time.=,=h5>>Retrieve Titanic=,=img>>caso-2-rm-1.jpg>>100%>>75=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=br=,=h5>>Select Attributes=,=img>>caso-2-rm-2.jpg>>100%>>400.=,=br=,=code>>.dataset.drop(labels = [\"body\",\"cabin\",\"name\",\"pclass\",\"ticket\"], axis = 1, inplace = True)=,=br=,=br=,=h5>>First replace Missing Values=,=img>>caso-2-rm-3.jpg>>100%>>175=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age \"].mean(), inplace = True)=,=br=,=br=,=h5>>Second replace Missing Values=,=img>>caso-2-rm-4.jpg>>100%>>400=,=br=,=code>>.dataset[\"boat\"].fillna('0', inplace = True)=,=br=,=code>>.dataset[\"home.dest \"].fillna('0', inplace = True)=,=br=,=br=,=h5>>Filter Examples=,=img>>caso-2-rm-5.jpg>>100%>>400.=,=br=,=code>>.dataset = dataset[dataset['sibsp'] <= 5]=,=br=,=code>>.dataset = dataset[dataset['parch'] < = 5]=,=br=,=code>>.dataset = dataset[dataset['age'] <= 70]=,=br=,=code>>.dataset = dataset[dataset['fare'] < = 350]=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna (subset=['embarked'], how='all', inplace=True)=,=br=,=br=,=h5>>Nominal to Numerical=,=img>>caso-2-rm-6. jpg>>100%>>400..=,=br=,=code>>.le = LabelEncoder()=,=br=,=code>>.le.fit(dataset[\"sex\"]) =,=br=,=code>>.encoded_sex_training = le.transform(dataset[\"sex\"])=,=br=,=code>>.dataset[\"sex \"] = encoded_sex_training=,=br=,=code>>.le.fit(dataset[\"embarked\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\" embarked\"])=,=br=,=code>>.dataset[\"embarked\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"home.dest\" ])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"home.dest\"])=,=br=,=code>>.dataset[\"home.dest\" ] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"boat\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"boat\"])=,=br=,=code>>.dataset[\"boat\"] = encoded_embarked_training=,=br=,=br=,=h5>>Cross Validation=,=img>>caso-2- rm-7.jpg>>100%>>150.=,=br=,=code>>.X = dataset.drop(labels=[\"survived\"], axis=1)=,=br=,=code>>.y = dataset[\"survived\"]=,=br=,=code>>.X, y = make_classification(n_samples=1283, n_features=10)=,=br=,=code>> .cv = StratifiedKFold(n_splits=10, random_state=None, shuffle=True)=,=br=,=code>>.model = LogisticRegression()=,=br=,=code>>.scores = cross_val_score(model, x, y, score g='accuracy', cv=cv, n_jobs=-1)=,=br=,=code>>.print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores )))=,=br=,=br=,=h4>>Results=,=p>>A very high level of accuracy was obtained in RapidMiner with 96.88% +/- 1.1%. =,=img>>caso-2-9.jpg>>100%>>200=,=br=,=br=,=img>>caso-2-10.jpg>>30%>>25=,=p>> Regarding the SciKitLearn model, an accuracy of 94.5% +/- 1.2% was obtained. In this way, it can be noted that similar precisions were obtained for the identical logistic regression models applied to the Titanic dataset carried out in the two tools.=,=br=,=a>>Download RapidMiner model>>Titanic Logistic Regression.rmp>>d=,=br=,=a>>Download SciKitLearn code>>Titanic Logistic Regression.py>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "unit":"Linear algorithms",
        "titulo":"Deportes",
        "title":"Sports",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificación a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.",
        "description":"We will use Linear Discriminant Analysis (LDA) with Python's Sckit-learn and RapidMiner to train and classify from the Sports dataset to finally compare the predictions made by both models.",
        "contenido":"h3>>Problema=,=p>>Según características personales se desea predecir cual es el mejor deporte para una persona por lo cual este es un problema supervisado de clasificación. Para este se planteará un modelado con Linear Descriminant Analysis a continuación.=,=h3>>Análisis de datos=,=p>>El dataset tiene 8 columnas de entrada numéricas y la variable objetivo categórica \"DeportePrimario\" conteniendo 493 filas.=,=h4>>Atributos=,=p>>- Edad=,=p>>- Fuerza=,=p>>- Velocidad =,=p>>- Lesiones=,=p>>- Vision =,=p>>- Resistencia =,=p>>- Agilidad =,=p>>- CapacidadDecision =,=p>>- DeportePrimario=,=br=,=h3>>Preparación de datos=,=br=,=h5>>Missing Values=,=p>>En el dataset no hay datos faltantes.=,=h5>>Outliers=,=p>>Para LDA es importante quitar los outliers y tras observar los histogramas de los atributos se decidió quitar los datos con CapacidadDecision<3 o CapacidadDecision>100. La distribución para esta variable es la siguiente.=,=img>>caso-3-1.jpg>>100%>>200=,=h4>>Estándarización=,=p>>El algoritmo a utilizar asume que las variables de entrada tienen la misma varianza por lo que es buena idea estandarizar los datos para que tengan media 0 y varianza 1.=,=h3>>Modelo=,=p>>El procedimiento descripto anteriormente y el modelo LDA fueron realizados de forma equivalente en RapidMiner y SciKitLearn. Se contó con un dataset de entrenamiento y uno de prueba para el cual realizar predicciones.=,=h4>>Diseño de RapidMiner=,=img>>UT3-PD5-3.jpg>>100%>>300=,=br=,=br=,=h4>>Código SciKitLearn=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = ['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']=,=br=,=code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training['CapacidadDecision'] >= 3) & (data_training['CapacidadDecision'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring['CapacidadDecision'] >= 3) & (data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar proceso>>Sports LDA.rmp>>d=,=br=,=a>>Descargar código SciKitLearn>>Sports LDA.py>>d=,=br=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d",
        "content":"h3>>Problem=,=p>>According to personal characteristics, it is desired to predict which is the best sport for a person, for which this is a supervised classification problem. For this, a modeling with Linear Discriminant Analysis will be proposed below.=,=h3>>Data analysis=,=p>>The dataset has 8 numeric input columns and the categorical target variable \"DeportePrimario\" containing 493 rows. =,=h4>>Attributes=,=p>>- Age=,=p>>- Strength=,=p>>- Speed ​​=,=p>>- Injuries=,=p>>- Vision =,= p>>- Endurance =,=p>>- Agility =,=p>>- DecisionCapacity =,=p>>- PrimarySport=,=br=,=h3>>Data preparation=,=br=,=h5 >>Missing Values=,=p>>There are no missing data in the dataset.=,=h5>>Outliers=,=p>>For LDA it is important to remove the outliers and after observing the histograms of the attributes it was decided to remove the data with CapacityDecision<3 or CapacityDecision>100. The distribution for this variable is as follows.=,=img>>caso-3-1.jpg>>100%>>200=,=h4>>Standardization=,=p>>The algorithm to use assumes that the variables input have the same variance so it is a good idea to standardize the data so that they have mean 0 and variance 1.=,=h3>>Model=,=p>>The procedure described above and the LDA model were performed equivalently in RapidMiner and SciKitLearn. We had a training dataset and a test dataset for which to make predictions.=,=h4>>RapidMiner Design=,=img>>UT3-PD5-3.jpg>>100%>>300=,=br =,=br=,=h4>>SciKitLearn code=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>> .from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = ['Age','Strength','Speed','Injuries','Vision ','Endurance','Agility','CapacityDecision']=,=br=,=code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training['CapacityDecision'] >= 3) & ( data_training['DecisionCapacity'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring['DecisionCapacity'] >= 3) & (data_scoring['DecisionCapacity'] ision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br =,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['PrimarySport'].values=,=br=,=code>>.=,= br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame( {'Prediction': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediction.xlsx', engine='xlsxwriter')=,=br=,=code>>.df. to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Comparison of results between Scikitlearn and RapidMiner=,= p>>98.81% equal prediction values ​​were obtained for the models made with Sckitlearn and RapidMiner.=,=a>>Download process>>Sports LDA.rmp >>d=,=br=,=a>>Download SciKitLearn Code>>Sports LDA.py>>d=,=br=,=a>>Download Comparison Template>>Rapid Miner Sci Kit Learn Comparison.xlsx>> d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "unit":"Non linear algorithms",
        "titulo":"CART para clasificación binaria simple",
        "title":"CART for Simple Binary Classification",
        "descripcion":"Emplearemos un modelo CART para un problema de clasificación binaria simple con dos variables de entrada X1 y X2 y una variable de salida Y.",
        "description":"We will use a CART model for a simple binary classification problem with two input variables X1 and X2 and one output variable Y.",
        "contenido":"p>>Los árboles de decisión son un importante algoritmo de predicción de machine learning para predicción supervisada de tanto para clasificación como para regresión. Esta estructura es la misma de algoritmos y estructuras de datos, árboles binarios, pero en este caso cada nodo representa una sola variable de entrada x y un split point para esa variable. Los nodos hojas se diferencian en que contienen un valor de salida el cual es utilizado para hacer la predicción. Tras tener entrenado el árbol es muy sencillo hacer prediccíones, solo basta con recorrer el árbol con los datos de entrada a etiquetar.=,=p>>Para entrenar los árboles de decisión recursivamente se divide el espacio de la entrada de forma binaria utilizando un algoritmo voraz el cual consiste en probar diferentes splits utilizando una funcion de costos y quedarse con el que lo minimice.=,=p>>En los problemas de regression la funcion de costos a minimizar es la suma de error cuadrático.=,=img>>pds0.jpg>>35%>>75=,=br=,=br=,=p>>En los problemas de clasificación se utiliza el índice de Gini que indica que tan puros son los nodos hoja.=,=img>>aa1.jpg>>35%>>75=,=br=,=br=,=p>>El criterio de freno del alguritmo recursivos más común es utilizar un mínimo de instancias asignadas a cada nodo hoja, cuando al realizar un splir se encuentra con un número menor a ese no lo hace y ese nodo se designa hoja. Cuantos más nodos por hoja más genérico es el árbol y al tener menos es más específico lo que puede tender al overfitting.=,=p>>A continuación se realizará un ejercicio en una hoja de cálculo que muestra como realizar una partición en base al índice de Gini para armar un árbol de decisión sencillo que luego será utilizado para realizar predicciones.=,=p>>- Se ingresó el dataset de entrada y se graficaron los datos diferenciando las clases Y = 0 e Y = 1. =,=img>>UT4-PD1-1.jpg>>100%>>200=,=p>> - Se utilizó la variable X1 para armar el modelo CART con un valor para el punto de división de X1 = 2.771244718 calculando el índice Gini del modelo producido. =,=p>> - Se repitió el proceso anterior con un punto de división de X1 = 6.642287351 y se obtuvo el valor del coeficiente Gini del CART generado además de una descripción del árbol de decisión generado.=,=img>>aa2.jpg>>100%>>400=,=p>> Se utilizó el CART producido para hacer predicciones de un dataset de test y se calculó la exactitud.=,=img>>aa3.jpg>>50%>>300=,=br=,=br=,=p>> Las planillas de trabajo en las que se hicieron los pasos explicados están accesibles para descargar con los siguientes enlaces:=,=a>>Descargar plantilla entrenamiento>>Algoritmos no lineales PD1-1.xlsx>>d=,=br=,=a>>Descargar plantilla predicciones>>Algoritmos no lineales PD1-2.xlsx>>d",
        "content":"p>>Decision trees are an important machine learning prediction algorithm for supervised prediction for both classification and regression. This structure is the same as algorithms and data structures, binary trees, but in this case each node represents a single input variable x and a split point for that variable. Leaf nodes differ in that they contain an output value which is used to make the prediction. After having trained the tree, it is very easy to make predictions, it is enough to traverse the tree with the input data to label.=,=p>>To train the decision trees recursively, the input space is divided binary using a greedy algorithm which consists of trying different splits using a cost function and staying with the one that minimizes it.=,=p>>In regression problems the cost function to be minimized is the sum of quadratic error.=,=img >>pds0.jpg>>35%>>75=,=br=,=br=,=p>>In classification problems, the Gini index is used, which indicates how pure the leaf nodes are.=,= img>>aa1.jpg>>35%>>75=,=br=,=br=,=p>>The most common recursive algorithm brake criterion is to use a minimum number of instances assigned to each leaf node, when at performing a splir encounters a number less than that does not and that node is designated a leaf. The more nodes per leaf, the more generic the tree is and having fewer is more specific, which can tend to overfitting.=,=p>>Next, an exercise will be carried out in a spreadsheet that shows how to perform a partition based on the Gini index to build a simple decision tree that will later be used to make predictions.=,=p>>- The input dataset was entered and the data was plotted, differentiating the classes Y = 0 and Y = 1. =,= img>>UT4-PD1-1.jpg>>100%>>200=,=p>> - The variable X1 was used to build the CART model with a value for the division point of X1 = 2.771244718 calculating the Gini index of the model produced. =,=p>> - The previous process was repeated with a division point of X1 = 6.642287351 and the value of the Gini coefficient of the generated CART was obtained, as well as a description of the generated decision tree.=,=img>>aa2.jpg>>100%>>400=,=p>> The CART produced was used to make predictions of a test dataset and the accuracy was calculated.=,=img>>aa3.jpg>>50%>>300=,=br=,=br=,=p>> The worksheets in which the explained steps were carried out are accessible for download with the following links:=,=a>>Download training template>>Nonlinear algorithms PD1- 1.xlsx>>d=,=br=,=a>>Download Predictions Template>>Nonlinear Algorithms PD1-2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "unit":"Non linear algorithms",
        "titulo":"Árboles de decisión en KNIME",  
        "title":"Decision Trees in KNIME",      
        "descripcion":"Estudiaremos la construcción de un modelo de árbol de decisión de regresión simple en la herramienta KNIME.",
        "description":"We will study the construction of a simple regression decision tree model in the KNIME tool.",
        "contenido":"img>>UT4-PD2-1.jpg>>100%>500=,=br=,=br=,=h3>>Análisis=,=br=,=h5>>Descripción=,=p>> Este workflow utiliza el dataset Iris dividiéndolo con muestreo estratíficado en una parte de entrenamiento de un 80% y otra de prueba de un 20% para el caso de un modelo de árbol de regresión simple del cual se obtiene la performance y se la visualiza a través de una es gráfica. En este modelo a partir del ancho y el largo del sépalo, el largo de los pétalos y la clase de una flor entre Iris-setosa, Iris-versicolor y Iris virgínica predice el ancho de sus pétalos. =,=h5>> Componentes =,=h6>>File reader=,=p>>El operador equivalente a FileReader en RapidMiner es Retrieve. Estos se diferencian en que el primero deja editar características del dataset luego de ser incluido en el proyecto como los tipos para las variables y en RM esto se configura solo al importar el dataset. En el dataset sepal length, sepal width, petal length y pethal width son doubles y class es una string.=,=img>>UT4-PD2-2.jpg>>50%>>650=,=br=,=br=,=h6>>Partitioning=,=p>> El operador partitioning ofrece elegir el tamaño de la partición y la forma en la que se quiere separar determinando por ejemplo si se eligen los primeros valores o si se hace muestreo estartíficado, da la opción para usar una seed como en RM, permite alternativas acerca de las Flow Variables que funcionan para a hacer variar ciertas configuraciones en el nodo de forma dinámica con cada ejecución y deja a elección políticas sobre el uso de la memoria. =,=p>> El operador equivalente a Patitioning en RapidMiner es Split. La principal diferencia es que mientras que en el componente de KNIME se hace una partición por cada unidad del componente, en RM se permite realizar varias a la vez.=,=img>>UT4-PD2-3.jpg>>50%>>50=,=br=,=br=,=h6>>Simple Regression Tree Learner.=,=p>> El proceso utiliza un algoritmo base de árbol de regresión simple siguiendo el algoritmo descripto en “Classification and Regression Tress” (Breiman et al, 1984) con algunas simplifaciones como no pruning, no necesariamente árboles binarios, tratar de encontrar la mejor dirección para los missing values, etc. En estos árboles de regesión el valor de predicción el valor para cada nodo hoja es la media de los registros dentro de ella y la predicción mejora cuanto menor sea la varianza de los valores dentro de una hoja. Por lo tanto, para armarlo en cada nodo se hacen splits que minimicen la suma de errores cuadráticos de los hijos. El operador Simple Regression Tree Learner soporta predictores de tipo numérico y categórico aunque solo soporta target columns de tipo numérico.=,=p>> Los parámetros que se pueden configurar del algoritmo son determinar el uso o no splits binarios para los atributos nominales, la forma en la que se manejan los missing values siendo XGBoost un algoritmo que calcula la mejor dirección para los valores faltantes y Surrogate que calcula para cada Split otros alternativos que mejoran la aproximación, el límite para la profundidad del árbol, el mínimo de valores que puede tener un nodo para que el Split se intente y el mínimo de registros que un nodo hijo puede tener.=,=img>>UT4-PD2-4.jpg>>50%>>800=,=br=,=br=,=h6>>Simple Regression Tree Predictor=,=p>> Este operador recibe por un lado el modelo entrenado y por otro los datos de test, cómo salida tiene las predicciones realizadas. Permite modificar manualmente la columna de predicción, utlizar Flow Variables y decidir sobre políticas de memoria. =,=img>>UT4-PD2-5.jpg>>50%>>300=,=br=,=br=,=h6>>Line Plot=,=p>> Este componente muestra una gráfica que compara los valores de predicción con la salida conocida para esos inputs del dataset de entrenamiento.=,=p>> Los parámetros que se pueden editar son el número de filas a mostrar, el límite del valor nominal a partir del cual una columna sea ignorada y la opción de colocar Flow Variables.=,=img>>UT4-PD2-6.jpg>>50%>>300=,=br=,=br=,=img>>UT4-PD2-7.jpg>>70%>>300=,=br=,=br=,=h6>>Numeric Scorer=,=p>> El operador funciona realizando los cálculos para los valores de coeficiente de terminación, media de error absoluto, error cuadrático medio y desviación media con signo de la predicción realizada.=,=img>>UT4-PD2-8.jpg>>50%>>400=,=br=,=br=,=img>>UT4-PD2-9.jpg>>30%>>125",
        "content":"img>>UT4-PD2-1.jpg>>100%>500=,=br=,=br=,=h3>>Analysis=,=br=,=h5>>Description=,=p>> This workflow uses the Iris dataset dividing it with stratified sampling into a training part of 80% and a test part of 20% for the case of a simple regression tree model from which the performance is obtained and it is displayed through a graph. In this model from the width and length of the sepal, the length of the petals and the class of a flower between Iris-setosa, Iris-versicolor and Iris virginica predicts the width of its petals. =,=h5>> Components =,=h6>>File reader=,=p>>The operator equivalent to FileReader in RapidMiner is Retrieve. These differ in that the first one allows dataset characteristics to be edited after being included in the project, such as the types for the variables, and in RM this is configured only when importing the dataset. In the dataset sepal length, sepal width, petal length and pethal width are doubles and class is a string.=,=img>>UT4-PD2-2.jpg>>50%>>650=,=br=,=br =,=h6>>Partitioning=,=p>> The partitioning operator offers to choose the size of the partition and the way in which it is wanted to be separated, determining for example if the first values ​​are chosen or if statified sampling is done, it gives the option to use a seed as in RM, allows alternatives about the Flow Variables that work to vary certain configurations in the node dynamically with each execution and leaves to choose policies on the use of memory. =,=p>> The equivalent operator to Patitioning in RapidMiner is Split. The main difference is that while in the KNIME component a partition is made for each unit of the component, in RM several are allowed at the same time.=,=img>>UT4-PD2-3.jpg>>50%>>400=,=br=,=br=,=h6>>Simple Regression Tree Learner.=,=p>> The process uses a simple regression tree base algorithm following the algorithm described in “Classification and Regression Tress” ( Breiman et al, 1984) with some simplifications like no pruning, not necessarily binary trees, trying to find the best direction for missing values, etc. In these regression trees, the prediction value for each leaf node is the mean of the records within it, and the prediction improves the smaller the variance of the values ​​within a leaf. Therefore, to assemble it in each node, splits are made that minimize the sum of squared errors of the children. The Simple Regression Tree Learner operator supports numeric and categorical type predictors, although it only supports numeric type target columns.=,=p>> The parameters that can be configured for the algorithm are to determine whether or not to use binary splits for nominal attributes, the way in which missing values ​​are handled, XGBoost being an algorithm that calculates the best address for missing values ​​and Surrogate that calculates for each Split other alternatives that improve the approximation, the limit for the depth of the tree, the minimum of values ​​that can have a node for the split to try and the minimum number of records a child node can have.=,=img>>UT4-PD2-4.jpg>>50%>>800=,=br=,=br=,=h6>>Simple Regression Tree Predictor=,=p>> This operator receives the trained model on the one hand and the test data on the other, as output it has the predictions made. It allows manually modifying the prediction column, using Flow Variables and deciding on memory policies. =,=img>>UT4-PD2-5.jpg>>50%>>300=,=br=,=br=,=h6>>Line Plot=,=p>> This component displays a plot comparing the prediction values ​​with the known output for those training dataset inputs.=,=p>> The parameters that can be edited are the number of rows to display, the nominal value limit from which a column is ignored, and the option to place Flow Variables.=,=img>>UT4-PD2-6.jpg>>50%>>300=,=br=,=br=,=img>>UT4-PD2-7.jpg>>70%>>300=,=br=,=br=,=h6>>Numeric Scorer=,=p>> The operator works by performing calculations for the values ​​of completion coefficient, mean absolute error, mean square error, and deviation signed mean of the prediction made.=,=img>>UT4-PD2-8.jpg>>50%>>400=,=br=,=br=,=img>>UT4-PD2-9.jpg>>30%>>125"
    },
    {
        "unidad":"Algoritmos no lineales",
        "unit":"Non linear algorithms",
        "titulo":"Comparación de Decision Trees en herramientas de ML",
        "title":"Comparison of Decision Trees in ML Tools",
        "descripcion":"Comparación de árboles de decisión entre RapidMiner y Weka aplicados al dataset Iris. Además se analiza el componente mencionado en Azure ML Studio, KNIME y Python Sci kit learn.",
        "description":"Comparison of decision trees between RapidMiner and Weka applied to the Iris dataset. In addition, the mentioned component is analyzed in Azure ML Studio, KNIME and Python Sci kit learn.",
        "contenido":"p>>Se utiliza el dataset Iris y las herramientas RapidMiner y Weka para hacer una comparación de la performance de sus modelos de árboles de decisión midiéndola con un Split de datos de un 70% y un 30% para entrenamiento y test respectivamente en ambas.=,=h4>>RapidMiner=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. C4.5=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión. =,=h6>>Parámetros=,=p>>- Criterion: gain_ratio. Selecciona el criterio que se usará para seleccionar los atributos sobre los que hacer los splits.=,=p>>- Maximal Depth: 10. La máxima profundidad del árbol.=,=p>>- Apply pruning: Activado. Si se aplica pruning o no.=,=p>>- Confidence: 0.1. Nivel de confianza usado para el error pesimista del cálculo de pruning.=,=p>>- Apply prepruning: Activado. Si se aplica prepruning o no=,=p>>- Minimal gain: 0.01. La ganancia de un nodo se calcula antes del Split. Se hace el Split si la ganancia es mayor al minimal gain.=,=p>>- Minimal leaf size: 2. Tamaño mínimo de observaciones por hoja.=,=p>>- Minimal size for split: 4. El tamaño de un nodo es el número de ejemplos en él. Solo se hace el Split para obtener nodos con un tamaño mayor al minimal size for Split.=,=p>>- Number of alternatives for pruning: 3. Numero de nodos alternativos testeados para un Split cuando el prepruning previene un Split.=,=br=,=h4>>Weka=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. C4.5.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión.=,=h6>>Parámetros=,=p>>- batchSize: 100. Numero de instancias a procesar si la predicción batch está siendo realiza.=,=p>>- Debug: False. Si es verdadero el clasificador podría poner info adicional como salida en consola.=,=p>>- doNotCheckCapabilities: False. Si es verdadero las capacidades del clasificador no son checkeadas antes de la compilación=,=p>>- initialCount: 0.0. Valor inicial del contador de la clase.=,=p>>- MaxDepth: 10. Máxima profundidad del árbol, con -1 no hay restricción.=,=p>>- MinNum: 2.0. Mínimo peso total para las instancias de una hoja.=,=p>>- minVariancePrep: 0.001. Mínima proporción de la varianza de todos los datos que necesita estar en un nodo para que se haga un Split.=,=p>>- noPruning. False. Si se realiza pruning.=,=p>>- numDecimalPlaces. 2. Número de posiciones decimales para usar en la salida del modelo.=,=p>>- numFolds: 3. Determina el tamaño de los datos usados para pruning.=,=p>>- Seed. 1. La semilla usada para la aleatoriedad de los datos.=,=p>>- spreadIntialCount. False. Distribuir el recuento inicial en todos los valores en lugar de utilizar el recuento por valor.=,=br=,=h4>>Resultados=,=p>>Se obtuvo una performance de un 91.11% en el caso de RapidMiner y un 96% en Weka utilizando los parámetros que se muestran en la sección anterior para cada uno.=,=h6>>RapidMiner=,=img>>UT4-PD5-1.jpg>>100%>>200=,=br=,=br=,=h6>>Weka=,=img>>UT4-PD5-2.jpg>>70%>>800=,=br=,=br=,=h4>>Otras herramientas=,=br=,=h5>>Azure Machine Learning Studio=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. Se utiliza una versión mejorada de los árboles de decisión con una gradient boosting machine.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión.=,=h6>>Parámetros=,=p>>- Maximum number of leaves per tree. Número máximo de hojas del árbol=,=p>>- Minimum number of samples per leaf node. Mínimo número de ejemplos en un nodo hoja.=,=p>>- Learning rate. Ritmo de aprendizaje.=,=p>>- Total num trees constructed. Número de árboles construidos al entrenar el algoritmo.=,=p>>- Random number seed. Semilla de entrenamiento.=,=p>>- Allow unknown categorical levels. Seleccionado crea un nuevo nivel para cada atributo categórico.=,=br=,=h5>>KNIME=,=p>>Tipo de problema. Clasificación.=,=p>>Algoritmo base. C4.5.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden solo ser numéricas o nominales. La variable objetivo solo puede ser nominal.=,=h6>>Parámetros=,=p>>- Class column. Selecciona la variable objetivo.=,=p>>- Quality measure. Para seleccionar la medida de calidad para la cual se calcularan los Splits. Las opciones son Gini index y Gain Ratio.=,=p>>- Pruning method. Pruning reduce el tamaño del árbol y evita el overfitting.=,=p>>- Reduced error pruning. Si se checkea se usa un simple método de pruning.=,=p>>- Min number records per node. Mínimo numero de registros requeridos por nodo.=,=p>>- Number records to store for view. Selecciona el número de registros guardados en un tree para la view.=,=p>>-  Average Split point. Al checkearla el valor para Split con atributos numéricos se determina según la media de los valores que separan a las dos particiones.=,=p>>- Number threads. Permite multiprocesamiento.=,=p>>- Skip nominal columns without domain information. Seleccionada las columnas nominales que no información de valores de dominio se saltean.=,=p>>- Force root split column. Seleccionada, el primer split es calculado en la columna elegida sin evaluar ninguna otra para posibles splits.=,=p>>- Binary nominal splits. Al seleccionarla a los atributos nominales se les hacen splits binarios.=,=p>>- Max nominal. Número máximo de valores nominales.=,=p>>- Filter invalid attribute values in child nodes. Habilitando esta opción se hace un post procesamiento del tree y se filtran checkeos inválidos.=,=p>>- No true child strategy: Opciones para cuando el valor de los atributos de un nodo es desconocido.=,=p>>- Missing value strategy. Opciones para los valores faltantes.=,=br=,=h5>>Python Sci kit learn=,=p>>Tipo de problema. Clasificación y regresión=,=p>>Algoritmo base. CART=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden solo ser numéricas. La variable objetivo puede ser nominal o numérica.=,=h6>>Parámetros=,=p>>- Criterio. Función para medir la calidad del Split. Puede ser Gini o entropy=,=p>>- Splitter. Estrategia utilizada para elegir el Split en cada nodo. Las opciones son mejor o random.=,=p>>- Max Depth. Máxima profundidad del árbol.=,=p>>- Min samples. Mínimo de ejemplos requeridos para hacer un Split generando un nodo interno.=,=p>>- Min samples leaf. Mínimo de ejemplos requeridos para hacer un Split generando un nodo hoja.=,=p>>- Min weight fraction. Fracción de peso mínimo del total de peso requerido en un nodo hoja.=,=p>>- Max features. Máximo número de variables de entrada considerado para hacer el mejor Split.=,=p>>- Random_state. Controla la aleatoriedad del estimador.=,=p>>- Max_leaf_node. Máximo número de nodos hoja.=,=p>>- Min_impurity_decrease: Se le hará un Split a un nodo si el Split da un decenso de la impureza mayor o igual a este valor.=,=p>>- Class_weight: Pesos asociados a las clases.=,=p>>- Ccp_alpha: Parámetro de complehidad usado para el pruning de mínimo costo-complejidad.",
        "content":"p>>The Iris dataset and the RapidMiner and Weka tools are used to compare the performance of their decision tree models, measuring it with a data split of 70% and 30% for training and testing, respectively, in both. =,=h4>>RapidMiner=,=p>>Type of problem. Classification and regression.=,=p>>Base algorithm. C4.5=,=p>>Required characteristics of attributes and label. Input variables can be numeric or nominal. A nominal objective variable is required for classification and numerical for regression. =,=h6>>Parameters=,=p>>- Criterion: gain_ratio. Select the criteria that will be used to select the attributes on which to make the splits.=,=p>>- Maximal Depth: 10. The maximum depth of the tree.=,=p>>- Apply pruning: Activated. If pruning is applied or not.=,=p>>- Confidence: 0.1. Confidence level used for the pessimistic error of the pruning calculation.=,=p>>- Apply prepruning: Activated. If prepruning is applied or not=,=p>>- Minimal gain: 0.01. The gain of a node is calculated before the split. Split is done if the gain is greater than minimal gain.=,=p>>- Minimal leaf size: 2. Minimum size of observations per leaf.=,=p>>- Minimal size for split: 4. The size of a node is the number of instances in it. The Split is only done to obtain nodes with a size greater than the minimal size for Split.=,=p>>- Number of alternatives for pruning: 3. Number of alternative nodes tested for a Split when prepruning prevents a Split.=,=br=,=h4>>Weka=,=p>>Type of problem. Classification and regression.=,=p>>Base algorithm. C4.5.=,=p>>Required characteristics of attributes and label. Input variables can be numeric or nominal. A nominal objective variable is required for classification and numerical for regression.=,=h6>>Parameters=,=p>>- batchSize: 100. Number of instances to process if the batch prediction is being performed.=,=p>>-Debug:False. If true the classifier could put additional info as console output.=,=p>>- doNotCheckCapabilities: False. If true classifier capabilities are not checked before compilation=,=p>>- initialCount: 0.0. Initial value of the class counter.=,=p>>- MaxDepth: 10. Maximum depth of the tree, with -1 there is no restriction.=,=p>>- MinNum: 2.0. Minimum total weight for the instances of a sheet.=,=p>>- minVariancePrep: 0.001. Minimum proportion of the variance of all the data that needs to be in a node for a Split to be done.=,=p>>- noPruning. False. If pruning is done.=,=p>>- numDecimalPlaces. 2. Number of decimal places to use in the model output.=,=p>>- numFolds: 3. Determines the size of the data used for pruning.=,=p>>- Seed. 1. The seed used for randomization of the data.=,=p>>- spreadIntialCount. False. Distribute the initial count over all values ​​instead of using count per value.=,=br=,=h4>>Results=,=p>>A performance of 91.11% was obtained in the case of RapidMiner and 96 % in Weka using the parameters shown in the previous section for each.=,=h6>>RapidMiner=,=img>>UT4-PD5-1.jpg>>100%>>200=,=br=,=br=,=h6>>Weka=,=img>>UT4-PD5-2.jpg>>70%>>800=,=br=,=br=,=h4>>Other tools=,=br=,=h5>>Azure Machine Learning Studio=,=p>>Issue type. Classification and regression.=,=p>>Base algorithm. An improved version of decision trees with a gradient boosting machine is used.=,=p>>Required attributes and label features. Input variables can be numeric or nominal. A nominal objective variable is required for classification and numerical for regression.=,=h6>>Parameters=,=p>>- Maximum number of leaves per tree. Maximum number of tree leaves=,=p>>- Minimum number of samples per leaf node. Minimum number of examples in a leaf node.=,=p>>- Learning rate. Learning rate.=,=p>>- Total num trees constructed. Number of trees built when training the algorithm.=,=p>>- Random number seed. Training seed.=,=p>>- Allow unknown categorical levels. Selected creates a new level for each categorical attribute.=,=br=,=h5>>KNIME=,=p>>Issue type. Classification.=,=p>>Base algorithm. C4.5.=,=p>>Required characteristics of attributes and label. Input variables can only be numeric or nominal. The target variable can only be nominal.=,=h6>>Parameters=,=p>>- Class column. Select the target variable.=,=p>>- Quality measure. To select the quality measure for which Splits will be calculated. The options are Gini index and Gain Ratio.=,=p>>- Pruning method. Pruning reduces the size of the tree and avoids overfitting.=,=p>>- Reduced pruning error. If checked, a simple pruning method is used.=,=p>>- Min number records per node. Minimum number of records required per node.=,=p>>- Number records to store for view. Select the number of records stored in a tree for the view.=,=p>>- Average Split point. When checking it, the value for Split with numeric attributes is determined according to the average of the values ​​that separate the two partitions.=,=p>>- Number threads. Allows multiprocessing.=,=p>>- Skip nominal columns without domain information. Selected nominal columns that no domain value information is skipped.=,=p>>- Force root split column. When checked, the first split is calculated on the chosen column without evaluating any other for possible splits.=,=p>>- Binary nominal splits. When selecting it, the nominal attributes are split binary.=,=p>>- Max nominal. Maximum number of nominal values.=,=p>>- Filter invalid attribute values ​​in child nodes. Enabling this option makes a post processing of the tree and invalid checks are filtered.=,=p>>- No true child strategy: Options for when the value of the attributes of a node is unknown.=,=p>>- Missing value strategy. Options for missing values.=,=br=,=h5>>Python Sci kit learn=,=p>>Type of problem. Classification and regression=,=p>>Base algorithm. CART=,=p>>Required characteristics of attributes and label. Input variables can only be numeric. The target variable can be nominal or numerical.=,=h6>>Parameters=,=p>>- Criterion. Function to measure the quality of the Split. It can be Gini or entropy=,=p>>- Splitter. Strategy used to choose the Split at each node. Options are best or random.=,=p>>- Max Depth. Maximum depth of the tree.=,=p>>- Min samples. Minimum of examples required to make a Split generating an internal node.=,=p>>- Min samples leaf. Minimum of examples required to make a Split generating a leaf node.=,=p>>- Min weight fraction. Minimum weight fraction of the total weight required in a leaf node.=,=p>>- Max features. Maximum number of input variables considered to make the best Split.=,=p>>- Random_state. Controls the randomness of the estimator.=,=p>>- Max_leaf_node. Maximum number of leaf nodes.=,=p>>- Min_impurity_decrease: A node will be split if the split gives a decrease in impurity greater than or equal to this value.=,=p>>- Class_weight: Associated weights to classes.=,=p>>- Ccp_alpha: Complexity parameter used for minimum cost-complexity pruning."
    },
    {
        "unidad":"Algoritmos no lineales",
        "unit":"Non linear algorithms",
        "titulo":"SVM lineal con descenso de sub gradiente",       
        "title":"Linear SVM with Sub-Gradient Descent", 
        "descripcion":"En una hoja de cálculo analizaremos el algoritmo de Support Vector Machines lineal con descenso de sub gradiente y lo utilizaremos para realizar predicciones.",
        "description":"In a spreadsheet, we will discuss the Sub-Gradient Descent Linear Support Vector Machines algorithm and use it to make predictions.",
        "contenido":"p>>Support Vector machines es uno de los algoritmos de machine learning más populares. Este modelo se basa en la definición de hiperplanos n dimensionales, siendo n el número de variables de entrada, para dividir los puntos ejemplos en dos clases aceptando solo variables de predicción binomiales de forma tal de que se maximice el margen de la separación entre clases. De todas maneras se puede permitir dejar que algunos puntos violen la línea de separación a partir de un coeficiente C se determina la magnitud de este fenómeno.=,=p>>La división de las clases en SVM depende de los llamados kernels. Estos determinan el tipo de hiperplano con el que se dividirá el espacio n dimensinal. Algunos ejemplos son lineal, polinomial, radial, etc.=,=p>>Proximamente estudiaremos un caso de SVM dividiendo un espacio bidimensional al tener solamente las variables de entrada x1 y x2, utilizando un kernel lineal formando así nada más ni nada menos que una recta que se deberá definir.  =,=p>>- Se graficaron los datos en dos series para Y=1 y para Y=-1.=,=img>>UT4-PD3-1.jpg>>70%>>250=,=br=,=br=,=p>>- Para hallar los coeficientes B1 y B2 del modelo SVM lineal B0 + B1 x X1 + B2 x X2 = 0 se utilizó el método del descenso de sub-gradiente. El coeficiente B0 fue descartado por lo cual la recta resultante pasa por el origen.=,=p>>Para aplicar este algoritmo se comienza con los coeficientes B1 y B2 en 0 y posteriormente se calcula un primer valor de salida con la fórmula dispuesta a continuación.=,=img>>UT4-PD3-2.jpg>>50%>>50=,=p>>Si el valor de salida es mayor a 1 el patrón de entrenamiento no es un vector de soporte y por lo tanto se aplica la siguiente función para b1 y b2=,=img>>UT4-PD3-3.jpg>>20%>>75=,=p>>En caso contrario se aplica la siguiente fórmula sobre los valores de los vectores utilizando en este caso lambda=0.45 y donde t es la iteración actual=,=img>>UT4-PD3-4.jpg>>70%>>75=,=p>>- Este procedimiento fue realizado por 16 épocas en las que para cada una se itera sobre todo el dataset.=,=p>>- Se realizó una gráfica de la exactitud en función de las épocas obteniendo el siguiente resultado.=,=img>>UT4-PD3-5.jpg>>70%>>350=,=br=,=br=,=p>>- Tras todas las iteraciones los coeficientes obtenidos fueron B1=0.55 Y B2=-0.72 obteniendo el plano 0.55xX1 -0.72xX2 = 0. Este se utilizó para calcular las predicciones con los datos de entrenamiento y se obtuvo un 100% de exactitud.=,=img>>aa7.jpg>>50%>>200=,=br=,=br=,=a>>Descargar planilla de trabajo>>Algoritmos no lineales PD3.xlsx>>d",
        "content":"p>>Support Vector machines is one of the most popular machine learning algorithms. This model is based on the definition of n-dimensional hyperplanes, where n is the number of input variables, to divide the example points into two classes accepting only binomial prediction variables in such a way as to maximize the margin of separation between classes. In any case, it is permissible to allow some points to violate the line of separation. Starting from a coefficient C, the magnitude of this phenomenon is determined.=,=p>>The division of classes in SVM depends on the so-called kernels. These determine the type of hyperplane with which the n-dimensional space will be divided. Some examples are linear, polynomial, radial, etc.=,=p>>Soon we will study a case of SVM dividing a two-dimensional space by having only the input variables x1 and x2, using a linear kernel thus forming nothing more and nothing less than a line to be defined. =,=p>>- The data was plotted in two series for Y=1 and for Y=-1.=,=img>>UT4-PD3-1.jpg>>70%>>250=,=br=,=br=,=p>>- To find the coefficients B1 and B2 of the linear SVM model B0 + B1 x X1 + B2 x X2 = 0, the sub-gradient descent method was used. Coefficient B0 was discarded, so the resulting straight line passes through the origin.=,=p>>To apply this algorithm, start with coefficients B1 and B2 at 0 and then calculate a first output value with the formula provided below continuation.=,=img>>UT4-PD3-2.jpg>>50%>>50=,=p>>If the output value is greater than 1 the training pattern is not a support vector and therefore Therefore, the following function is applied to b1 and b2=,=img>>UT4-PD3-3.jpg>>20%>>75=,=p>>Otherwise, the following formula is applied to the values ​​of the vectors using in this case lambda=0.45 and where t is the current iteration=,=img>>UT4-PD3-4.jpg>>70%>>75=,=p>>- This procedure was performed for 16 epochs in the which for each one is iterated over the entire dataset.=,=p>>- A graph of the accuracy was made as a function of the epochs, obtaining the following result.=,=img>>UT4-PD3-5.jpg>>70%>>350=,=br=,=br=,=p>>- After all the iterations, the coefficients obtained were B1=0.55 and B2 =-0.72 obtaining the plane 0.55xX1 -0.72xX2 = 0. This was used to calculate the predictions with the training data and 100% accuracy was obtained.=,=img>>aa7.jpg>>50%>> 200=,=br=,=br=,=a>>Download worksheet>>Nonlinear algorithms PD3.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "unit":"Non linear algorithms",
        "titulo":"SVM no lineal en RapidMiner",
        "title":"Nonlinear SVM in RapidMiner",
        "descripcion":"En este ejercicio se analizará el componente SVM de RapidMiner y se lo utilizará para resolver un problema no separable linealmente.",
        "description":"In this exercise you will explore the SVM component of RapidMiner and use it to solve a linearly non-separable problem.",
        "contenido":"p>>El operador SVM utiliza la implementación de Java de support vector machine mySVM de Stefan Rueping pudiendo ser usado para regresión y clasificación. Este fue empleado para resolver un problema no separable linealmente primero con los parámetros por defecto y luego se lo resolvió modificando el kernel a polinomial.=,=p>> A continuación se describen y muestran los valores de los parámetros utilizados del operador y luego los resultados de performance obtenidos para ambos casos. =,=h5>>Parámetros=,=p>>- Kernel type: Tipo de función kernel a utilizar en el algoritmo. Valor inicial dot, final polynomial.=,=p>>- Kernel cache: Fija el tamaño de cache para las evaluaciones kernel. Valor: inicial 200, final 200.=,=p>>- C: Es una constante de complejidad que fija la tolerancia a clasificación errónea, cuando más alta más suaves son los límites y cuanto baja estos son más duros. Si es demasiado alta puede dar overfitting y si es muy baja puede dar over generalization. Valor: inicial 0.0, final 0.0.=,=p>>- Convergence épsilon: Especifica la precisión de las KKT conditions. Valor: inicial 0.01, final 0.01.=,=p>>- Max iterations: Número máximo de iteraciones. Valor: inicial 100000, final 100000.=,=p>>- Scale: Si está activado los valores se escalan. Valor: inicial activado, final activado.=,=p>>- Lpos: Factor para constante de complejidad del SVM caso positivos. Valor: inicial 1.0, final 1.0.=,=p>>- Lneg: Factor para constante de complejidad del SVM caso negativos. Valor: inicial 1.0, final 1.0.=,=p>>- Épsilon: Constante de insensibilidad. Valor: inicial 0.0, final 0.0.=,=p>>- Épsilon plus: Parámetro parte de la función de pérdida. Valor: inicial 0.0, final 0.0.=,=p>>- Épsilon minus: Parámetro parte de la función de pérdida. Valor: inicial 0.0, final 0.0.=,=p>>- Balance cost: Adapta Cpos y Cneg al tamaño relative de las clases. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss pos: Usa pérdida cuadrática para desviación positiva. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss neg: Usa pérdida cuadrática para desviación negativa. Valor: inicial desactivado, final desactivado.=,=br=,=h5>>Resultados=,=br=,=h6>>Caso inicial=,=img>>UT4-PD4-1.jpg>>100%>>150=,=br=,=br=,=h6>>Caso final=,=img>>UT4-PD4-2.jpg>>100%>>150=,=br=,=br=,=a>>Descargar proceso de RapidMiner>>UT4-TA7.rmp>>d",
        "content":"p>>The SVM operator uses the Java implementation of support vector machine mySVM by Stefan Rueping being able to be used for regression and classification. This was used to solve a linearly non-separable problem first with the default parameters and then it was solved by modifying the kernel to polynomial.=,=p>> The values ​​of the used parameters of the operator are described and shown below and then the performance results obtained for both cases. =,=h5>>Parameters=,=p>>- Kernel type: Type of kernel function to use in the algorithm. Initial value dot, final polynomial.=,=p>>- Kernel cache: Sets the size of the cache for kernel evaluations. Value: initial 200, final 200.=,=p>>- C: It is a complexity constant that fixes the tolerance to erroneous classification, the higher it is, the softer the limits are and the lower it is, they are harder. If it is too high it can give overfitting and if it is too low it can give over generalization. Value: initial 0.0, final 0.0.=,=p>>- Convergence epsilon: Specifies the precision of the KKT conditions. Value: initial 0.01, final 0.01.=,=p>>- Max iterations: Maximum number of iterations. Value: initial 100000, final 100000.=,=p>>- Scale: If activated, the values ​​are scaled. Value: initial activated, final activated.=,=p>>- Lpos: Factor for the complexity constant of the SVM in positive cases. Value: initial 1.0, final 1.0.=,=p>>- Lneg: Factor for the complexity constant of the SVM in negative cases. Value: initial 1.0, final 1.0.=,=p>>- Epsilon: Insensitivity constant. Value: initial 0.0, final 0.0.=,=p>>- Epsilon plus: Parameter part of the loss function. Value: initial 0.0, final 0.0.=,=p>>- Epsilon minus: Parameter part of the loss function. Value: initial 0.0, final 0.0.=,=p>>- Balance cost: Adapts Cpos and Cneg to the relative size of the classes. Value: start off, end off.=,=p>>- Quadratic loss pos: Use quadratic loss for positive offset. Value: start off, end off.=,=p>>- Quadratic loss neg: Use quadratic loss for negative offset. Value: start off, end off.=,=br=,=h5>>Results=,=br=,=h6>>Initial Case=,=img>>UT4-PD4-1.jpg>>100%>> 150=,=br=,=br=,=h6>>Final case=,=img>>UT4-PD4-2.jpg>>100%>>150=,=br=,=br=,=a>>Download RapidMiner process>>UT4-TA7.rmp>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "unit":"Non linear algorithms",
        "titulo":"Naive Bayes en planilla electrónica",
        "title":"Naive Bayes in spreadsheet",
        "descripcion":"Implementación de un modelo de Naive Bayes en una planilla electrónica.",
        "description":"Implementation of a Naive Bayes model in an electronic spreadsheet.",
        "contenido":"p>>El modelo Naive Bayes para clasificación en problemas binarios o multiclase se caracteriza por asumir que los atributos son independientes entre sí (lo cual rara vez acontece) y en basarse en el teorema de Bayes.=,=h6>>Teorema de Bayes=,=p>>Este es utilizado para calcular probabilidad de una hipótesis dado un suceso de la siguiente manera.=,=img>>UT4-PD6-1.jpg>>40%>>100=,=p>>Siendo=,=p>>- P(h|d) la probabilidad de la hipótesis h dado el susceso d (conocida como la probabilidad a posteriori).=,=p>>- P(d|h) la probabilidad del suceso d dado que la hipótesis h sea cierta.=,=p>>- P(h) la probabilidad de que la hipótesis h sea cierta (conocida como la probabilidad a priori).=,=p>>- P(d) la probabilidad de que suceda el suceso d más allá de que se cumpla h o no.=,=p>>En Naive Bayesian se calcula las probabilidades a priori para cada clase, las probabilidades para cada combinación entre un valor de un atributo y una clase, se emplea el Teorema de Bayes para obtener la probabilidad a posteriori y finalmente se clasifica según la clase que produjo el valor más elevado.=,=p>>Ahora mostraremos el comportamiento del modelo Naive Bates empleándolo para el dataset Jugar Tenis prediciendo si se juega o no al tenis en condiciones metereológicas particulares definidas por los atributos de los datos.=,=h5>>Dataset=,=img>>UT4-PD6-2.jpg>>70%>>300=,=br=,=br=,=p>>Se calcularon las siguientes probabilidades para generar el modelo tal y como se describió anteriormente pero para este caso particular.=,=img>>UT4-PD6-3.jpg>>80%>>450=,=br=,=br=,=p>>Posteriormente se utilizó el modelo generado para realizar las predicciones que se encuentran a continuación.=,=img>>UT4-PD6-4.jpg>>90%>>100",
        "content":"p>>The Naive Bayes model for classification in binary or multiclass problems is characterized by assuming that the attributes are independent of each other (which rarely happens) and based on in Bayes theorem.=,=h6>>Bayes theorem=,=p>>This is used to calculate probability of a hypothesis given an event as follows.=,=img>>UT4-PD6-1.jpg>>40%>>100=,=br=,=br=,=p>>Being=,=p>>- P(h|d) is the probability of hypothesis h given event d (known as the posterior probability).=,= p>>- P(d|h) the probability of event d given that hypothesis h is true.=,=p>>- P(h) the probability that hypothesis h is true (known as the prior probability ).=,=p>>- P(d) the probability of event d happening beyond s e meets ho no.=,=p>>In Naive Bayesian, the prior probabilities for each class are calculated, the probabilities for each combination between a value of an attribute and a class, Bayes' Theorem is used to obtain the probability a posteriori and finally it is classified according to the class that produced the highest value.=,=p>>Now we will show the behavior of the Naive Bates model using it for the Play Tennis dataset predicting whether or not tennis is played in particular weather conditions defined by the data attributes.=,=h5>>Dataset=,=img>>UT4-PD6-2.jpg>>70%>>300=,=br=,=br=,=p>>The following were calculated probabilities to generate the model as described above but for this particular case.=,=img>>UT4-PD6-3.jpg>>80%>>450=,=br=,=br=,=p>>Later, the generated model was used to make the following predictions.=,=img>>UT4-PD6-4.jpg>>90%>>100"
    },
    {
        "unidad":"Algoritmos no lineales",
        "unit":"Non linear algorithms",
        "titulo":"KNN en hoja de cálculo y RapidMiner",
        "title":"KNN in Sheet calculation and RapidMiner",
        "descripcion":"Empleo de KNN para realizar predicciones en una hoja de cálculo y RapidMiner.",
        "description":"Using KNN to make predictions in a spreadsheet and RapidMiner.",
        "contenido":"p>>El algoritmo para clasificación y regresión K-Nearest Neighbors se diferencia de los demás en que no es un modelo aprendido sino que se realizan predicciones a partir del dataset de entrenamiento directamente. El procedimiento consiste en que para cada punto a predecir se busca entre las instancias más similares en base a medidas como distancia euclideana, Hamming, Manhattan, Jaccard, Minkowski, etc.=,=p>>Para utilizar KNN para predecir basta con elegir un valor de k y una medida de distancia que serán utilizados para hallar los k puntos más cercanos de entre los que están en los datos de entrenamiento respecto al ejemplo que se quiere predecir. A partir de allí para clasificación se compara las clases de estos k puntos y la más repetida será la designada como predicción y para regresión se asigna el promedian de la salida de los k valores más cercanos. Cabe destacacr que es importante normalizar si las escalas de los datos se encuentran en diferentes magnitudes para que esto no afecte la calidad de las predicciones de nuestro modelo.=,=p>>A continiuación aplicaremos el algoritmo KNN para un sencillo ejemplo de clasificación con distintos valores de k y distancia euclideana en una hoja de cálculo.=,=p>>- Graficamos de los datos en dos series para Y=0 y para Y=1=,=img>>UT4-PD8-1.jpg>>75%>>200=,=br=,=br=,=p>>- Agregamos el punto con (x1, x2)= (8.093607318, 3.365731514) para clasificarlo usando diferentes valores de k.=,=img>>aa5.jpg>>100%>>150=,=p>>K=3: Puntos más cercanos: (x1, x2) = (7.423436942, 4.696522875), (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Predicción Y = 1.=,=p>>K=2: Puntos más cercanos: (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Predicción Y = 1.=,=p>>K=1: Puntos más cercanos: (x1, x2) = (7.792783481, 3.424088941). Predicción Y = 1.=,=p>>- Añadimos el punto con (x1, x2)= (5, 2.5) para clasificarlo usando diferentes valores de k.=,=img>>aa6.jpg>>100%>>150=,=p>>K=3: Puntos más cercanos: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (3.110073483, 1.781539638), (x1, x2) = (5.745051997, 3.533989803). Predicción Y = 0.=,=p>>K=2: Puntos más cercanos: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (5.745051997, 3.533989803). Predicción Y = N/A.=,=p>>K=1: Puntos más cercanos: (x1, x2) = (5.745051997, 3.533989803). Predicción Y = 1.=,=a>>Descargar planilla electrónica>>KVecinosMasCercanos.xlsx>>d=,=br=,=br=,=p>>Ahora estudiaremos al algoritmo KNN en RapidMiner para el dataset de clasificación Iris observando cómo es el operador y cuáles son sus parámetros en esta herramienta.=,=h6>>Modelo=,=img>>UT4-PD8-2.jpg>>100%>>300=,=br=,=br=,=h6>>Gráfica=,=img>>UT4-PD8-3.jpg>>100%>>450=,=br=,=br=,=h6>>Consideraciones=,=p>>Se puede observar que los datos correspondientes a la clase iris-setosa se encuentran bastante separados de los de las otras dos. En el caso de estos últimos iris-versicolor e iris-virginica, si bien tienen un poco de solapamiento también se pueden ver áreas diferenciables.=,=h6>>Preparación de datos=,=p>>Es conveniente estandarizar los datos con un range transformation entre 0 y 1.=,=h6>>Operador KNN en RapidMiner=,=p>>Voto ponderado: Si se activa este parámetro los valores de distancia entre los ejemplos se tienen en cuenta para la predicción. Es útil para ponderar las contribuciones de los vecinos de forma tal que los vecinos más cercanos contribuyan más que los más lejanos.=,=p>>Tipos de medición: Este parámetro se utiliza para seleccionar el tipo de medida que se utilizará para encontrar los vecinos más cercanos. Las opciones son las siguientes.=,=p>>- MixedMeasures: Se utiliza para calcular distancias en el caso de atributos tanto nominales como numéricos.=,=p>>- NominalMeasure: Se usa para el caso de solo atributos nominales.=,=p>>- NumericalMeasure: Se usa para el caso de solo atributos numéricos.=,=p>>- BregmannDivergences: Se selecciona para emplear diveregencias de Bregmann como tipos de medidas de cercanía.=,=p>>Funciones de medición:=,=p>>- EuclideanDistance. Dist = Sqrt ( Sum_(j=1) [y(1,j)-y(2,j)]^2).=,=p>>- CanberraDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)| / (|y(1,j)|+|y(2,j)|).=,=p>>- ChebychevDistance. Dist = max_(j=1) (|y(1,j)-y(2,j)|).=,=p>>- CosineSimilarity. Mide el coseno del ángulo entre los vectores de atributos de los dos ejemplos.=,=p>>- DiceSimilarity. La DiceSimilarity para atributos numéricos se calcula como 2 * Y1Y2 / (Y1 + Y2). Y1Y2 = Suma sobre el producto de valores = Suma_ (j = 1) y (1, j) * y (2, j). Y1 = Suma de los valores del primer Ejemplo = Suma_ (j = 1) y (1, j) Y2 = Suma de los valores del segundo Ejemplo = Suma_ (j = 1) y (2, j).=,=p>>- DynamicTimeWarpingDistance. Se calcula la distancia en una ruta de deformación óptima desde el vector atributo del primer ejemplo al segundo ejemplo.=,=p>>- InnerProductSimilarity. Dist = -Similarity = -Sum_(j=1) y(1,j)*y(2,j).=,=p>>- JaccardSimilarity. Dist = Y1Y2/(Y1+Y2-Y1Y2).=,=p>>- KernerlEuclideanDistance. La distancia se calcula mediante la distancia euclidiana de los dos Ejemplos, en un espacio transformado. La transformación está definida por el kernel y los parámetros correspondientes elegidos.=,=p>>- ManhattanDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)|.=,=p>>- MaxProductSimilarity. Dist = -Similarity = -max_(j=1) (y(1,j)*y(2,j)).=,=p>>- OverlapSimilarity. Dist = minY1Y2 / min (Y1, Y2).=,=h6>>Resultados=,=p>>K=3, euclidean distance:=,=img>>UT4-PD8-4.jpg>>100%>>150=,=p>>K=2, euclidean distance:=,=img>>UT4-PD8-5.jpg>>100%>>150=,=p>>K=3, manhattan distance:=,=img>>UT4-PD8-7.jpg>>100%>>150=,=p>>K=2, manhattan distance:=,=img>>UT4-PD8-6.jpg>>100%>>150=,=br=,=br=,=h6>>Conclusiones=,=p>>Con distancia euclideana, al variar el valor de k se obtuvieron diferentes valores de performance, pero esto no sucedió en los ejemplos realizados para la distancia manhattan.=,=a>>Descargar proceso de RapidMiner>>UT4-TA8.rmp>>d",
        "content":"p>>The algorithm for classification and regression K-Nearest Neighbors differs from the others in that it is not a learned model, rather predictions are made from the training dataset directly. The procedure consists of searching for each point to be predicted among the most similar instances based on measurements such as Euclidean distance, Hamming, Manhattan, Jaccard, Minkowski, etc.=,=p>>To use KNN to predict, simply choose a value of k and a measure of distance that will be used to find the k closest points among those in the training data with respect to the example to be predicted. From there, for classification, the classes of these k points are compared and the most repeated will be designated as prediction, and for regression the average of the output of the k closest values ​​is assigned. It should be noted that it is important to normalize if the data scales are at different magnitudes so that this does not affect the quality of our model's predictions.=,=p>>Next we will apply the KNN algorithm for a simple classification example with different values ​​of k and Euclidean distance in a spreadsheet.=,=p>>- We plot the data in two series for Y=0 and for Y=1=,=img>>UT4-PD8-1.jpg>> 75%>>200=,=br=,=br=,=p>>- We add the point with (x1, x2)= (8.093607318, 3.365731514) to classify it using different values ​​of k.=,=img>>aa5.jpg>>100%>>150=,=p>>K=3: Closest points: (x1, x2) = (7.423436942, 4.696522875), (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Prediction Y = 1.=,=p>>K=2: Closest points: (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Prediction Y = 1.=,=p>>K=1: Closest points: (x1, x2) = (7.792783481, 3.424088941). Prediction Y = 1.=,=p>>- We add the point with (x1, x2)= (5, 2.5) to classify it using different values ​​of k.=,=img>>aa6.jpg>>100%>> 150=,=p>>K=3: Closest points: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (3.110073483, 1.781539638), (x1, x2) = (5.745051997, 3.533989803) . Prediction Y = 0.=,=p>>K=2: Closest points: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (5.745051997, 3.533989803). Prediction Y = N/A.=,=p>>K=1: Closest points: (x1, x2) = (5.745051997, 3.533989803). Prediction Y = 1.=,=a>>Download spreadsheet>>KNearestNeighbors.xlsx>>d=,=br=,=br=,=p>>Now we will study the KNN algorithm in RapidMiner for the Iris classification dataset observing how is the operator and what are its parameters in this tool.=,=h6>>Model=,=img>>UT4-PD8-2.jpg>>100%>>300=,=br=,=br=,=h6>>Graph=,=img>>UT4-PD8-3.jpg>>100%>>450=,=br=,=br=,=h6>>Considerations=,=p>>It can be seen that the data corresponding to the iris-setosa class are quite separated from those of the other two. In the case of the latter iris-versicolor and iris-virginica, although they have a little overlap, distinguishable areas can also be seen.=,=h6>>Data preparation=,=p>>It is convenient to standardize the data with a range transformation between 0 and 1.=,=h6>>KNN operator in RapidMiner=,=p>>Weighted vote: If this parameter is activated, the distance values ​​between the examples are taken into account for the prediction. It is useful for weighting the neighbor contributions so that the nearest neighbors contribute more than the farthest.=,=p>>Measure Types: This parameter is used to select the type of measure to use to find the neighbors. closest neighbors. The options are as follows.=,=p>>- MixedMeasures: Used to calculate distances in the case of both nominal and numeric attributes.=,=p>>- NominalMeasure: Used in the case of only nominal attributes.=,=p>>- NumericalMeasure: Used for the case of only numeric attributes.=,=p>>- BregmannDivergences: Selected to use Bregmann divergences as closeness measure types.=,=p>>Measurement functions :=,=p>>- EuclideanDistance. Dist = Sqrt ( Sum_(j=1) [y(1,j)-y(2,j)]^2).=,=p>>- CanberraDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)| / (|y(1,j)|+|y(2,j)|).=,=p>>- ChebychevDistance. Dist = max_(j=1) (|y(1,j)-y(2,j)|).=,=p>>- CosineSimilarity. Measures the cosine of the angle between the attribute vectors of the two examples.=,=p>>- DiceSimilarity. The DiceSimilarity for numeric attributes is calculated as 2 * Y1Y2 / (Y1 + Y2). Y1Y2 = Sum over the product of values ​​= Sum_ (j = 1) and (1, j) * and (2, j). Y1 = Sum of the values ​​of the first Example = Sum_ (j = 1) and (1, j) Y2 = Sum of the values ​​of the second Example = Sum_ (j = 1) and (2, j).=,=p>>-DynamicTimeWarpingDistance. Calculate the distance in an optimal deformation path from the attribute vector of the first example to the second example.=,=p>>- InnerProductSimilarity. Dist = -Similarity = -Sum_(j=1) y(1,j)*y(2,j).=,=p>>- JaccardSimilarity. Dist = Y1Y2/(Y1+Y2-Y1Y2).=,=p>>- KernerlEuclideanDistance. The distance is calculated by the Euclidean distance of the two Examples, in a transformed space. The transformation is defined by the kernel and the corresponding parameters chosen.=,=p>>- ManhattanDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)|.=,=p>>- MaxProductSimilarity. Dist = -Similarity = -max_(j=1) (y(1,j)*y(2,j)).=,=p>>- OverlapSimilarity. Dist = minY1Y2 / min (Y1, Y2).=,=h6>>Results=,=p>>K=3, euclidean distance:=,=img>>UT4-PD8-4.jpg>>100%>> 150=,=p>>K=2, euclidean distance:=,=img>>UT4-PD8-5.jpg>>100%>>150=,=p>>K=3, manhattan distance:=,= img>>UT4-PD8-7.jpg>>100%>>150=,=p>>K=2, manhattan distance:=,=img>>UT4-PD8-6.jpg>>100%>>150 =,=br=,=br=,=h6>>Conclusions=,=p>>With Euclidean distance, by varying the value of k, different performance values ​​were obtained, but this did not happen in the examples carried out for the Manhattan distance .=,=a>>Download RapidMiner Process>>UT4-TA8.rmp>>d"
    },
    {
        "unidad":"No supervisado",
        "unit":"Unsupervised",
        "titulo":"DBSCAN en Wine",
        "title":"DBSCAN in Wine",
        "descripcion":"Aplicamos el algoritmo de clustering DBSCAN para encontrar agrupaciones en el dataet Wine",
        "description":"We apply the DBSCAN clustering algorithm to find clusters in the Wine dataset.",
        "contenido":"p>>El algoritmo no supervisado de clustering DBSCAN se basa en las densidades de ejemplos para definir agrupaciones. Se fundamenta en descubrir áreas de alta densidad de datos rodeadas de otras de baja densidad según los parámetros de radio épsilon (ε) y el mínimo de puntos que puede haber en clúster. A partir de estos valores claifica a los puntos en 3 tipos, puntos de núncleo los cuales están en una región de alta densidad de al menos un punto, los de borde que están a un a distancie ε de algún punto y los de ruido son aquellos que caen fuera de las dos clases anteriores. De esta manera, el usuario puede modificar ambos parámetros según su conveniencia pero no definirá la cantidad de clusteres que el modelo hallará como sucede con kmeans.=,=p>>Para ver un caso de empleo de DBSCAN nos ayudaremos de RapidMiner para aplicarlo a los atributos petal-width y petal height del dataset Iris. Para estos casos se mantendrá constante el mínimo de puntos con 5 y se irá modificando el valor de ε en distintas iteraciones para luego analizar como varían los resultados obtenidos.=,=h6>>Proceso RapidMiner=,=img>>dbscan-m.jpg>>100%>>250=,=a>>Descargar proceso>>ut5 ta1 ej2.rmp>>d=,=br=,=br=,=h6>>Con ε=1.0=,=img>>dbscan-0.jpg>>70%>>550=,=br=,=br=,=h6>>Con ε=0.3=,=img>>dbscan-1.jpg>>70%>>550=,=br=,=br=,=h6>>Con ε=0.2=,=img>>dbscan-2.jpg>>70%>>550=,=br=,=br=,=p>>Se observa como el cluster 0 corresponde a puntos de ruido. Al ir disminuyendo el radio cada vez aparecen más puntos de ruido y nuevos clusteres, con ε=1.0 y ε=0.3 hay solo 2 pero a partir de ε=0.2 se suma uno nuevo y si se continúa realizando este descenso en el parámetro cada vez aparecerán más.",
        "content":"p>>The unsupervised DBSCAN clustering algorithm relies on sample densities to define clusters. It is based on discovering areas of high data density surrounded by others of low density according to the parameters of radius epsilon (ε) and the minimum number of points that can be in a cluster. Based on these values, it classifies the points into 3 types: core points, which are in a high-density region of at least one point, edge points, which are at a distance ε from some point, and noise points are those that they fall outside of the above two classes. In this way, the user can modify both parameters according to his convenience but he will not define the number of clusters that the model will find as it happens with kmeans.=,=p>>To see a case of using DBSCAN we will use RapidMiner to apply it to the petal-width and petal-height attributes of the Iris dataset. For these cases, the minimum number of points will be kept constant with 5 and the value of ε will be modified in different iterations to later analyze how the results obtained vary.=,=h6>>RapidMiner process=,=img>>dbscan-m.jpg>>80%>>250=,=br=,=a>>Download process>>ut5 ta1 ej2.rmp>>d=,=br=,=br=,=h6>>With ε=1.0=,=img>>dbscan-0.jpg>>70%>>550=,=br=,=br=,=h6>>With ε=0.3=,=img>>dbscan-1.jpg>>70%>>550=, =br=,=br=,=h6>>With ε=0.2=,=img>>dbscan-2.jpg>>70%>>550=,=br=,=br=,=p>>It is observed as cluster 0 corresponds to noise points. As the radius decreases, more and more noise points and new clusters appear, with ε=1.0 and ε=0.3 there are only 2 but from ε=0.2 a new one is added and if this decrease in the parameter is continued each time more will appear."
    },
    {
        "unidad":"No supervisado",
        "unit":"Unsupervised",
        "titulo":"PCA para dataset Cereals",
        "title":"PCA for Cereals dataset",
        "descripcion":"Estudio del algoritmo PCA utilizando los datos de Cereals",
        "description":"Study of the PCA algorithm using Cereals data",
        "contenido":"p>>Principal Component Analyisis (PCA) es un algoritmo que reduce los atributos en algunos atributos principales que contienen la mayor variabilidad transformando variables existentes en componentes principales o nuevas variables que no tienen correlación entre sí, de forma acumulada explican la mayor cantidad de varianza entre los datos y son revertibles a las variables originales por factores de peso. Otras de sus características son que solamente se puede aplicar para variables numéricas y que se pierde el significado de las variables del problema con los componentes principales.=,=p>>A continuación realizaremos un ejemplo en RapidMiner sobre el dataset cereals que cuenta con 77 ejemplos cuyas variables son las siguientes.=,=p>>- Name (categórica): Nombre del cereal=,=p>>- mfr (categórica): Manufacturero del cereal (A = American Home Food Products; G = General Mills; K = Kelloggs; N = Nabisco; P = Post; Q = Quaker Oats; R = Ralston Purina)=,=p>>- type (categórico): frío o caliente.=,=p>>- protein (numérico): proteínas del cereal.=,=p>>- fat (numérico): grasas del cereal.=,=p>>- sodium (numérico): miligramos de sodio.=,=p>>- fiber (numérico): gramos de fibra dietética.=,=p>>- carbo (numérico): gramos de carbohidratos complejos.=,=p>>- sugars (numérico): gramos de azúcar.=,=p>>- potass (numérico): miligramos de potasio.=,=p>>- vitamines (numérico): viraminas y minerales indicando el porcentaje típico de las recomendaciones del FDA.=,=p>>- shelf (numérico): Estante (1,2,3,4).=,=p>>- Weight (numérico): Peso en onzas por porción.=,=p>>- Cups (numérico): Cantidad de tazas por porción.=,=p>>- Rating (numérico): Puntaje para los cereales.=,=a>>Descargar dataset>>Cereals.xls>>d=,=br=,=br=,=p>>Se importó el modelo y para poder aplicar el algoritmo se tuvieron que quitar los atributos categóricos Name, mfr y type y reemplazar los 4 valores faltantes por el promedio del atributo correspondiente debido a que PCA ni variables categóricas ni missing values.=,=p>>Luego del pre procesamiento anterior se colocó el operador PCA con 0.95 de variance threshold que refiere a la cantidad de varianza que buscará explicar por las variables que seleccione en los componentes principales generados y keep reduction en dimensionality reduction por lo que los atributos con varianza mayor al umbral se eliminan del dataset.=,=h6>>Esquema RapidMiner =,=img>>pca-0.jpg>>80%>>200=,=br=,=a>>Descargar modelo>>pca.rmp>>d=,=br=,=br=,=p>>Los resultados obtenidos son los siguientes.=,=img>>pca-1.jpg>>80%>>400=,=br=,=br=,=img>>pca-2.jpg>>80%>>400=,=br=,=br=,=p>>Observando la primer imagen se puede concluir que se supera a la varianza acumulada establecida de en variance threshold de 0.95 contando con tan solo con los primeros componentes al llegar 0.966. En la segunda representación se ve el peso que tiene cada atributo en cada componente lo cual nos permite observar que rating, calories, potasium y vitamins son los atributos que más aportan en los 3 primeros componentes principales y por lo tanto los más importantes.",
        "content":"p>>Principal Component Analysis (PCA) is an algorithm that reduces the attributes into some principal attributes that contain the greatest variability by transforming existing variables into principal components or new variables that have no correlation with each other, cumulatively they explain the greatest amount of variance between the data and are revertible to the original variables by weight factors. Other characteristics are that it can only be applied to numerical variables and that the meaning of the variables of the problem with the main components is lost.=,=p>>Next we will carry out an example in RapidMiner on the cereals dataset that has 77 examples whose variables are as follows.=,=p>>- Name (categorical): Name of the cereal=,=p>>- mfr (categorical): Manufacturer of the cereal (A = American Home Food Products; G = General Mills; K = Kelloggs; N = Nabisco; P = Post; Q = Quaker Oats; R = Ralston Purina)=,=p>>- type (categorical): hot or cold.=,=p>>- protein (numeric): cereal proteins.=,=p>>- fat (numeric): cereal fats.=,=p>>- sodium (numeric): milligrams of sodium.=,=p>>- fiber (numeric): grams of dietary fiber.=,=p>>- carbo (numeric): grams of complex carbohydrates.=,=p>>- sugars (numeric): grams of sugar.=,=p>>- potass (numeric): milligrams of potassium.=,=p>>- vitamins (numeric): viramines and minerals indicating the typical percentage of FDA recommendations.=,=p>>- shelf (numeric): Shelf (1,2,3,4).=,=p>>- Weight (numeric): Weight in ounces per serving.=,=p>>- Cups (numeric): Number of cups per serving.=,=p>>- Rating (numeric): Score for cereals.=,=a>>Download dataset>>Cereals.xls>>d =,=br=,=br=,=p>>The model was imported and in order to apply the algorithm the Name, mfr and type categorical attributes had to be removed and the 4 missing values ​​replaced by the average of the corresponding attribute due to that PCA neither categorical variables nor missing values.=,=p>>After the previous pre-processing, the PCA operator was placed with a variance threshold of 0.95, which refers to the amount of variance that it will seek to explain by the variables that it selects in the main components generated and keep reduction in dimensionality reduction so that attributes with variance greater than the threshold are removed from the dataset.=,=h6>>RapidMiner Scheme =,=img>>pca-0.jpg>>80%>>200=,= br=,=a>>Download r model>>pca.rmp>>d=,=br=,=br=,=p>>The results obtained are the following.=,=img>>pca-1.jpg>>80%>>400=,=br=,=br=,=img>>pca-2.jpg>>80%>>400=,=br=,=br=,=p>>Observing the first image, it can be concluded that it exceeds the cumulative variance established in the variance threshold of 0.95, counting only with the first components when reaching 0.966. In the second representation, the weight that each attribute has in each component can be seen, which allows us to observe that rating, calories, potassium and vitamins are the attributes that contribute the most in the first 3 main components and therefore the most important."
    },
    {
        "unidad":"case",
        "unit":"case",
        "titulo":"Ensambles",
        "title":"Ensembles",
        "descripcion":"",
        "description":"",
        "contenido":"p>>Los algoritmos de ensamble se caracterizan por generar múltiples modelos independientes de machine learning y combinar sus predicciones para obtener mejores performances.=,=h6>>Bagging=,=p>>Es un método de ensamble que se basa en crear muchas sub muestras tomadas con reemplazo, entrenar distintos modelos en ellos de alta varianza (típicamente CARTs) y dado un nuevo dataset calcular el promedio para cada predicción en el caso de problemas de regresión o mediante voto para clasificación. Para el caso de los árboles de decisión, al usar bagging nos preocupamos menos porque uno en particular haga overfitting de los datos de forma tal de que se puede permitir que los árboles indivualmente crezcan profundamente y sin poda. =,=h6>>Random Forest=,=p>>Es uno de los algoritmo más potentes de machine learing. Se basa en árboles de decisión que utiliza bagging pero modificando a los árboles de decisión en que en lugar de buscar los split points de forma óptima se limita este procedimiento a aprender de una muestra aleatoria de los predictores con el fin de que los árboles generados sean más independientes entre sí.=,=h6>>Boosting=,=p>>Es otra técnica de ensamble que busca generar un modelo fuerte a partir de varios clasificadores débiles. Esto se hace a partir de crear un modelo a partir de los datos de entrenamiento, luego creando un segundo modelo que mejore los errores del primer modelo y así hasta que la predicción es perfecta (pudiendo causar overfitting) o hasta que un máximo de modelos son añadidos.=,=h6>>Ada Boost=,=p>>Es un algoritmo basado en boosting que es muy útil para clasificación binaria y el modelo débil que utiliza típicamente son árboles de decisión de un nivel. Estos se entrenan poniendo pesos a cada ejemplo del dataset de entrenamiento, añadiendo peso si los modelos débiles clasifican equivocadamente para darle más importancia en la siguiente iteración a esa instancia, y calculando una ponderación para el modelo débil que luego que luego se utiliza en cada predicción que este modelo haga. Lo cual se repite hasta un criterio de parada o hasta que no se pueda seguir mejorando la performance. Las predicciones se realizan calculando el promedio de las predicciones ponderadas de los modelos débiles clasificando según se obtiene un valor positivo o negativo.=,=p>>A continuación aplicaremos Random Forest y Ada Boost con árboles de decisión aplicado al sencillo dataset de clasificación binaria Banking en RapidMiner. Mediremos sus preformances y compararemos sus curvas ROC.=,=h5>>Dataset=,=p>>El conjunto de datos banking marketing cuenta con 17 atributos correspondientes a características de clientes para predecir si un cliente invierte en un depósito a plazo fijo según una campaña de marketing telefónico. Entre los datos hay 4521 ejemplos y no se encuentran missing values.=,=p>>Los atributos son las siguientes:=,=p>>- age (numérico): La edad del cliente.=,=p>>- job (categórico): tipo de trabajo entre admin, unknown, unemployed, management, housemaid, entrepreneur, student, blue-collar, self-employed, retired, technician, services.=,=p>>- marital (categórico): Estado civil entre married, divorced (divorciado y viudo se condieran dentro de esta clase) y single.=,=p>>- education (categórico): Nivel educativo entre unknown, secondary, primary y tertiary.=,=p>>- default (categórico): Si tiene el crédito en default entre yes y no.=,=p>>- balance (numérico): promedio de balance anual en euros.=,=p>>- loan (categórico): si el cliente tiene préstamos entre yes y no.=,=p>>- contact (categorico): Medio de contacto entre unknown, telephone y cellular.=,=p>>- day (numérico): Día del último contacto.=,=p>>- month (categórico): Mes del último contacto.=,=p>>- duration (numérico): Duración del último contacto en segundos.=,=p>>- campaign (numérico): número de conactos realizados con el cliente esta campaña.=,=p>>- pdays (numérico): número de días desde que el cliente fue llamado desde una campaña previa en la que -1 significa que nunca fue contactado.=,=p>>- previous (numérico): número de contactos realizados antes de esta campaña con el cliente.=,=p>>- poutcome (categórico): Resultado de la última campaña de marketing entre unknown, other, failure y success.=,=p>>- y (categórico): El cliente se sucribió al plazo fijo entre yes y no.=,=h5>>Operador=,=h6>>Descripciones=,=img>>1.jpg>>75%>>500=,=br=,=br=,=img>>2.jpg>>75%>>500=,=br=,=br=,=h6>>Parámetros=,=p>>-Random Forest: Cuenta con number of trees que determina la cantidad de árboles generados, voting strategy utilizado para elegir la estrategia de predicción entre los árboles del modelo y las configuraciones propias del componente de árbol de decisión para seleccionar cualidades de aquellos que lo componen.=,=p>>-AdaBoost: Iterations utilizado para fijar el número máximo de iteraciones del algoritmo.=,=h5>>Modelo=,=img>>3.jpg>>75%>>600=,=br=,=br=,=h5>>Resultados=,=p>>Performance Random Forest.=,=img>>4.jpg>>90%>>175=,=br=,=br=,=p>>Performance AdaBoost=,=img>>5.jpg>>90%>>175=,=br=,=br=,=p>>Comparación entre curvas ROC=,=img>>6.jpg>>90%>>600=br=,=br=,=br=,=h4>>Conclusiones=,=p>>Observamos que para este caso particular tanto los valores de accuracy, precisión de clases y recall de categorías dieron practicamente iguales con leves ventajas para Random Forest aunque ambos algoritmos dieron muy buenos resultados en todos exceptuando la precisión y sobre todo el recall para la clase yes fallando los dos en lo mismo.=,=p>>En donde si se pueden ver más diferencias es en la comparación de las curvas ROC. En esta se puede observar que la curva de AdaBoost tiene un mucho mayor crecimiento de verdaderos positivos respecto a falsos positivos que Random Forest con lo cual resulta ser un mejor modelo.",
        "content":"p>>Ensemble learning algorithms are characterized by generating multiple independent machine learning models and combining their predictions to obtain better performances.=,=h6>>Bagging=,=p>>It is an assembly method that is based on creating many sub-samples taken with replacement, train different models on them with high variance (typically CARTs) and given a new dataset calculate the average for each prediction in the case of regression problems or by voting for classification. For the case of decision trees, by using bagging we worry less about one particular overfitting the data in such a way that the individual trees can be allowed to grow deeply and without pruning. =,=h6>>Random Forest=,=p>>It is one of the most powerful machine learning algorithms. It is based on decision trees that use bagging but modifying the decision trees in that instead of searching for the split points optimally, this procedure is limited to learning from a random sample of the predictors so that the trees generated are more independent from each other.=,=h6>>Boosting=,=p>>This is another assembly technique that seeks to generate a strong model from several weak classifiers. This is done by creating a model from the training data, then creating a second model that improves the errors of the first model and so on until the prediction is perfect (which may cause overfitting) or until a maximum of models are added.=,=h6>>Ada Boost=,=p>>It is a boosting based algorithm that is very useful for binary classification and the weak model it typically uses is one level decision trees. These are trained by putting weights on each example in the training dataset, adding weight if weak models misclassify to give that instance more weight in the next iteration, and calculating a weight for the weak model that is then used in each prediction. that this model does. Which is repeated until a stop criterion or until it is not possible to continue improving the performance. The predictions are made by calculating the average of the weighted predictions of the weak models, classifying according to a positive or negative value.=,=p>>Next we will apply Random Forest and Ada Boost with decision trees applied to the simple binary classification dataset Banking in RapidMiner. We will measure their preformances and compare their ROC curves.=,=h5>>Dataset=,=p>>The banking marketing data set has 17 attributes corresponding to customer characteristics to predict whether a customer invests in a fixed-term deposit according to a telephone marketing campaign. Among the data there are 4521 examples and there are no missing values.=,=p>>The attributes are the following:=,=p>>- age (numeric): The age of the client.=,=p>>- job (categorical): type of job between admin, unknown, unemployed, management, housemaid, entrepreneur, student, blue-collar, self-employed, retired, technician, services.=,=p>>- marital (categorical): Marital status between married, divorced (divorced and widowed are considered within this class) and single.=,=p>>- education (categorical): Educational level between unknown, secondary, primary and tertiary.=,=p>>- default ( categorical): If the loan is in default between yes and no.=,=p>>- balance (numeric): average annual balance in euros.=,=p>>- loan (categorical): if the client has loans Between yes and no.=,=p>>- contact (categorical): Means of contact between unknown, telephone and cellular.=,=p>>- day (numeric): Day of last contact.=,=p>> - month (categorical): Month of the last contact.=,=p>>- duration (numeric): Duration of the last c oncontact in seconds.=,=p>>- campaign (numeric): number of contacts made with the client this campaign.=,=p>>- pdays (numeric): number of days since the client was called from a campaign previous in which -1 means that he was never contacted.=,=p>>- previous (numeric): number of contacts made before this campaign with the client.=,=p>>- poutcome (categorical): Result of the last marketing campaign between unknown, other, failure and success.=,=p>>- y (categorical): The client subscribed to the fixed term between yes and no.=,=h5>>Operador=,=h6>>Descriptions=,=img>>1.jpg>>75%>>500=,=br=,=br=,=img>>2.jpg>>75%>>500=,=br=,=br =,=h6>>Parameters=,=p>>-Random Forest: It has number of trees that determines the number of generated trees, voting strategy used to choose the prediction strategy between the model trees and the component's own configurations decision tree to select qualities of those that compose it.=,=p>>-AdaBoost: Iteration s used to set the maximum number of iterations of the algorithm.=,=h5>>Model=,=img>>3.jpg>>75%>>600=,=br=,=br=,=h5>>Results =,=p>>Performance Random Forest.=,=img>>4.jpg>>90%>>175=,=br=,=br=,=p>>Performance AdaBoost=,=img>>5.jpg>>90%>>175=,=br=,=br=,=p>>Comparison between ROC curves=,=img>>6.jpg>>90%>>600=br=,=br=,=br=,=h4>>Conclusions=,=p>>We observe that for this particular case both the accuracy, class precision and category recall values ​​were practically the same with slight advantages for Random Forest although both algorithms gave very good results in all except the precision and especially the recall for the yes class, both failing in the same thing.=,=p>>Where if you can see more differences is in the comparison of the ROC curves. In this it can be seen that the AdaBoost curve has a much higher growth of true positives compared to false positives than Random Forest, which turns out to be a better model."
    },
    {
        "unidad":"case",
        "unit":"case",
        "titulo":"Enfermedad cardíaca",
        "title":"Heart disease",
        "descripcion":"",
        "description":"",
        "contenido":"h2>>Contexto=,=p>>Las enfermedades cardiovasculares son la principal causa de muerte en el mundo según la Organización Mundial de la Salud (OMS) llevándose un estimado de 17.9 millones de vidas cada año lo cual supone un 32% de todas las muertes anuales alrededor del planeta. En el Uruguay según datos oficiales del Ministerio de Salud Pública (MSP), para cada año del quinquenio (2015-2019)  anterior a la pandemia del SARS-CoV2, conocido coloquialmente como coronavirus, estas enfermedades motivaron entre un 25% y un 27% de la totalidad de decesos conforme a lo que pudo identificar nuestro sistema de salud.=,=p>>Entre las principales causas de las enfermedades coronarias identificadas por el Centro de Control y Prevención de Enfermedades de los Estados Unidos (CDC) se encuentran alta presión arterial, colesterol elevado, fumar, diabetes, sobrepeso u obesidad, dietas poco saludables, poca actividad física y excesivo uso consumo de alcohol.=,=p>>Por otra parte, según datos del CDC se observa que esta enfermedad se da con mayor frecuencia para los hombres que para las mujeres con un 13.6% frente a un 8.4% de las respectivas poblaciones habiendo reportado padecimientos coronarios y que también se da con más ocurrencia al envejecer llegando hasta la franja a partir de los 65 años en la que un 17% de esa población reportó tenerla.=,=p>>La mayor sugerencia realizada por el sector de la salud al respecto de los padecimientos del corazón es tratar de prevenirlos lo máximo posible teniendo una vida saludable. De todas maneras, existen tratamientos como prescripción de cambios de rutina, medicación y hasta procedimientos quirúrgicos dependiendo de los diversos factores que estén causando la enfermedad, pero para todos ellos es fundamental tener un diagnóstico acertado y a tiempo con un médico.=,=p>>=,=br=,=h2>>Entendimiento del negocio=,=hr=,=p>>En este caso de estudio se desarrollarán todos los pasos CRISP-DM para la problemática supervisada y de clasificación acerca de la predicción de enfermedades cardíacas con fines académicos, sin que exista inicialmente la finalidad de poner en producción el resultado conseguido. A continuación se pasará por las etapas de entendimiento de datos, preparación, modelado y evaluación para la realización de predicciones acerca de si un paciente cuenta o no con una enfermedad del corazón a partir de algunas de sus características empleando para la herramienta RapidMiner.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>Para el trabajo se cuenta con cuatro bases de datos vinculadas al diagnóstico de enfermedades del corazón y cualidades de los pacientes provenientes respectivamente de V.A Medical Center (California, Estados Unidos) autor: Robert Detrano M.D. Ph.D,  Cleveland Clinic Foundation (Ohio, Estados Unidos) autor: Robert Detrano M.D. Ph.D, University Hospital (Zurich, Suiza) autor: William Steinbrunn M.D y Hungarian Institute of Cardiology (Budapest, Hungría) autor: Andras Janosi M.D.=,=p>> Los datos fueron analizados en la herramienta RapidMiner y para ello hubo que realizarles un pre procesamiento en Python armando documentos en formato .csv con un formato con una cabecera conteniendo los nombres de los atributos y los registros de un paciente por fila a partir de cada uno de los cuatro archivos originales individualmente en los que los datos están separados por espacios y en varios renglones para cada individuo del cual se cuenta con información.=,=code>>.lines = open(\"switzerland.data\", \"r\", errors='ignore')=,=code>>.writer = open(\"cleanedSwitzerland.data\", 'w', encoding=\"cp437\", errors='ignore')=,=br=,=code>>.header = (\"id,ccf,age,sex,painloc,painexer,relrest,pncaden,cp,trestbps,htn,chol,smoke,cigs,years,\"=,=br=,=code>>.\u00a0\u00a0        \"fbs,dm,famhist,restecg,ekgmo,ekgday,ekgyr,dig,prop,nitr,pro,diuretic,proto,thaldur,\"=,=br=,=code>>.\u00a0\u00a0        \"thaltime,met,thalach,thalrest,tpeakbps,tpeakbpd,dummy,trestbpd,exang,xhypo,oldpeak,\"=,=br=,=code>>.\u00a0\u00a0        \"slope,rldv5,rldv5e,ca,restckm,exerckm,restef,restwm,exeref,exerwm,thal,thalsev,thalpul,\"=,=br=,=code>>.\u00a0\u00a0        \"earlobe,cmo,cday,cyr,num,lmt,ladprox,laddist,diag,cxmain,ramus,om1,om2,rcaprox,rcadist,\"=,=br=,=code>>.\u00a0\u00a0        \"lvx1,lvx2,lvx3,lvx4,lvf,cathef,junk\")=,=br=,=code>>.writer.write(header+'\n')=,=br=,=code>>.info = []=,=br=,=code>>.print(lines)=,=br=,=code>>.for line in lines:=,=br=,=code>>.  lineList = line[:-1].split(\" \")=,=br=,=code>>.  if'name' not in lineList:=,=br=,=code>>.\u00a0    info.extend(lineList)=,=br=,=code>>.  else:=,=br=,=code>>.\u00a0    info.extend(lineList[:-1])=,=br=,=code>>.\u00a0    infoWithMV = [\"?\" if x in [\"-9\",\"-9.\",\"-9.0\"]  else x for x in info]=,=br=,=code>>.\u00a0    infoToWrite = \",\".join(infoWithMV)=,=br=,=code>>.\u00a0    writer.write(infoToWrite+'\n')=,=br=,=code>>.\u00a0    info = []=,=br=,=code>>.writer.close()=,=br=,=code>>.lines.close()=,=br=,=br=,=h6>>Descargas:=,=a>>Descripción de datos>>cardiac description.txt>>d=,=br=,=a>>Datos>>cardiac data.zip>>d=,=br=,=br=,=p>>Los datasets cuentan con una totalidad de 920 ejemplos etiquetados para los cuales hay 76 atributos numéricos que especifican diversas características de los pacientes como su edad(age), sex (sexo), colesterol (chol), azúcar en sangre en reposo (fbs), etc. La variable objetivo del problema llamada num la cual representa el diagnóstico de enfermedad cardíaca. Esta última tiene un valor de 0 si no hay presencia de enfermedad en el individuo y valores del 1 al 4 según diferentes tipos de padecimientos del corazón.=,=p>>A pesar de la gran cantidad de atributos 10 de ellos no fueron utilizados, tal y como se expresa en la descripción de los datasets, siendo estos thalsev, thalpul, earlobe, lvx1, lvx2, lvx3, lvx4, lvf, cathef y junk. Por otra parte, hay un gran número de predictores con valores faltantes los cuales vienen represntados con el valor -9. Existen 20 de ellos con más de un 50% de missing values respecto a la totalidad de los registros.=,=br=,=h2>>Preparación de datos=,=hr=,=p>>Lo primero que se realizó fue importar los datos correspondientes a las cuatro bases en la herramienta RapidMiner comprobando que todos se cargaban con los mismos tipos para cada uno de sus atributos. Posteriormente se procedió a juntarlas utilizando el componente Append y obteniendo así el conjunto con todos los datos etiquetados.=,=p>>Se declaró a los registros con valor -9 como missing values y primeramente se descartaron las columnas de id y las que tenían más de 50% de valores faltantes considerando que la mayor parte de estas concierne atributos no utilizados según lo expresado en la descripcion de los datos y a que estas pueden generar dificultades para el aprendizaje de los modelos a crear, los restantes fueron reemplazados por el promedio de los datos correspondientes a cada predictor según el caso.=,=p>>Otros procedimientos realizados fueron la remoción de outliers utilizando los operadores Detect Outliers y Filter Examples para quitar los 10 outliers más lejanos de los 10 vecinos más cercanos según distancia euclideana, el cambio de tipo de la variable objetivo de numérica a polinomial usando Numerical to Polynomial y la estandarización de todos los predictores con el componente Normalize.=,=br=,=h2>>Modelado=,=hr=,=p>>Para los modelos a desarrollar se consideraron aquellos supervisados y de clasificación que soportaran variables polinomiales como Naive Bayes, Árboles de Decisión, K-Nearest Neighbors y los algoritmos de ensamble siendo estos uno de tipo bagged como Random Forest y otro de boosting como Gradient Boosted Trees.=,=p>> De los estudiados fueron descartados K-Nearest Neighbors debido a la alta dimensionalidad de los datos y Árboles de Decisión al incluir Random Forest dado que este último es una evolucion del anterior en ser menos propenso al overfitting al estar compuesto por muchos árboles con predictores seleccionados randomicamente con reemplazo en cada uno brindando así generalización al modelo.=,=p>>Por lo tanto, se eligieron los modelos de Naive Bayes como línea de base, Random Forest y Gradient Boosting Trees, aplicando en todos los casos Cross Validation de 10 folds con la misma random seed local número 1992 y midiendo las performances para cada uno.=,=h6>>Proceso en RapidMiner=,=img>>cardiac-0.jpg>>100%>>550=,=a>>Descargar proceso>>Heart Disease.rmp>>d=,=br=,=br=,=h2>>Resultados=,=hr=,=p>>Para evaluar los modelos y medir su rendimiento se estudiaron las matrices de confusión obteniendo las siguientes tablas.=,=h6>>Naive Bayes=,=img>>cardiac-1.jpg>>100%>>250=,=br=,=br=,=h6>>Random Forest=,=img>>cardiac-2.jpg>>100%>>250=,=p>>Params: num trees = 1000, criterion = gain_ratio, min leaf size = 2, min size for split = 4 y max depth = 10.=,=h6>>Gradient Boosting Trees=,=img>>cardiac-3.jpg>>100%>>250=,=p>>Params: num trees = 50, max depth = 5, min rows = 10, min split imp. = 1.0E-5, num bins = 20, lear. rate = 0.01. =,=br=,=h2>>Conclusiones=,=hr=,=p>>Se puede observar que los que dan mejores resultados de accuracy son los de ensambles con Gradient Boosting Trees en la delantera y Random Forest muy cerca. Igualmente, si bien la performance global siempre es muy importante, para este caso de diagnóstico de enfermedades podría llegarse a considerar hasta de mayor relevancia la presición para la clase 0 que representa el pocentaje de acierto en la predicción de ausencia de enfermedad dado que no se pueden permitir altos valores de falsos positivos en problemas tan sensibles como la salud del corazón en la que si ocurren pueden llegar hasta ser terminales para los pacientes. En cuanto a esto se puede destacar que Naive Bayes tiene la presición más alta para la clase 0 aunque también es el que tiene menos recall para esta identificando la mayor cantidad de personas que no tienen enfermedades cardíacas como que si las tuvieran de entre los modelos realizados.=,=p>>Para decidir entre que modelo es mejor entre Naive Bayes y su notable predicción para la clase 0 y Gradient Boosting Machines y su increíble acurracy prediciendo mejor para todas las demás clases es necesario conocer que tan profunda es la diferenciar entre enfermedades de corazón tipo 4, 3, 2 y 1. Si la diferencia es grande en cuanto al tratamiento requerido y las posibilidades del paciente entonces el mejor algoritmo es Gradient Boosting Machines, pero si no es así entre estos modelos yo eligiría Naive Bayes para asegurarme de tener la menor cantidad de falsos negativos de la enfermedad. Lamentablemente no se dispone de esta información en el dataset y en un caso real habría que consultar equipos médicos para tomar la decisión adecuada.",
        "content":"h2>>Context=,=p>>Cardiovascular diseases are the main cause of death in the world according to the World Health Organization (WHO), taking an estimated 17.9 million lives each year, which represents 32% of all annual deaths around the planet. In Uruguay, according to official data from the Ministry of Public Health (MSP), for each year of the five-year period (2015-2019) prior to the SARS-CoV2 pandemic, colloquially known as coronavirus, these diseases caused between 25% and 27% of all deaths according to what our health system could identify.=,=p>>Among the main causes of coronary diseases identified by the United States Center for Disease Control and Prevention (CDC) are high blood pressure, high cholesterol, smoking, diabetes, overweight or obesity, unhealthy diets, little physical activity and excessive use of alcohol.=,=p>>On the other hand, according to data from the CDC, it is observed that this disease occurs with greater frequency for men than for women with 13.6% compared to 8.4% of the respective populations having reported coronary diseases and that also occurs with more occurrence as they age, reaching the range from of the 65 years in which 17% of that population reported having it.=,=p>>The greatest suggestion made by the health sector regarding heart diseases is to try to prevent them as much as possible by having a healthy life . In any case, there are treatments such as prescription of routine changes, medication and even surgical procedures depending on the various factors that are causing the disease, but for all of them it is essential to have an accurate and timely diagnosis with a doctor.=,=p>>=,=br=,=h2>>Understanding the business=,=hr=,=p>>In this case study, all the CRISP-DM steps will be developed for the supervised and classification problem about disease prediction for academic purposes, without initially having the purpose of putting the achieved result into production. Next, it will go through the stages of data understanding, preparation, modeling and evaluation to make predictions about whether or not a patient has heart disease based on some of its characteristics using the RapidMiner tool. =,=br=,=h2>>Data knowledge=,=hr=,=p>>For the work, there are four databases linked to the diagnosis of heart diseases and qualities of the patients, respectively from the VA Medical Center ( California, United States) author: Robert Detrano MD Ph.D, Cleveland Clinic Foundation (Ohio, United States) author: Robert Detrano MD Ph.D, University Hospital (Zurich, Switzerland) author: William Steinbrunn MD and Hungarian Institute of Cardiology ( Budapest, Hungary) author: Andras Janosi MD=,=p>> The data was analyzed in the RapidMiner tool and for this it was necessary to perform a pre-processing in Python, putting together documents in .csv format with a format with a header containing the names of the attributes and the records of a patient per row from each of the four original files individually in which the data is separated by spaces and in several rows for each individual for whom information is available.=,=code>>.lines = open(\"switzerland.data\", \"r\", errors='ignore')=,=code>>.writer = open(\"cleanedSwitzerland.data\", 'w ', encoding=\"cp437\", errors='ignore')=,=br=,=code>>.header = (\"id,ccf,age,sex,painloc,painexer,relrest,pncaden,cp, trestbps,htn,chol,smoke,cigs,years,\"=,=br=,=code>>.\u00a0\u00a0 \"fbs,dm,famhist,restecg,ekgmo,ekgday,ekgyr,dig,prop,nitr ,pro,diuretic,proto,thaldur,\"=,=br=,=code>>.\u00a0\u00a0 \"thaltime,met,thalach,thalrest,tpeakbps,tpeakbpd,dummy,trestbpd,exang,xhypo,oldpeak, \"=,=br=,=code>>.\u00a0\u00a0 \"slope,rldv5,rldv5e,ca,restckm,exerckm,restef,restwm,exeref,exerwm,thal,thalsev,thalpul,\"=,= br=,=code>>.\u00a0\u00a0 \"earlobe,cmo,cday,cyr,num,lmt,ladprox,laddist,diag,cxmain,ramus,o m1,om2,rcaprox,rcadist,\"=,=br=,=code>>.\u00a0\u00a0 \"lvx1,lvx2,lvx3,lvx4,lvf,cathef,junk\")=,=br=,= code>>.writer.write(header+'\n')=,=br=,=code>>.info = []=,=br=,=code>>.print(lines)=,=br=,=code>>.for line in lines:=,=br=,=code>>. lineList = line[:-1].split(\" \")=,=br=,=code>>. if'name' not in lineList:=,=br=,=code>>.\u00a0 info.extend(lineList)=,=br=,=code>>. else:=,=br=,=code>>.\u00a0 info.extend(lineList[:-1])=,=br=,=code>>.\u00a0 infoWithMV = [\"?\" if x in [\"-9\",\"-9.\",\"-9.0\"] else x for x in info]=,=br=,=code>>.\u00a0 infoToWrite = \",\" .join(infoWithMV)=,=br=,=code>>.\u00a0 writer.write(infoToWrite+'\n')=,=br=,=code>>.\u00a0 info = []=,=br=,=code>>.writer.close()=,=br=,=code>>.lines.close()=,=br=,=br=,=h6>>Downloads:=,=a>>Description data>>cardiac description.txt>>d=,=br=,=a>>Data>>cardiac data.zip>>d=,=br=,=br=,=p>>The datasets have a totality of 920 labeled examples for which there are 76 numerical attributes that specify various characteristics of the patients such as their age (age), sex (sex), cholesterol (chol), resting blood sugar (fbs), etc. The objective variable of the problem called num which represents the diagnosis of heart disease. The latter has a value of 0 if there is no disease in the individual and values ​​from 1 to 4 according to different types of heart disease.=,=p>>Despite the large number of attributes, 10 of them were not used , as expressed in the description of the datasets, these being thalsev, thalpul, earlobe, lvx1, lvx2, lvx3, lvx4, lvf, cathef and junk. On the other hand, there is a large number of predictors with missing values ​​which are represented by the value -9. There are 20 of them with more than 50% missing values ​​with respect to all the records.=,=br=,=h2>>Data preparation=,=hr=,=p>>The first thing that was done was import the data corresponding to the four databases in the RapidMiner tool, verifying that they were all loaded with the same types for each of their attributes. Subsequently, they were joined using the Append component and thus obtaining the set with all the labeled data.=,=p>>The records with value -9 were declared as missing values ​​and first the id columns and those with more than 50% of missing values ​​considering that most of these concern attributes not used as expressed in the description of the data since these can generate difficulties for learning the models to be created, the rest were replaced by the average of the data corresponding to each predictor according to the case.=,=p>>Other procedures carried out were the removal of outliers using the Detect Outliers and Filter Examples operators to remove the 10 farthest outliers from the 10 closest neighbors according to Euclidean distance, the changing the type of the target variable from numeric to polynomial using Numerical to Polynomial and standardizing all predictors with the comp Normalize component.=,=br=,=h2>>Modelado=,=hr=,=p>>For the models to be developed, supervised and classification models that supported polynomial variables such as Naive Bayes, Decision Trees, K- Nearest Neighbors and the assembly algorithms, one being bagged as Random Forest and another boosting as Gradient Boosted Trees.=,=p>> Of those studied, K-Nearest Neighbors were discarded due to the high dimensionality of the data and Trees of Decision by including Random Forest since the latter is an evolution of the former in being less prone to overfitting as it is composed of many trees with randomly selected predictors with replacement in each one, thus providing generalization to the model.=,=p>>Therefore Therefore, the Naive Bayes models were chosen as a baseline, Random Forest and Gradient Boosting Trees, applying in all cases 10-fold Cross Validation with the same local random seed number 1992 and measuring the performances. it is for everyone.=,=h6>>Process in RapidMiner=,=img>>cardiac-0.jpg>>100%>>550=,=a>>Download process>>Heart Disease.rmp>>d=,=br=,=br=,=h2>>Results=,=hr=,=p>>To evaluate the models and measure their performance, the confusion matrices were studied, obtaining the following tables.=,=h6>>Naive Bayes=,=img>>cardiac-1.jpg>>100%>>250=,=br=,=br=,=h6>>Random Forest=,=img>>cardiac-2.jpg>>100% >>250=,=p>>Params: num trees = 1000, criterion = gain_ratio, min leaf size = 2, min size for split = 4 and max depth = 10.=,=h6>>Gradient Boosting Trees=,= img>>cardiac-3.jpg>>100%>>250=,=p>>Params: num trees = 50, max depth = 5, min rows = 10, min split imp. = 1.0E-5, num bins = 20, read. rate = 0.01. =,=br=,=h2>>Conclusions=,=hr=,=p>>It can be seen that the ones that give the best accuracy results are those of assemblages with Gradient Boosting Trees in the lead and Random Forest very close. Likewise, although the global performance is always very important, for this case of diagnosis of diseases, the accuracy for class 0 could be considered even more relevant, which represents the percentage of success in predicting the absence of disease since they can allow high values ​​of false positives in problems as sensitive as heart health in which, if they occur, they can even be terminal for patients. Regarding this, it can be highlighted that Naive Bayes has the highest precision for class 0, although it is also the one with the least recall for this, identifying the largest number of people who do not have heart disease as if they had it among the models made. .=,=p>>To decide between which model is better between Naive Bayes and his remarkable prediction for class 0 and Gradient Boosting Machines and his incredible acurracy predicting better for all other classes it is necessary to know how deep the difference between heart diseases type 4, 3, 2 and 1. If the difference is great in terms of the treatment required and the possibilities of the patient, then the best algorithm is Gradient Boosting Machines, but if it is not the case between these models, I would choose Naive Bayes to make sure to have the fewest false negatives for the disease. Unfortunately, this information is not available in the dataset and in a real case, medical teams would have to be consulted to make the appropriate decision." 
    },
    {
        "unidad":"case",
        "unit":"case",
        "titulo":"Bienes raíces en Ames Iowa",
        "title":"Ames housing",
        "description":"",
        "descripcion":"",
        "contenido":"h2>>Contexto=,=p>>La industria de bienes raíces o real estate es una de las que genera más actividad y crecimiento en el mundo. En los Estados Unidos según datos anualizados del Bureau of Economic Analyisis (BEA) para el segundo cuarto de 2021 el sector vinculado a la vivienda es una de las industrias que genera mayor actividad económica en dicho país con un PIB de 2.908 trillones de dólares. Conforme datos también anualizados del FMI de octubre 2021, esta cifra supera el PIB de países como España, Italia, Brasil, Australia, Canadá, Rusia, Corea del Sur y equivale a 48.38 veces el de Uruguay.=,=p>>El valor de compra de una propiedad depende de muchos factores entre los cuales los expertos destacan los siguientes.=,=p>>Localización: Uno de los principales está vinculado a la famosa frase de la industria de bienes raíces “location, location, location” haciendo referencia a que gran parte del valor de un inmueble depende la calidad de oportunidades y servicios que se puedan obtener viviendo en ese lugar como por ejemplo posibilidades de obtener un buen empleo, seguridad, proximidad a escuelas de calidad, etc.=,=p>>Espacio: Es un factor fundamental para determinar su valor haciendo especial énfasis en el espacio utilizable que es aquél en el que no se cuenta el área correspondiente a áticos, sótanos y los lugares que las personas rara vez utilizan en su día a día. Muchas personas eligen vivir en suburbios en los que pueden disponer de interiores, jardines o fondos más amplios que vivir en el centro de la ciudad en una inmueble más pequeño por más que esto le quite la posibilidad de acceder a alguna oportunidad o servicio con la misma facilidad. Esto se puede ver en gran extensión en países como Estados Unidos o Canadá, pero también en Uruguay con Ciudad de la Costa.=,=p>>Formato: Los compradores están dispuestos a pagar por conseguir una vivienda que sea acorde a sus necesidades actuales o a sus perspectivas de futuro como sería el caso de una familia que tiene expectativas de tener más hijos y busca una casa lo suficientemente grande para ello. Dentro de estas características se destacan algunas como el número de habitaciones disponibles, la cantidad de baños y si el inmueble tiene garage o no. Por otra parte, el formato en si de la vivienda está relacionado fuertemente a su precio, no es lo mismo una casa que un apartamento con todas las variantes que estos pueden tener.=,=p>>Estado: El estado de una propiedad es un factor fundamental para su precio siendo esta es la razón por la cual es conveniente económicamente reacondicionar una propiedad antes de venderla. Por otra parte, a mayor antigüedad las construcciones son más propensas a tener defectos y por lo tanto se valora la modernidad.=,=br=,=h2>>Entendimiento del negocio=,=hr=,=p>>Este caso de estudio tiene como finalidad poder desarrollar el modelo CRISP-DM sobre un problema con fines académicos y no teniendo como objetivo un deployment o que forme parte de un producto más grande.=,=p>>Se realizará preparación de datos, modelado y evaluación para predecir el valor de viviendas a partir de características de las propiedades o de sus entornos utilizando Python y librerías como SciKitLearn, Pandas, Numpy, Seaborn, entre otras. El código completo al que se hace referencia en este trabajo se encuentra disponible en la última sección de Anexo.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>Para este proyecto se cuenta con datos de venta de propiedades de la localidad de 66.000 habitantes llamada Ames del estado de Iowa Estados Unidos entre los años 2006 y 2010 obtenidos del Ames City Assessor´s Office y preprocesados por Dean De Cock PhD en Probabilidad de la Iowa State University. Los registros se encuentran en dos datasets, uno de training y uno de test.=,=h6>>Descargas:=,=a>>Descripción de datos>>data_description.txt>>d=,=br=,=a>>train.csv>>train.csv>>d=,=br=,=a>>test.csv>>test.csv>>d=,=br=,=br=,=h4>>Análisis de atributos=,=p>>La estructura de los datasets a utilizar tiene 78 predictores más la variable objetivo numérica continua en el caso del de train. Esta se querrá predecir con el modelado a desarrollar, siendo de esta manera un problema supervisado de regresión.=,=p>>De las 78 variables de entrada es muy importante tener en consideración sus diferentes tipos que en este caso son definidos en la descripción de datos para cada uno. De esta manera se cuenta con:=,=p>>- 44 categóricas (23 nominales y 21 ordinales)=,=p>>- 34 numéricas (19 continuas y 15 discretas)=,=p>> Aunque las variables nominales y ordinales son categóricas se diferencian en que las primeras son ordenables mientras que las segundas son simplemente características cuyos valores dificimente son comparables entre ellos en el sentido de que uno se pueda considerar mayor o menor que el otro de por sí.=,=p>>Una mejor explicación se podría dar considerando atributos del problema con ExterQual (ordinal) y Foundation (nominal).=,=p>>- ExterQual: hace referencia a la calidad de los materiales del exterior de la casa y sus valores posibles son Ex (excelente), Gd (bueno), TA (promedio), Fa (justo), Po (pobre) por lo cual es fácil de trasladar a una escala en la cual los primeros valores son mayores a los últimos dado que es indiscutible que en cualquier caso una calidad excelente es mejor que una promedio o una pobre.=,=p>>- Foundation: describe a los cimientos de una casa contando con las clases BrkTil (ladrillos y tejas), CBlock (bloque de cemento), PConc (concreto), Slab (losa), Stone (piedra) y Wood (madera). Dado que en algunos terrenos podría ser mejor contar con cimientos de un tipo y en otros de otro con lo que el orden dependería de circunstancias ajenas a la variable en si dificilmente pueda ser ordenable.=,=br=,=h4>>Análisis de ejemplos=,=p>>En cuanto a la cantidad de ejemplos, hay 1460 datos etiquetados en el train y 1459 sin etiquetar en el test. El caso de estudio se enfocará en trabajar con el dataset de entrenamiento dado que este nos permite medir la performance al desarrollar modelos de predicción comparando los valores obtenidos por los mismos con las etiquetas brindadas. De esta forma todo el análisis que se realizará a continuación refiere a ese conjunto de datos.=,=br=,=h5>>Datos Faltantes=,=p>>Analizando los datos faltantes se puede notar que todos corresponden a la variables continua LotFrontage (28), GarageYrBlt (81) y MasVnrArea (8).=,=img>>7.jpg>>25%>>150 =,=p>>A proósito, es conveniente destacar que en muchas variables categóricas existe la palabra NA como uno de los valores posibles siendo un ejemplo de ellos GarageType en cuyo caso se utiliza para referir a que las propiedades que no tienen garage. Estas observaciones con NA no deben ser confundidas con datos faltantes.=,=br=,=h5>>Correlaciones=,=p>>Es interesante observar la correlación que mantienen tanto cada uno con la variable objetivo como entre sí. =,=p>>Estudiaremos la matriz de correlación generada a partir del coeficiente de Pearson que mide la correlación lineal entre dos variables en una escala entre -1 y 1 interpretándose como cuanto más cerca del cero menor dependencia lineal y cuanto más lejos mayor ya sea directa en el caso de la aproximación al 1 u opuesta en los acercamientos al -1. Este coeficiente solo puede ser calculado para variables numéricas o en su defecto, como se hará a continuación, se pueden agregar también a las ordinales tras convertirlas a una escala numérica. =,=p>>Filtrando entre los atributos que tienen una correlación mayor a 0.5 o menor a -0.5 respecto a la variable objetivo según Pearson se obtienen de forma ordenada los siguientes 13 predictores.=,=img>>ames-1.jpg>>25%>>250=,=br=,=br=,=p>>Realizando los cálculos para las correlaciones con la variable objetivo con 55 predictores entre los numéricos y los ordinales tras haber sido codificados para que también lo sean, solamente 13 muestran una predicción superior a 0.5 con esta lo cual es un 23%. Además, estas realaciones todas son de forma directa y ninguna opuesta.=,=p>>A continuación se calculan las correlaciones lineales entre los predictores resultando esta matriz de Pearson.=,=img>>ames-2.jpg>>100%>>400=,=br=,=br=,=p>>En cuanto a las correlaciones entre los propios predictores se obtuvieron valores positivos en todos los casos con lo cual todos tienen una cierta dependencia lineal directa por más mínima que pueda ser. Se consideraron a los valores superiores a >0.66 como correlaciones altas lo cual se da entre los pares GrLivArea y TotRmsAvgGrd con 0.82, GarageArea y GarageCars con 0.88, TotalBsmtSF y 1stFlrSF con 0.81.=,=br=,=h5>>Outliers=,=p>>Estudiando las distribuciones de los atributos que tienen alta correlación con la variable a predecir y baja correlación entre sí se pueden observar outliers para GrLivArea en sus valores sobre 4000 (esto es expresado por el propio Dean De Cock en su paper sobre el problema) y en TotalBsmtSF por sobre 3000 como se muestra en las representaciones a continuación.=,=img>>ames-3.jpg>>50%>>300=,=img>>ames-4.jpg>>50%>>300=,=br=,=br=,=br=,=h5>>Sesgos=,=p>>Visualizando los histogramas se pudieron ver sesgos hacia la derecha en YearBuilt y YearRemodAdd los cuales corresponden respectivamente al año de construcción y al de la última remodelación de las propiedades. Se puede ver que en el registro hay bastantes más datos de casas hechas o remodeladas hace pocos años con un 40% y un 59% de la muestra con año posterior a 1980 en cada caso.=,=img>>ames-5.jpg>>50%>>300=,=img>>ames-6.jpg>>50%>>300=,=br=,=br=,=br=,=h5>>Distribuciones de atributos nominales=,=p>>Al realizar el análisis de las variables nominales es útil observar sus distintas categorías en relación a la variable objetivo lo cual es posible con diagramas de caja que nos dejan visualizar cuanta variabilidad hay en el dataset entre las clases de una categoría nominal respecto a la variable objetivo con el rango, la mediana y los datos atípicos que tienen. Si la variabilidad es mucha entonces es una buena variable para aprender de ella.=,=p>>Los diagramas más destacables se encuentran posteriormente. =,=p>>-Neighborhood=,=img>>ames-13.jpg>>80%>>400=,=br=,=br=,=p>>-HouseStyle=,=img>>ames-14.jpg>>80%>>300 =,=br=,=br=,=p>>-Foundation=,=img>>ames-8.jpg>>80%>>300=,=br=,=br=,=h2>>Preparación de datos=,=hr=,=p>>=,=h5>>Conjuntos de entrenamiento generados=,=p>>Se armaron 3 conjuntos de datos para entrenamiento distintas características para aplicar modelos sobre ellos y poder realizar comparaciones.=,=p>>- Datos de entrenamiento 1: Contiene todas los predictores del dataset con las variables numéricas y categóricas codificadas y sin datos faltantes.=,=p>>- Datos de entrenamiento 2: Con atributos seleccionados como se explicará a continuación, variables numéricas y categóricas codificadas, pero sin que se les haya realizado más pre procesamiento. Su objetivo es probar el feature selection hecho.=,=p>>- Datos de entrenamiento 3: Comprende los mismos atributos seleccionados para el caso anterior, variables numéricas y categóricas codificadas pero además sobre este se realizaron otras técnicas de pre procesamiento de datos mostradas debajo. Su finalidad es probar los métodos de pre processing aplicados. =,=br=,=h5>>Carga de datos=,=p>>Los datos fueron cargados y se reemplazaron los valores con NA de las variables categóricas por una palabra que significa que no hay eso de lo que trata el predictor lo cual puede tratarse de garages, sótanos, etc.=,=br=,=h5>>Codificación atributos ordinales (para datos de entrenamiento 1, 2 y 3)=,=p>>Los atributos ordinales se codificaron transformando sus escalas a discretas con el OrdinalEncoder() de SKLearn, lo importante aquí es mantener el ordenamiento y que no pase como en Python que codifica al revés y luego al calcular las correlaciones por ejemplo con la variable objetivo se obtiene el valor opuesto al que debería pero igualmente esto puede arreglarse fácil como se muestra en el Anexo.=,=br=,=h5>>Selección de atributos numéricos y ordinales (para datos de entrenamiento 2 y 3)=,=p>>Nos quedamos con los predictores que mantuvieran una correlación superior a 0.5 o inferior a -0.5 respecto al atributo objetivo y  una correlación con las demás que sea menor a 0.66 o mayor a -0.66 en todos los casos. Obteniendo las siguientes variables.=,=img>>vars.jpg>>100%>>40=,=br=,=br=,=br=,=h5>>Selección de atributos nominales (para datos de entrenamiento 2 y 3) =,=p>>Se observaron diagramas diagramas de caja analizando cuales brindan la mayor variabilidad respecto a la variable objectivo. De esto se consiguieron los predictores HouseStyle, Foundation, CentralAir y Neighborhood. Los demás se descartaron por por no poder justificar que tengan una variabilidad considerable entre sus distintas clases respecto a la variable objetivo que sea determinante para el problema.=,=br=,=h5>>Codificación de atributos nominales (para datos de entrenamiento 1, 2 y 3)=,=p>>Los atributos nominales fueron codificados con one hot encoding que se encuentra en SKLearn y en Pandas. Lo que hace es generar dummy variables que no son otra cosa que nuevos atributos binarios por cada uno de los diferentes valores que pueden serguir las variables nominales seleccionadas lo cual hace que las variable original no pierda su naturaleza nominal de no tener un orden definido.=,=br=,=h5>>Remoción de outliers (para datos de entrenamiento 3)=,=p>>Tras haber seleccionado los atributos con los que se trabajará y fundamentándonos en el análisis hecho con antelación se prosigió a eliminar outliers quitando los valores de GrLivArea<4000 y TotalBsmtSF>3000.=,=h6>>GrLivArea=,=img>>con outliers 1.JPG>>50%>>250=,=img>>sin outliers 1.jpg>>50%>>250=,=br=,=br =,=h6>>TotalBsmtSF=,=img>>con outliers 2.jpg>>50%>>250=,=img>>sin outliers 2.jpg>>50%>>250=,=br=,=br=,=h5>>Tratamiento del sesgo (para datos de entrenamiento 3)=,=p>>Se trató el sesgo en los atributos YearBuilt y YearRemodAdd con transformaciones logarítmicas.=,=h6>>YearBuilt=,=img>>yb sin log.jpg>>50%>>250=,=img>>yb con log.jpg>>50%>>250=,=br=,=br=,=h6>>YearRemodAdd=,=img>>yra sin log.jpg>>50%>>250=,=img>>yra con log.jpg>>50%>>250=,=br=,=br=,=br=,=h5>>Datos faltantes (para datos de entrenamiento 1)=,=p>>Sustituimos por el promedio de los atributos en las filas que contenían datos faltantes para el caso 1. Los conjuntos 2 y 3 no tenían missing values.=,=img>>1antes.jpg>>30%>>150=,=img>>1despues.jpg>>28%>>150=,=br=,=br=,=br=,=h5>>Estandarización (para datos de entrenamiento 3)=,=p>> Se estandarizaron las variables numéricas y ordinales para que sus escalas no sean determinantes.=,=h2>>Modelado=,=hr=,=p>>En la decisión de los modelos a desarrollar es fundamental considerar el tipo de problema el cual en este caso consiste en predecir el valor de una propiedad a partir de un conjunto de predictores y por lo tanto se trata de uno supervisado y de regresión. Existen un conjunto de algoritmos que se pueden emplear en estas condiciones entre los cuales se encuentran regresión lineal considerando también sus variaciones Lasso y Ridge, árboles de decisión, random forests, gradient boosting regression, k-nearest neighbors, support vector regression, entre otros. Para decidir entre los modelos aplicables es necesario ver la estructura de los datos que tenemos en este problema en particular.=,=p>>Como se estudió en la sección de análisis de datos con los coeficientes de Pearson y los diagramas, los predictores mantienen una relación lineal con la salida lo cual se acentúa para los atributos seleccionados y trabajados en el data pre processing. Como dicho algoritmo asume una relación lineal entre las entradas y la salida, aplicar regresión lineal para este caso es una buena idea por lo que será empleada a continuación. Además, aplicaremos el algoritmo de ensamble Random Forest para hacer una comparación entre ambos.  =,=p>>Los modelos seleccionados será entrenado con los 3 conjuntos de datos de entrenamiento. Para estos se utilizará cross validation con 10 pliegos de forma tal de evitar el problema en el que la parte de testeo pueda llegar a ser más fácil o más dificil para algún modelo en específico como podría pasar con la técnica hold out.=,=br=,=h2>>Resultados=,=hr=,=p>>Para la medición de resultados se usarán dos coeficientes, el primero es RMSE o root mean squared error que mide la raíz cudrada del promedio de los cuadrados de las diferencias entre las predicciones y los datos etiquetados del dataset train al cuadrado por lo que cuanto menor mejor.=,=p>>Se consiguieron las siguientes cifras.=,=h5>>Regresión lineal=,=p>>- Datos de entrenamiento 1: RMSE = 25935=,=p>>- Datos de entrenamiento 2: RMSE = 25744=,=p>>- Datos de entrenamiento 3: RMSE = 24911=,=br=,=h5>>Random Forest=,=p>>Con 1000 árboles y máximo de atributos a considerar en los splits de 20.=,=p>>- Datos de entrenamiento 1: RMSE = 24564=,=p>>- Datos de entrenamiento 2: RMSE = 27302=,=p>>- Datos de entrenamiento 3: RMSE = 27692=,=br=,=h2>>Conclusiones=,=hr=,=p>>=,=p>>En los resultados se puede visualizar que seleccionar atributos tanto ordinales como nominales cuidadosamente y aplicar otros métodos de pre procesamiento de datos como estandarización, remoción de outliers y transformaciones merece la pena para el caso de la regresión lineal dado que esto optimizó el parámetro utilizados para medir la performance lo cual se puede ver tanto del caso 1 al 2 como del 2 al 3.=,=p>>En cuanto a Random Forest se observa que da mejores resultados con los atributos originales sin realizar seleccion de atributos ni otros métodos de procesamiento de datos. Esto puede deberse a que con estas modificaciones se pierda parte de la buena representación del problema el cual es el único requerimiento exigido por este algoritmo en cuanto a la preparación de datos.=,=p>>Con lo anterior se comprueba en un caso practico que lo que puede ser una buena preparación de datos para un algoritmo de machine learning puede ser mala para otro. De esta forma, para obtener una mejor performance para un problema no solo basta con aplicar distintos algoritmos a un mismo conjunto de prueba preparado y observar cual da mejor sino que también es necesario probar diferentes preparaciones de datos a ser aplicadas a los distintos algoritmos de machine learning de forma tal de sacarles el máximo provecho.=,=p>>Por otra parte, resulta interesante que a partir del desarrollo realizado tanto en el ánálisis de datos por ejemplo al ver la alta correlación que hay entre el tamaño disponible y el precio de la propiedad como en la preparación de datos y modelado siendo un caso conreto la selección de ciertas variables nominales que mejoran mucho el rendimiento de los modelos como es el caso del barrio, se logra ver como afectan en un caso real las características de las propiedades que los expertos mencionan como las más importantes lo que se analizó en un principio en la sección de contexto. Se puede ver como al incluir y procesar correctamente esos factores que ellos más destacan es lo que más termina determinando que al armar nuestros modelos estos hagan mejores predicciones.=,=br=,=h2>>Anexo=,=hr=,=iframe>>100%>>550",
        "content":"h2>>Context=,=p>>The real estate industry is one of the industries that generates the most activity and growth in the world. In the United States, according to annualized data from the Bureau of Economic Analysis (BEA) for the second quarter of 2021, the sector linked to housing is one of the industries that generates the greatest economic activity in that country with a GDP of 2,908 trillion dollars. According to data also annualized from the IMF for October 2021, this figure exceeds the GDP of countries such as Spain, Italy, Brazil, Australia, Canada, Russia, South Korea and is equivalent to 48.38 times that of Uruguay.=,=p>>The value buying a property depends on many factors, among which experts highlight the following.=,=p>>Location: One of the main ones is linked to the famous phrase of the real estate industry “location, location, location” making reference to the fact that a large part of the value of a property depends on the quality of opportunities and services that can be obtained by living in that place, such as the possibilities of obtaining a good job, security, proximity to quality schools, etc.=,=p>>Space: It is a fundamental factor to determine its value, with special emphasis on usable space, which is the one in which the area corresponding to attics, basements and places that people rarely use in their day to day are not counted. Many people choose to live in suburbs where they can have larger interiors, gardens or funds than to live in the center of the city in a smaller property even though this deprives them of the possibility of accessing any opportunity or service with the same ease. This can be seen to a great extent in countries such as the United States or Canada, but also in Uruguay with Ciudad de la Costa.=,=p>>Format: Buyers are willing to pay to get a home that meets their current needs or to their future prospects, as would be the case of a family that has expectations of having more children and is looking for a house large enough to do so. Some of these characteristics stand out, such as the number of rooms available, the number of bathrooms and whether the property has a garage or not. On the other hand, the format of the house itself is strongly related to its price, a house is not the same as an apartment with all the variants that these may have.=,=p>>Status: The status of a property is a fundamental factor for its price, this being the reason why it is economically convenient to recondition a property before selling it. On the other hand, the older the constructions are, the more likely they are to have defects and therefore modernity is valued.=,=br=,=h2>>Understanding the business=,=hr=,=p>>This case of The purpose of this study is to be able to develop the CRISP-DM model on a problem for academic purposes and not with a deployment objective or as part of a larger product.=,=p>>Data preparation, modeling and evaluation will be carried out to predict the value of homes from characteristics of the properties or their environments using Python and libraries such as SciKitLearn, Pandas, Numpy, Seaborn, among others. The complete code referred to in this work is available in the last section of the Annex.=,=br=,=h2>>Data knowledge=,=hr=,=p>>For this project we have Property sales data for the town of 66,000 inhabitants called Ames in the state of Iowa United States between 2006 and 2010 obtained from the Ames City Assessor's Office and preprocessed by Dean De Cock PhD in Probability from Iowa State University. The records are in two datasets, one for training and one for test.=,=h6>>Downloads:=,=a>>Data description>>data_description.txt>>d=,=br=,=a>>train.csv>>train.csv>>d=,=br=,=a>>test.csv>>test.csv>>d=,=br=,=br=,=h4>>Analysis of attributes =,=p>>The structure of the datasets to be used has 78 predictors plus the continuous numerical target variable in the case of the train predictor. This will be predicted with the modeling to be developed, thus being a supervised regression problem.=,=p>>Of the 78 input variables, it is very important to take into account their different types, which in this case are defined in the description of data for each. In this way we have:=,=p>>- 44 categorical (23 nominal and 21 ordinal)=,=p>>- 34 numerical (19 continuous and 15 discrete)=,=p>> Although the nominal and Ordinals are categorical, they differ in that the former are orderable while the latter are simply characteristics whose values ​​are hardly comparable between them in the sense that one can be considered greater or less than the other by itself.=,=p>> A better explanation could be given considering attributes of the problem with ExterQual (ordinal) and Foundation (nominal).=,=p>>- ExterQual: refers to the quality of the materials outside the house and its possible values ​​are Ex ( excellent), Gd (good), TA (average), Fa (fair), Po (poor) for which it is easy to transfer to a scale in which the first values ​​are greater than the last since it is indisputable that in any case excellent quality is better than average or poor.=,=p>>- Foundation: describes the cimie nts of a house with the classes BrkTil (bricks and tiles), CBlock (cement block), PConc (concrete), Slab (slab), Stone (stone) and Wood (wood). Given that in some areas it might be better to have foundations of one type and in others of another, so the order would depend on circumstances unrelated to the variable itself, it can hardly be ordered.=,=br=,=h4>>Analysis of examples=,=p>>As for the number of examples, there are 1460 labeled data in the train and 1459 unlabeled data in the test. The case study will focus on working with the training dataset since it allows us to measure performance when developing prediction models by comparing the values ​​obtained by them with the labels provided. In this way, all the analysis that will be carried out below refers to that data set.=,=br=,=h5>>Missing Data=,=p>>Analyzing the missing data, it can be seen that they all correspond to the continuous variables LotFrontage (28), GarageYrBlt (81) and MasVnrArea (8).=,=img>>7.jpg>>25%>>150 =,=p>>By the way, it is convenient to point out that in many categorical variables there is the word NA as one of the possible values ​​being an example of them GarageType in which case it is used to refer to properties that do not have a garage. These observations with NA should not be confused with missing data.=,=br=,=h5>>Correlations=,=p>>It is interesting to observe the correlation that each one maintains both with the target variable and with each other. =,=p>>We will study the correlation matrix generated from the Pearson coefficient, which measures the linear correlation between two variables on a scale between -1 and 1, interpreted as the closer to zero, the less linear dependence, and the further, greater. be direct in the case of the approach to 1 or opposite in the case of approaches to -1. This coefficient can only be calculated for numerical variables or, failing that, as will be done below, they can also be added to the ordinal variables after converting them to a numerical scale. =,=p>>Filtering between the attributes that have a correlation greater than 0.5 or less than -0.5 with respect to the target variable according to Pearson, the following 13 predictors are obtained in an orderly manner.=,=img>>ames-1.jpg >>25%>>250=,=br=,=br=,=p>>Performing the calculations for the correlations with the target variable with 55 predictors between the numeric and the ordinal ones after having been coded so that they are also ordinal, only 13 show a prediction greater than 0.5 with this one, which is 23%. In addition, these relationships are all direct and none are opposite.=,=p>>Next, the linear correlations between the predictors are calculated, resulting in this Pearson matrix.=,=img>>ames-2.jpg>>100% >>400=,=br=,=br=,=p>>Regarding the correlations between the predictors themselves, positive values ​​were obtained in all cases, with which they all have a certain direct linear dependence, no matter how minimal it may be . Values ​​greater than >0.66 were considered high correlations, which occurs between the pairs GrLivArea and TotRmsAvgGrd with 0.82, GarageArea and GarageCars with 0.88, TotalBsmtSF and 1stFlrSF with 0.81.=,=br=,=h5>>Outliers=, =p>>Studying the distributions of the attributes that have a high correlation with the variable to be predicted and a low correlation with each other, outliers can be observed for GrLivArea in its values ​​above 4000 (this is expressed by Dean De Cock himself in his paper on the problem) and TotalBsmtSF above 3000 as shown in the renderings below.=,=img>>ames-3.jpg>>50%>>300=,=img>>ames-4.jpg>>50% >>300=,=br=,=br=,=br=,=h5>>Biases=,=p>>Viewing the histograms, biases to the right could be seen in YearBuilt and YearRemodAdd, which correspond respectively to the year of construction and to the last remodeling of the properties. It can be seen that in the registry there is much more data on houses built or remodeled a few years ago, with 40% and 59% of the sample having a year after 1980 in each case.=,=img>>ames-5.jpg >>50%>>300=,=img>>ames-6.jpg>>50%>>300=,=br=,=br=,=br=,=h5>>Nominal attribute distributions=,= p>>When performing the analysis of the nominal variables, it is useful to observe their different categories in relation to the target variable, which is possible with box plots that allow us to visualize how much variability there is in the dataset between the classes of a nominal category with respect to the target variable with the range, median and outliers they have. If the variability is high then it is a good variable to learn from.=,=p>>The most notable diagrams are found later. =,=p>>-Neighborhood=,=img>>ames-13.jpg>>80%>>400=,=br=,=br=,=p>>-HouseStyle=,=img>>ames-14.jpg>>80%>>300 =,=br=,=br=,=p>>-Foundation=,=img>>ames-8.jpg>>80%>>300=,=br=,=br=,=h2>>Data preparation=,=hr=,=p>>=,=h5>>Training sets generated=,=p>>3 data sets were assembled for training different characteristics to apply models on them and be able to make comparisons.=,=p>>- Training data 1: Contains all the predictors of the dataset with the coded numerical and categorical variables and without missing data.=,=p>>- Training data 2: With attributes selected as explained below, numeric and categorical variables coded, but without further pre-processing. Its objective is to test the feature selection made.=,=p>>- Training data 3: It includes the same attributes selected for the previous case, numeric and categorical variables encoded, but also other data pre-processing techniques shown were performed on this below. Its purpose is to test the applied pre-processing methods. =,=br=,=h5>>Loading data=,=p>>The data was loaded and the values ​​with NA of the categorical variables were replaced by a word that means that there is not what the predictor is about. which can be garages, basements, etc.=,=br=,=h5>>Coding ordinal attributes (for training data 1, 2 and 3)=,=p>>Ordinal attributes were coded transforming their scales to discrete with SKLearn's OrdinalEncoder(), the important thing here is to maintain the ordering and not to happen as in Python that encodes backwards and then when calculating the correlations, for example with the target variable, the opposite value is obtained from what it should be, but this can still be easily fixed as shown in the Annex.=,=br=,=h5>>Selection of numerical and ordinal attributes (for training data 2 and 3)=,=p>>We kept the predictors that maintained a higher correlation to 0.5 or less than -0.5 with respect to the objective attribute and a correlation with the others that are a less than 0.66 or greater than -0.66 in all cases. Obtaining the following variables.=,=img>>vars.jpg>>100%>>40=,=br=,=br=,=br=,=h5>>Nominal attribute selection (for training data 2 and 3) =,=p>>Box plot diagrams were observed, analyzing which ones provide the greatest variability with respect to the objective variable. From this, the predictors HouseStyle, Foundation, CentralAir and Neighborhood were obtained. The others were discarded because they could not justify that they have considerable variability between their different classes with respect to the objective variable that is decisive for the problem.=,=br=,=h5>>Coding of nominal attributes (for training data 1 , 2 and 3)=,=p>>The nominal attributes were encoded with the one hot encoding found in SKLearn and Pandas. What it does is generate dummy variables that are nothing more than new binary attributes for each of the different values ​​that the selected nominal variables can have, which means that the original variables do not lose their nominal nature of not having a defined order.= ,=br=,=h5>>Removal of outliers (for training data 3)=,=p>>After having selected the attributes with which we will work and basing ourselves on the analysis done in advance, we proceeded to eliminate outliers by removing the values ​​of GrLivArea<4000 and TotalBsmtSF>3000.=,=h6>>GrLivArea=,=img>>con outliers 1.JPG>>50%>>250=,=img>>sin outliers 1.jpg>>50% >>250=,=br=,=br =,=h6>>TotalBsmtSF=,=img>>con outliers 2.jpg>>50%>>250=,=img>>sin outliers 2.jpg>>50%>>250=,=br=,=br=,=h5>>Treatment of bias (for training data 3)=,=p>>Treatment of bias in YearBuilt and YearRemodAdd attributes with logarithmic transformations.=,=h6>>YearBuilt=,=img>>yb sin log.jpg>>50%>>250=,=img>>yb con log.jpg>>50%>>250=,=br=,=br=,=h6>>YearRemodAdd=,=img>>yra sin log.jpg>>50%>>250=,=img>>yra con log.jpg>>50% >>250=,=br=,=br=,=br=,=h5>>Missing data (for training data 1)=,=p>>We substitute the average of the attributes in the rows that contained missing data for case 1. Sets 2 and 3 had no missing values.=,=img>>1antes.jpg>>30%>>150=,=img>>1despues.jpg>>28%>>150=,= br=,=br=,=br=,=h5>>Standardization (for training data 3)=,=p>> Numerical and ordinal variables were standardized so that their scales are not determinant.=,=h2>> Modeling=,=hr=,=p>>In deciding which models to develop, it is essential to consider the type of problem, which in this case consists of predicting the value of a property from a set of predictors and therefore it is a supervised one and regression. There is a set of algorithms that can be used in these conditions, among which are linear regression considering also its Lasso and Ridge variations, decision trees, random forests, gradient boosting regression, k-nearest neighbors, support vector regression, among others. To decide between the applicable models it is necessary to see the structure of the data we have in this particular problem.=,=p>>As studied in the data analysis section with the Pearson coefficients and the plots, the predictors maintain a linear relationship with the output, which is accentuated for the attributes selected and worked on in the data pre-processing. As said algorithm assumes a linear relationship between the inputs and the output, applying linear regression for this case is a good idea, so it will be used below. In addition, we will apply the Random Forest ensemble algorithm to make a comparison between the two. =,=p>>The selected models will be trained with the 3 training data sets. For these, cross validation will be used with 10 specifications in order to avoid the problem in which the testing part may become easier or more difficult for a specific model, as could happen with the hold out technique.=,=br =,=h2>>Results=,=hr=,=p>>Two coefficients will be used to measure the results, the first is RMSE or root mean squared error, which measures the square root of the average of the squares of the differences between the predictions and the labeled data of the dataset are trained squared, so the smaller the better.=,=p>>The following figures were obtained.=,=h5>>Linear regression=,=p>>- Training data 1: RMSE = 25935=,=p>>- Training data 2: RMSE = 25744=,=p>>- Training data 3: RMSE = 24911=,=br=,=h5>>Random Forest=,=p>>With 1000 trees and a maximum of attributes to consider in the splits of 20.=,=p>>- Training data 1: RMSE = 24564=,=p>>- Training data 2: RMSE = 27302=,=p >>- Training data 3: RMSE = 2769 2=,=br=,=h2>>Conclusions=,=hr=,=p>>=,=p>>In the results it can be seen that selecting both ordinal and nominal attributes carefully and applying other methods of pre-processing Data such as standardization, removal of outliers and transformations is worthwhile for the case of linear regression since this optimized the parameter used to measure performance, which can be seen both from cases 1 to 2 and from 2 to 3.=,= p>>Regarding Random Forest, it is observed that it gives better results with the original attributes without performing attribute selection or other data processing methods. This may be due to the fact that with these modifications part of the good representation of the problem is lost, which is the only requirement demanded by this algorithm in terms of data preparation.=,=p>>With the above it is verified in a practical case that what may be good data preparation for one machine learning algorithm may be bad for another. In this way, to obtain a better performance for a problem, it is not only enough to apply different algorithms to the same prepared test set and observe which one gives the best, but it is also necessary to test different data preparations to be applied to the different machine algorithms. learning in such a way as to get the most out of them.=,=p>>On the other hand, it is interesting that based on the development carried out both in data analysis, for example, when seeing the high correlation between the available size and the price of the property as in the preparation of data and modeling, being a concrete case the selection of certain nominal variables that greatly improve the performance of the models, as is the case of the neighborhood, it is possible to see how they affect the characteristics of the properties in a real case that the experts mention as the most important what was analyzed at the beginning in the context section. You can see how correctly including and processing those factors that stand out the most is what ends up determining that when building our models they make better predictions.=,=br=,=h2>>Annex=,=hr=,=iframe >>100%>>550"
    },
    {
        "unidad":"case",
        "unit":"case",
        "titulo":"Segmentación de clientes",
        "title":"Customer segmentation",
        "descripcion":"",
        "description":"",
        "contenido":"h2>>Contexto=,=p>>Para cualquier empresa resulta fundamental distinguir cómo son sus clientes de forma tal de que les permita generar estrategias para ciertos públicos en particular como sacar una nueva línea de productos pensando en cierto sector o aprovecharse de poder hacer uso del marketing por microsegmentos que se basa en conocer distintos tipos de clientes y desarrollar campañas específicas para cada uno de estos sectores que muchas veces están motivados a consumir por razones distintas, entre otras razones.=,=p>>Más allá de que una estrategia o campaña sea buena o mala, con alto o bajo presupuesto, resulta impresindible que esté dirigida al público correcto y para ello los datos y su análisis con herramientas de machine learning no supervisado como clustering nos pueden ser de gran ayuda al permitirnos distinguir subgrupos en un conjunto de datos que bien pueden corresponder a los diferentes clases de clientes de una compañía.=,=br=,=h2>>Entendimiento de negocio=,=hr=,=p>>En este caso de estudio se desarrollará el modelo CRISP-DM sobre el problema no supervisado de segmentación de los clientes de un shopping con fines académicos sin tener como fin un deployment o que un producto más grande. Se realizará preparación de datos, modelado y evaluación para encontrar clusters o subgrupos a partir de valores acerca de diferentes cualidades de los compradores ayudándonos de la herramienta de minería de datos RapidMiner.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>En este caso de estudio se posee un conjunto de datos obtenidos de Kaggle que serán estudiados con la herramienta RapidMiner. Este cuenta con 200 ejemplos acerca de características de los clientes de una shopping representadas por los atributos dispuestos a continuación junto a sus descripciones e histogramas.=,=p>>- CustomerID (numérico): Simplemente un identificador único del cliente.=,=p>>- Gender (nominal): Si el cliente es masculino o femenino. La proporción es de 56% de mujeres frente a 44% de hombres.=,=img>>customer seg-1.jpg>>35%>>300=,=br=,=br=,=p>>- Age (numérico): Representa la edad del cliente La proprción. El grupo etario que contiene más individuos se sitúa desde en la década de los 26 a los 35 años.=,=img>>customer seg-2.jpg>>70%>>450=,=br=,=br=,=p>>- Annual Income (numérico): Corresponde al salario anual del cliente en miles de dólares. Los datos más repetidos se encuentran en el centro de la distribución yendo desde los 50 hasta los 90 mil dólares al año.=,=img>>customer seg-3.jpg>>70%>>450=,=br=,=br=,=p>>- Spending Score (numérico): Puntaje vinculado al consumo del cliente. Tiene un rango entre (0,100) y los valores más repetidos se encuentran en el centro de la distribución.=,=img>>customer seg-4.jpg>>90%>>450=,=br=,=br=,=p>>*Observación: Entre los datos no se hallan valores faltantes ni outliers.=,=h6>>Descargas:=,=a>>Datos>>Mall_Customers.csv>>d=,=br=,=br=,=h2>>Preparación de datos=,=hr=,=p>>Para algoritmos de clustering como k-means es importante verificar que no hay valores faltantes dado que no los soporta, eliminar outliers por ser muy sensible a ellos, manejar una baja dimensionalidad, solamente utilizar variables numéricas porque medir las distancias que terminan determinando las agrupaciones es más óptimo de esta manera y normalizar para que las escalas de los atributos no sean determinantes en los cálculos.=,=p>>De esta forma quitamos el parámetro CustomerID al no ser de interés para la formación de agrupaciones considerando que es diferente para cada fila, cambiamos el atributo Gender de categórico a numérico con un dummy encoding y quitamos outliers en base a distancias euclideanas. Respecto a los valores faltantes no se realizó ninguna técnica de procesamiento al no ser necesario para este caso particular y no se aplicó estandarización debido a que los predictores numéricos tienen unidades que utilizan rangos rangos similares.=,=br=,=h2>>Modelado=,=hr=,=p>>Empleamos k-means en RapidMiner cambiando la cantidad de clusteres comprendiendo los casos de k=2, k=3, k=4 y k=5 pero manteniendo siempre el mismo número de iteraciones del algoritmo con max runs = 10. Para cada caso se analizó la interpretabilidad del resultado obtenido con scatter plots y la performance según la distancia promedio de los puntos respecto a los centroides de los clusters.=,=h6>>Proceso en RapidMiner=,=img>>customer seg-5.jpg>>100%>>600=,=a>>Descargar proceso>>Customer Segmentation.rmp>>d=,=br=,=br=,=h2>>Resultados=,=hr=,=p>>Observando las gráficas resultantes de los modelos creados se llegó a que las mejores visualizaciones para darle significado a los clusters se obtenían de Spending Score contra Annual Income, Spending Score en función de Age y gráficos de barra de los clusters con colores para diferenciar a Gender.=,=h6>>K=2=,=p>>Avg. within centroid distance: -0.193=,=img>>customer seg-6.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-7.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-8.jpg>>75%>>400=,=br=,=br=,=h6>>K=3=,=p>>Avg. within centroid distance: -0.156=,=img>>customer seg-9.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-10.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-11.jpg>>75%>>400=,=br=,=br=,=h6>>K=4=,=p>>Avg. within centroid distance: -0.119=,=img>>customer seg-12.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-13.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-14.jpg>>75%>>400=,=br=,=br=,=h6>>K=5=,=p>>Avg. within centroid distance: -0.102=,=img>>customer seg-15.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-16.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-17.jpg>>75%>>400=,=br=,=br=,=h6>>Gráfica de Avg. within centroid distance en relación con k=,=img>>customer seg-18.jpg>>75%>>400=,=br=,=br=,=h2>>Conclusiones=,=hr=,=p>>Observado los resultados se puede llegar a que los clusters se van diferenciando visualmente más y más a medida que se aumenta el valor para k hasta llegar a 5. Si uno continúa aumentando esta cifra se empieza a complicar distinguir las agrupaciones según características de los clientes. Igualmente con k=5 para todos los clusteres hay gran cantidad de registros de género masculino y femenino sin diferenciarse largamente entre sí lo que podría mostrar que no existen enormes diferencias en el comportamiento de compra entre ambos casos.=,=p>> Con k=5 a partir de las tres gráficas del caso de pueden distinguir cualidades de cada una de las agrapaciones que aparecen:=,=p>>- Cluster 0 (naranja): Clientes de ingresos medios (40-70), todas las edades (18-70) y un puntaje de gasto medio (40-60).=,=p>>- Cluster 1 (negro): Clientes de altos ingresos (70-110), jóvenes (25-40) y de puntaje de gasto alto (60-100).=,=p>>- Cluster 2 (azul): Clientes de bajos ingresos (15-40), jóvenes (18-35) y de puntaje de gasto alto (60-100).=,=p>>- Cluster 3 (violeta): Clientes de altos ingresos (70-105), mediana edad (30-60) y puntaje de gasto bajo (0-40).=,=p>>- Cluster 4 (verde): Clientes de ingresos bajos (15-40), mediana edad (30-60) y puntaje de gasto bajo (0-40).=,=p>>Por otra parte, se puede ver como disminuye el promedio de distancias de los puntos de los clusteres respecto a sus centroides al subir el valor de k hasta llegar a 5 generándose un comportamiento asintótico que se manifiesta en la última gráfica.",
        "content":"h2>>Contexto=,=p>>For any company it is essential to distinguish what its customers are like in such a way that it allows them to generate strategies for certain audiences in particular, such as launching a new line of products with a certain sector in mind or taking advantage of being able to do use of micro-segment marketing that is based on knowing different types of customers and developing specific campaigns for each of these sectors that are often motivated to consume for different reasons, among other reasons.=,=p>>Whether a strategy or campaign is good or bad, with a high or low budget, it is essential that it is aimed at the right audience and for this data and its analysis with unsupervised machine learning tools such as clustering can be of great help to us by allowing us to distinguish subgroups in a set of data that may well correspond to the different classes of customers of a company.=,=br=,=h2>>Business understanding=,=hr=,=p>>En es The case of study will develop the CRISP-DM model on the unsupervised problem of customer segmentation in a shopping center for academic purposes, without the purpose of a deployment or a larger product. Data preparation, modeling and evaluation will be carried out to find clusters or subgroups from values ​​about different qualities of buyers with the help of the RapidMiner data mining tool.=,=br=,=h2>>Data knowledge=,=hr=,=p>>In this case study we have a set of data obtained from Kaggle that will be studied with the RapidMiner tool. This has 200 examples of customer characteristics of a shopping mall represented by the attributes listed below along with their descriptions and histograms.=,=p>>- CustomerID (numeric): Simply a unique identifier of the customer.=,= p>>- Gender (nominal): Whether the client is male or female. The proportion is 56% of women compared to 44% of men.=,=img>>customer seg-1.jpg>>35%>>300=,=br=,=br=,=p>>- Age (numeric): Represents the customer's age The proportion. The age group that contains the most individuals is between the ages of 26 and 35.=,=img>>customer seg-2.jpg>>70%>>450=,=br=,=br=,=p>>- Annual Income (numeric): Corresponds to the client's annual salary in thousands of dollars. The most repeated data is found in the center of the distribution ranging from 50 to 90 thousand dollars a year.=,=img>>customer seg-3.jpg>>70%>>450=,=br=,= br=,=p>>- Spending Score (numeric): Score linked to customer consumption. It has a range between (0.100) and the most repeated values ​​are in the center of the distribution.=,=img>>customer seg-4.jpg>>90%>>450=,=br=,=br=,=p>>*Note: There are no missing values ​​or outliers in the data.=,=h6>>Downloads:=,=a>>Data>>Mall_Customers.csv>>d=,=br=,=br=,=h2>>Data preparation=,=hr=,=p>>For clustering algorithms such as k-means it is important to verify that there are no missing values ​​since it does not support them, eliminate outliers as it is very sensitive to them, handle a low dimensionality, only use numerical variables because measuring the distances that end up determining the groupings is more optimal in this way and normalize so that the scales of the attributes are not decisive in the calculations.=,=p>>In this way we remove the CustomerID parameter, since it is not of interest for the formation of groups considering that it is different for each row, we change the Gender attribute from categorical to numeric with a dummy encoding and remove or utliers based on Euclidean distances. Regarding the missing values, no processing technique was performed as it was not necessary for this particular case and no standardization was applied because the numerical predictors have units that use similar range ranges.=,=br=,=h2>>Modeling =,=hr=,=p>>We use k-means in RapidMiner changing the number of clusters including the cases of k=2, k=3, k=4 and k=5 but always maintaining the same number of iterations of the algorithm with max runs = 10. For each case, the interpretability of the result obtained with scatter plots and the performance according to the average distance of the points with respect to the centroids of the clusters were analyzed.=,=h6>>Process in RapidMiner=,=img>>customer seg-5.jpg>>100%>>600=,=a>>Download Process>>Customer Segmentation.rmp>>d=,=br=,=br=,=h2>>Results=,=hr =,=p>>Observing the resulting graphs of the created models, it was found that the best visualizations to give meaning to the clusters were obtained from Spending Score with tra Annual Income, Spending Score as a function of Age and bar graphs of the clusters with colors to differentiate Gender.=,=h6>>K=2=,=p>>Avg. within centroid distance: -0.193=,=img>>customer seg-6.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-7.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-8.jpg>>75%>>400=,=br=,=br=,=h6>>K=3=,=p >>Avg. within centroid distance: -0.156=,=img>>customer seg-9.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-10.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-11.jpg>>75%>>400=,=br=,=br=,=h6>>K=4=,=p >>Avg. within centroid distance: -0.119=,=img>>customer seg-12.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-13.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-14.jpg>>75%>>400=,=br=,=br=,=h6>>K=5=,=p >>Avg. within centroid distance: -0.102=,=img>>customer seg-15.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-16.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-17.jpg>>75%>>400=,=br=,=br=,=h6>>Avg. plot within centroid distance in relation to k=,=img>>customer seg-18.jpg>>75%>>400=,=br=,=br=,=h2>>Conclusions=,=hr=,=p>>Observed the As a result, the clusters can be visually differentiated more and more as the value for k is increased until it reaches 5. If one continues to increase this figure, it becomes difficult to distinguish the groups according to the characteristics of the clients. Likewise, with k=5 for all the clusters, there is a large number of male and female records without differing greatly from each other, which could show that there are no huge differences in purchasing behavior between the two cases.=,=p>> With k =5 From the three graphs of the case, qualities of each of the groups that appear can be distinguished:=,=p>>- Cluster 0 (orange): Medium-income clients (40-70), all ages ( 18-70) and a medium spending score (40-60).=,=p>>- Cluster 1 (black): High-income clients (70-110), young people (25-40) and spending score high (60-100).=,=p>>- Cluster 2 (blue): Low-income clients (15-40), young (18-35) and high spending score (60-100).=,=p>>- Cluster 3 (purple): Clients with high income (70-105), middle age (30-60) and low spending score (0-40).=,=p>>- Cluster 4 (green ): Clients with low income (15-40), middle age (30-60) and low spending score (0-40).=,=p>>On the other hand, it can be e see how the average distance of the cluster points decreases with respect to their centroids as the value of k rises until it reaches 5, generating an asymptotic behavior that is manifested in the last graph."
    }
]