(self.webpackChunkportfolio=self.webpackChunkportfolio||[]).push([[179],{255:Ai=>{function Tn(Pi){return Promise.resolve().then(()=>{var an=new Error("Cannot find module '"+Pi+"'");throw an.code="MODULE_NOT_FOUND",an})}Tn.keys=()=>[],Tn.resolve=Tn,Tn.id=255,Ai.exports=Tn},950:(Ai,Tn,Pi)=>{"use strict";function an(t){return"function"==typeof t}let ol=!1;const xt={Promise:void 0,set useDeprecatedSynchronousErrorHandling(t){if(t){const e=new Error;console.warn("DEPRECATED! RxJS was set to use deprecated synchronous error handling behavior by code at: \n"+e.stack)}else ol&&console.log("RxJS: Back to a better error behavior. Thank you. <3");ol=t},get useDeprecatedSynchronousErrorHandling(){return ol}};function wr(t){setTimeout(()=>{throw t},0)}const Zo={closed:!0,next(t){},error(t){if(xt.useDeprecatedSynchronousErrorHandling)throw t;wr(t)},complete(){}},fp=Array.isArray||(t=>t&&"number"==typeof t.length);function gp(t){return null!==t&&"object"==typeof t}const Xo=(()=>{function t(e){return Error.call(this),this.message=e?`${e.length} errors occurred during unsubscription:\n${e.map((n,r)=>`${r+1}) ${n.toString()}`).join("\n  ")}`:"",this.name="UnsubscriptionError",this.errors=e,this}return t.prototype=Object.create(Error.prototype),t})();class fe{constructor(e){this.closed=!1,this._parentOrParents=null,this._subscriptions=null,e&&(this._ctorUnsubscribe=!0,this._unsubscribe=e)}unsubscribe(){let e;if(this.closed)return;let{_parentOrParents:n,_ctorUnsubscribe:r,_unsubscribe:i,_subscriptions:o}=this;if(this.closed=!0,this._parentOrParents=null,this._subscriptions=null,n instanceof fe)n.remove(this);else if(null!==n)for(let s=0;s<n.length;++s)n[s].remove(this);if(an(i)){r&&(this._unsubscribe=void 0);try{i.call(this)}catch(s){e=s instanceof Xo?mp(s.errors):[s]}}if(fp(o)){let s=-1,a=o.length;for(;++s<a;){const l=o[s];if(gp(l))try{l.unsubscribe()}catch(c){e=e||[],c instanceof Xo?e=e.concat(mp(c.errors)):e.push(c)}}}if(e)throw new Xo(e)}add(e){let n=e;if(!e)return fe.EMPTY;switch(typeof e){case"function":n=new fe(e);case"object":if(n===this||n.closed||"function"!=typeof n.unsubscribe)return n;if(this.closed)return n.unsubscribe(),n;if(!(n instanceof fe)){const o=n;n=new fe,n._subscriptions=[o]}break;default:throw new Error("unrecognized teardown "+e+" added to Subscription.")}let{_parentOrParents:r}=n;if(null===r)n._parentOrParents=this;else if(r instanceof fe){if(r===this)return n;n._parentOrParents=[r,this]}else{if(-1!==r.indexOf(this))return n;r.push(this)}const i=this._subscriptions;return null===i?this._subscriptions=[n]:i.push(n),n}remove(e){const n=this._subscriptions;if(n){const r=n.indexOf(e);-1!==r&&n.splice(r,1)}}}var t;function mp(t){return t.reduce((e,n)=>e.concat(n instanceof Xo?n.errors:n),[])}fe.EMPTY=((t=new fe).closed=!0,t);const Jo="function"==typeof Symbol?Symbol("rxSubscriber"):"@@rxSubscriber_"+Math.random();class de extends fe{constructor(e,n,r){switch(super(),this.syncErrorValue=null,this.syncErrorThrown=!1,this.syncErrorThrowable=!1,this.isStopped=!1,arguments.length){case 0:this.destination=Zo;break;case 1:if(!e){this.destination=Zo;break}if("object"==typeof e){e instanceof de?(this.syncErrorThrowable=e.syncErrorThrowable,this.destination=e,e.add(this)):(this.syncErrorThrowable=!0,this.destination=new bp(this,e));break}default:this.syncErrorThrowable=!0,this.destination=new bp(this,e,n,r)}}[Jo](){return this}static create(e,n,r){const i=new de(e,n,r);return i.syncErrorThrowable=!1,i}next(e){this.isStopped||this._next(e)}error(e){this.isStopped||(this.isStopped=!0,this._error(e))}complete(){this.isStopped||(this.isStopped=!0,this._complete())}unsubscribe(){this.closed||(this.isStopped=!0,super.unsubscribe())}_next(e){this.destination.next(e)}_error(e){this.destination.error(e),this.unsubscribe()}_complete(){this.destination.complete(),this.unsubscribe()}_unsubscribeAndRecycle(){const{_parentOrParents:e}=this;return this._parentOrParents=null,this.unsubscribe(),this.closed=!1,this.isStopped=!1,this._parentOrParents=e,this}}class bp extends de{constructor(e,n,r,i){super(),this._parentSubscriber=e;let o,s=this;an(n)?o=n:n&&(o=n.next,r=n.error,i=n.complete,n!==Zo&&(s=Object.create(n),an(s.unsubscribe)&&this.add(s.unsubscribe.bind(s)),s.unsubscribe=this.unsubscribe.bind(this))),this._context=s,this._next=o,this._error=r,this._complete=i}next(e){if(!this.isStopped&&this._next){const{_parentSubscriber:n}=this;xt.useDeprecatedSynchronousErrorHandling&&n.syncErrorThrowable?this.__tryOrSetError(n,this._next,e)&&this.unsubscribe():this.__tryOrUnsub(this._next,e)}}error(e){if(!this.isStopped){const{_parentSubscriber:n}=this,{useDeprecatedSynchronousErrorHandling:r}=xt;if(this._error)r&&n.syncErrorThrowable?(this.__tryOrSetError(n,this._error,e),this.unsubscribe()):(this.__tryOrUnsub(this._error,e),this.unsubscribe());else if(n.syncErrorThrowable)r?(n.syncErrorValue=e,n.syncErrorThrown=!0):wr(e),this.unsubscribe();else{if(this.unsubscribe(),r)throw e;wr(e)}}}complete(){if(!this.isStopped){const{_parentSubscriber:e}=this;if(this._complete){const n=()=>this._complete.call(this._context);xt.useDeprecatedSynchronousErrorHandling&&e.syncErrorThrowable?(this.__tryOrSetError(e,n),this.unsubscribe()):(this.__tryOrUnsub(n),this.unsubscribe())}else this.unsubscribe()}}__tryOrUnsub(e,n){try{e.call(this._context,n)}catch(r){if(this.unsubscribe(),xt.useDeprecatedSynchronousErrorHandling)throw r;wr(r)}}__tryOrSetError(e,n,r){if(!xt.useDeprecatedSynchronousErrorHandling)throw new Error("bad call");try{n.call(this._context,r)}catch(i){return xt.useDeprecatedSynchronousErrorHandling?(e.syncErrorValue=i,e.syncErrorThrown=!0,!0):(wr(i),!0)}return!1}_unsubscribe(){const{_parentSubscriber:e}=this;this._context=null,this._parentSubscriber=null,e.unsubscribe()}}const Ri="function"==typeof Symbol&&Symbol.observable||"@@observable";function es(t){return t}let ue=(()=>{class t{constructor(n){this._isScalar=!1,n&&(this._subscribe=n)}lift(n){const r=new t;return r.source=this,r.operator=n,r}subscribe(n,r,i){const{operator:o}=this,s=function(t,e,n){if(t){if(t instanceof de)return t;if(t[Jo])return t[Jo]()}return t||e||n?new de(t,e,n):new de(Zo)}(n,r,i);if(s.add(o?o.call(s,this.source):this.source||xt.useDeprecatedSynchronousErrorHandling&&!s.syncErrorThrowable?this._subscribe(s):this._trySubscribe(s)),xt.useDeprecatedSynchronousErrorHandling&&s.syncErrorThrowable&&(s.syncErrorThrowable=!1,s.syncErrorThrown))throw s.syncErrorValue;return s}_trySubscribe(n){try{return this._subscribe(n)}catch(r){xt.useDeprecatedSynchronousErrorHandling&&(n.syncErrorThrown=!0,n.syncErrorValue=r),function(t){for(;t;){const{closed:e,destination:n,isStopped:r}=t;if(e||r)return!1;t=n&&n instanceof de?n:null}return!0}(n)?n.error(r):console.warn(r)}}forEach(n,r){return new(r=vp(r))((i,o)=>{let s;s=this.subscribe(a=>{try{n(a)}catch(l){o(l),s&&s.unsubscribe()}},o,i)})}_subscribe(n){const{source:r}=this;return r&&r.subscribe(n)}[Ri](){return this}pipe(...n){return 0===n.length?this:function(t){return 0===t.length?es:1===t.length?t[0]:function(n){return t.reduce((r,i)=>i(r),n)}}(n)(this)}toPromise(n){return new(n=vp(n))((r,i)=>{let o;this.subscribe(s=>o=s,s=>i(s),()=>r(o))})}}return t.create=e=>new t(e),t})();function vp(t){if(t||(t=xt.Promise||Promise),!t)throw new Error("no Promise impl found");return t}const _r=(()=>{function t(){return Error.call(this),this.message="object unsubscribed",this.name="ObjectUnsubscribedError",this}return t.prototype=Object.create(Error.prototype),t})();class Q0 extends fe{constructor(e,n){super(),this.subject=e,this.subscriber=n,this.closed=!1}unsubscribe(){if(this.closed)return;this.closed=!0;const e=this.subject,n=e.observers;if(this.subject=null,!n||0===n.length||e.isStopped||e.closed)return;const r=n.indexOf(this.subscriber);-1!==r&&n.splice(r,1)}}class wp extends de{constructor(e){super(e),this.destination=e}}let ln=(()=>{class t extends ue{constructor(){super(),this.observers=[],this.closed=!1,this.isStopped=!1,this.hasError=!1,this.thrownError=null}[Jo](){return new wp(this)}lift(n){const r=new _p(this,this);return r.operator=n,r}next(n){if(this.closed)throw new _r;if(!this.isStopped){const{observers:r}=this,i=r.length,o=r.slice();for(let s=0;s<i;s++)o[s].next(n)}}error(n){if(this.closed)throw new _r;this.hasError=!0,this.thrownError=n,this.isStopped=!0;const{observers:r}=this,i=r.length,o=r.slice();for(let s=0;s<i;s++)o[s].error(n);this.observers.length=0}complete(){if(this.closed)throw new _r;this.isStopped=!0;const{observers:n}=this,r=n.length,i=n.slice();for(let o=0;o<r;o++)i[o].complete();this.observers.length=0}unsubscribe(){this.isStopped=!0,this.closed=!0,this.observers=null}_trySubscribe(n){if(this.closed)throw new _r;return super._trySubscribe(n)}_subscribe(n){if(this.closed)throw new _r;return this.hasError?(n.error(this.thrownError),fe.EMPTY):this.isStopped?(n.complete(),fe.EMPTY):(this.observers.push(n),new Q0(this,n))}asObservable(){const n=new ue;return n.source=this,n}}return t.create=(e,n)=>new _p(e,n),t})();class _p extends ln{constructor(e,n){super(),this.destination=e,this.source=n}next(e){const{destination:n}=this;n&&n.next&&n.next(e)}error(e){const{destination:n}=this;n&&n.error&&this.destination.error(e)}complete(){const{destination:e}=this;e&&e.complete&&this.destination.complete()}_subscribe(e){const{source:n}=this;return n?this.source.subscribe(e):fe.EMPTY}}function ts(t){return t&&"function"==typeof t.schedule}function oe(t,e){return function(r){if("function"!=typeof t)throw new TypeError("argument is not a function. Are you looking for `mapTo()`?");return r.lift(new Z0(t,e))}}class Z0{constructor(e,n){this.project=e,this.thisArg=n}call(e,n){return n.subscribe(new X0(e,this.project,this.thisArg))}}class X0 extends de{constructor(e,n,r){super(e),this.project=n,this.count=0,this.thisArg=r||this}_next(e){let n;try{n=this.project.call(this.thisArg,e,this.count++)}catch(r){return void this.destination.error(r)}this.destination.next(n)}}const Cp=t=>e=>{for(let n=0,r=t.length;n<r&&!e.closed;n++)e.next(t[n]);e.complete()},ns="function"==typeof Symbol&&Symbol.iterator?Symbol.iterator:"@@iterator",Dp=t=>t&&"number"==typeof t.length&&"function"!=typeof t;function Ep(t){return!!t&&"function"!=typeof t.subscribe&&"function"==typeof t.then}const sl=t=>{if(t&&"function"==typeof t[Ri])return(t=>e=>{const n=t[Ri]();if("function"!=typeof n.subscribe)throw new TypeError("Provided object does not correctly implement Symbol.observable");return n.subscribe(e)})(t);if(Dp(t))return Cp(t);if(Ep(t))return(t=>e=>(t.then(n=>{e.closed||(e.next(n),e.complete())},n=>e.error(n)).then(null,wr),e))(t);if(t&&"function"==typeof t[ns])return(t=>e=>{const n=t[ns]();for(;;){let r;try{r=n.next()}catch(i){return e.error(i),e}if(r.done){e.complete();break}if(e.next(r.value),e.closed)break}return"function"==typeof n.return&&e.add(()=>{n.return&&n.return()}),e})(t);{const n=`You provided ${gp(t)?"an invalid object":`'${t}'`} where a stream was expected. You can provide an Observable, Promise, Array, or Iterable.`;throw new TypeError(n)}};function al(t,e){return new ue(n=>{const r=new fe;let i=0;return r.add(e.schedule(function(){i!==t.length?(n.next(t[i++]),n.closed||r.add(this.schedule())):n.complete()})),r})}function qe(t,e){return e?function(t,e){if(null!=t){if(function(t){return t&&"function"==typeof t[Ri]}(t))return function(t,e){return new ue(n=>{const r=new fe;return r.add(e.schedule(()=>{const i=t[Ri]();r.add(i.subscribe({next(o){r.add(e.schedule(()=>n.next(o)))},error(o){r.add(e.schedule(()=>n.error(o)))},complete(){r.add(e.schedule(()=>n.complete()))}}))})),r})}(t,e);if(Ep(t))return function(t,e){return new ue(n=>{const r=new fe;return r.add(e.schedule(()=>t.then(i=>{r.add(e.schedule(()=>{n.next(i),r.add(e.schedule(()=>n.complete()))}))},i=>{r.add(e.schedule(()=>n.error(i)))}))),r})}(t,e);if(Dp(t))return al(t,e);if(function(t){return t&&"function"==typeof t[ns]}(t)||"string"==typeof t)return function(t,e){if(!t)throw new Error("Iterable cannot be null");return new ue(n=>{const r=new fe;let i;return r.add(()=>{i&&"function"==typeof i.return&&i.return()}),r.add(e.schedule(()=>{i=t[ns](),r.add(e.schedule(function(){if(n.closed)return;let o,s;try{const a=i.next();o=a.value,s=a.done}catch(a){return void n.error(a)}s?n.complete():(n.next(o),this.schedule())}))})),r})}(t,e)}throw new TypeError((null!==t&&typeof t||t)+" is not observable")}(t,e):t instanceof ue?t:new ue(sl(t))}class ll extends de{constructor(e){super(),this.parent=e}_next(e){this.parent.notifyNext(e)}_error(e){this.parent.notifyError(e),this.unsubscribe()}_complete(){this.parent.notifyComplete(),this.unsubscribe()}}class cl extends de{notifyNext(e){this.destination.next(e)}notifyError(e){this.destination.error(e)}notifyComplete(){this.destination.complete()}}function dl(t,e){if(e.closed)return;if(t instanceof ue)return t.subscribe(e);let n;try{n=sl(t)(e)}catch(r){e.error(r)}return n}function Ne(t,e,n=Number.POSITIVE_INFINITY){return"function"==typeof e?r=>r.pipe(Ne((i,o)=>qe(t(i,o)).pipe(oe((s,a)=>e(i,s,o,a))),n)):("number"==typeof e&&(n=e),r=>r.lift(new cw(t,n)))}class cw{constructor(e,n=Number.POSITIVE_INFINITY){this.project=e,this.concurrent=n}call(e,n){return n.subscribe(new dw(e,this.project,this.concurrent))}}class dw extends cl{constructor(e,n,r=Number.POSITIVE_INFINITY){super(e),this.project=n,this.concurrent=r,this.hasCompleted=!1,this.buffer=[],this.active=0,this.index=0}_next(e){this.active<this.concurrent?this._tryNext(e):this.buffer.push(e)}_tryNext(e){let n;const r=this.index++;try{n=this.project(e,r)}catch(i){return void this.destination.error(i)}this.active++,this._innerSub(n)}_innerSub(e){const n=new ll(this),r=this.destination;r.add(n);const i=dl(e,n);i!==n&&r.add(i)}_complete(){this.hasCompleted=!0,0===this.active&&0===this.buffer.length&&this.destination.complete(),this.unsubscribe()}notifyNext(e){this.destination.next(e)}notifyComplete(){const e=this.buffer;this.active--,e.length>0?this._next(e.shift()):0===this.active&&this.hasCompleted&&this.destination.complete()}}function Ni(t=Number.POSITIVE_INFINITY){return Ne(es,t)}function ul(t,e){return e?al(t,e):new ue(Cp(t))}function pl(){return function(e){return e.lift(new pw(e))}}class pw{constructor(e){this.connectable=e}call(e,n){const{connectable:r}=this;r._refCount++;const i=new hw(e,r),o=n.subscribe(i);return i.closed||(i.connection=r.connect()),o}}class hw extends de{constructor(e,n){super(e),this.connectable=n}_unsubscribe(){const{connectable:e}=this;if(!e)return void(this.connection=null);this.connectable=null;const n=e._refCount;if(n<=0)return void(this.connection=null);if(e._refCount=n-1,n>1)return void(this.connection=null);const{connection:r}=this,i=e._connection;this.connection=null,i&&(!r||i===r)&&i.unsubscribe()}}class Tp extends ue{constructor(e,n){super(),this.source=e,this.subjectFactory=n,this._refCount=0,this._isComplete=!1}_subscribe(e){return this.getSubject().subscribe(e)}getSubject(){const e=this._subject;return(!e||e.isStopped)&&(this._subject=this.subjectFactory()),this._subject}connect(){let e=this._connection;return e||(this._isComplete=!1,e=this._connection=new fe,e.add(this.source.subscribe(new gw(this.getSubject(),this))),e.closed&&(this._connection=null,e=fe.EMPTY)),e}refCount(){return pl()(this)}}const fw=(()=>{const t=Tp.prototype;return{operator:{value:null},_refCount:{value:0,writable:!0},_subject:{value:null,writable:!0},_connection:{value:null,writable:!0},_subscribe:{value:t._subscribe},_isComplete:{value:t._isComplete,writable:!0},getSubject:{value:t.getSubject},connect:{value:t.connect},refCount:{value:t.refCount}}})();class gw extends wp{constructor(e,n){super(e),this.connectable=n}_error(e){this._unsubscribe(),super._error(e)}_complete(){this.connectable._isComplete=!0,this._unsubscribe(),super._complete()}_unsubscribe(){const e=this.connectable;if(e){this.connectable=null;const n=e._connection;e._refCount=0,e._subject=null,e._connection=null,n&&n.unsubscribe()}}}function vw(){return new ln}function Z(t){for(let e in t)if(t[e]===Z)return e;throw Error("Could not find renamed property on target object.")}function z(t){if("string"==typeof t)return t;if(Array.isArray(t))return"["+t.map(z).join(", ")+"]";if(null==t)return""+t;if(t.overriddenName)return`${t.overriddenName}`;if(t.name)return`${t.name}`;const e=t.toString();if(null==e)return""+e;const n=e.indexOf("\n");return-1===n?e:e.substring(0,n)}function fl(t,e){return null==t||""===t?null===e?"":e:null==e||""===e?t:t+" "+e}const _w=Z({__forward_ref__:Z});function gl(t){return t.__forward_ref__=gl,t.toString=function(){return z(this())},t}function M(t){return function(t){return"function"==typeof t&&t.hasOwnProperty(_w)&&t.__forward_ref__===gl}(t)?t():t}class $n extends Error{constructor(e,n){super(function(t,e){return`${t?`NG0${t}: `:""}${e}`}(e,n)),this.code=e}}function He(t){return"function"==typeof t?t.name||t.toString():"object"==typeof t&&null!=t&&"function"==typeof t.type?t.type.name||t.type.toString():function(t){return"string"==typeof t?t:null==t?"":String(t)}(t)}function rs(t,e){const n=e?` in ${e}`:"";throw new $n("201",`No provider for ${He(t)} found${n}`)}function rt(t,e){null==t&&function(t,e,n,r){throw new Error(`ASSERTION ERROR: ${t}`+(null==r?"":` [Expected=> ${n} ${r} ${e} <=Actual]`))}(e,t,null,"!=")}function q(t){return{token:t.token,providedIn:t.providedIn||null,factory:t.factory,value:void 0}}function dn(t){return{providers:t.providers||[],imports:t.imports||[]}}function un(t){return xp(t,is)||xp(t,Ip)}function xp(t,e){return t.hasOwnProperty(e)?t[e]:null}function Mp(t){return t&&(t.hasOwnProperty(bl)||t.hasOwnProperty(Mw))?t[bl]:null}const is=Z({\u0275prov:Z}),bl=Z({\u0275inj:Z}),Ip=Z({ngInjectableDef:Z}),Mw=Z({ngInjectorDef:Z});var I=(()=>((I=I||{})[I.Default=0]="Default",I[I.Host=1]="Host",I[I.Self=2]="Self",I[I.SkipSelf=4]="SkipSelf",I[I.Optional=8]="Optional",I))();let yl;function Sn(t){const e=yl;return yl=t,e}function Ap(t,e,n){const r=un(t);return r&&"root"==r.providedIn?void 0===r.value?r.value=r.factory():r.value:n&I.Optional?null:void 0!==e?e:void rs(z(t),"Injector")}function xn(t){return{toString:t}.toString()}var ht=(()=>((ht=ht||{})[ht.OnPush=0]="OnPush",ht[ht.Default=1]="Default",ht))(),Ee=(()=>((Ee=Ee||{})[Ee.Emulated=0]="Emulated",Ee[Ee.None=2]="None",Ee[Ee.ShadowDom=3]="ShadowDom",Ee))();const Aw="undefined"!=typeof globalThis&&globalThis,Pw="undefined"!=typeof window&&window,Rw="undefined"!=typeof self&&"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope&&self,Nw="undefined"!=typeof global&&global,X=Aw||Nw||Pw||Rw,Cr={},ee=[],os=Z({\u0275cmp:Z}),vl=Z({\u0275dir:Z}),wl=Z({\u0275pipe:Z}),Pp=Z({\u0275mod:Z}),kw=Z({\u0275loc:Z}),pn=Z({\u0275fac:Z}),ki=Z({__NG_ELEMENT_ID__:Z});let Ow=0;function Wn(t){return xn(()=>{const n={},r={type:t.type,providersResolver:null,decls:t.decls,vars:t.vars,factory:null,template:t.template||null,consts:t.consts||null,ngContentSelectors:t.ngContentSelectors,hostBindings:t.hostBindings||null,hostVars:t.hostVars||0,hostAttrs:t.hostAttrs||null,contentQueries:t.contentQueries||null,declaredInputs:n,inputs:null,outputs:null,exportAs:t.exportAs||null,onPush:t.changeDetection===ht.OnPush,directiveDefs:null,pipeDefs:null,selectors:t.selectors||ee,viewQuery:t.viewQuery||null,features:t.features||null,data:t.data||{},encapsulation:t.encapsulation||Ee.Emulated,id:"c",styles:t.styles||ee,_:null,setInput:null,schemas:t.schemas||null,tView:null},i=t.directives,o=t.features,s=t.pipes;return r.id+=Ow++,r.inputs=Op(t.inputs,n),r.outputs=Op(t.outputs),o&&o.forEach(a=>a(r)),r.directiveDefs=i?()=>("function"==typeof i?i():i).map(Rp):null,r.pipeDefs=s?()=>("function"==typeof s?s():s).map(Np):null,r})}function Rp(t){return $e(t)||function(t){return t[vl]||null}(t)}function Np(t){return function(t){return t[wl]||null}(t)}const kp={};function Gn(t){return xn(()=>{const e={type:t.type,bootstrap:t.bootstrap||ee,declarations:t.declarations||ee,imports:t.imports||ee,exports:t.exports||ee,transitiveCompileScopes:null,schemas:t.schemas||null,id:t.id||null};return null!=t.id&&(kp[t.id]=t.type),e})}function Op(t,e){if(null==t)return Cr;const n={};for(const r in t)if(t.hasOwnProperty(r)){let i=t[r],o=i;Array.isArray(i)&&(o=i[1],i=i[0]),n[i]=r,e&&(e[i]=o)}return n}const Fe=Wn;function $e(t){return t[os]||null}function ft(t,e){const n=t[Pp]||null;if(!n&&!0===e)throw new Error(`Type ${z(t)} does not have '\u0275mod' property.`);return n}const V=11;function Wt(t){return Array.isArray(t)&&"object"==typeof t[1]}function It(t){return Array.isArray(t)&&!0===t[1]}function Dl(t){return 0!=(8&t.flags)}function cs(t){return 2==(2&t.flags)}function ds(t){return 1==(1&t.flags)}function At(t){return null!==t.template}function zw(t){return 0!=(512&t[2])}function Xn(t,e){return t.hasOwnProperty(pn)?t[pn]:null}class jp{constructor(e,n,r){this.previousValue=e,this.currentValue=n,this.firstChange=r}isFirstChange(){return this.firstChange}}function Lp(t){return t.type.prototype.ngOnChanges&&(t.setInput=Ww),$w}function $w(){const t=Bp(this),e=null==t?void 0:t.current;if(e){const n=t.previous;if(n===Cr)t.previous=e;else for(let r in e)n[r]=e[r];t.current=null,this.ngOnChanges(e)}}function Ww(t,e,n,r){const i=Bp(t)||function(t,e){return t[Vp]=e}(t,{previous:Cr,current:null}),o=i.current||(i.current={}),s=i.previous,a=this.declaredInputs[n],l=s[a];o[a]=new jp(l&&l.currentValue,e,s===Cr),t[r]=e}const Vp="__ngSimpleChanges__";function Bp(t){return t[Vp]||null}let Sl;function ge(t){return!!t.listen}const qp={createRenderer:(t,e)=>void 0!==Sl?Sl:"undefined"!=typeof document?document:void 0};function we(t){for(;Array.isArray(t);)t=t[0];return t}function bt(t,e){return we(e[t.index])}function ot(t,e){const n=e[t];return Wt(n)?n:n[0]}function Il(t){return 128==(128&t[2])}function In(t,e){return null==e?null:t[e]}function $p(t){t[18]=0}function Al(t,e){t[5]+=e;let n=t,r=t[3];for(;null!==r&&(1===e&&1===n[5]||-1===e&&0===n[5]);)r[5]+=e,n=r,r=r[3]}const N={lFrame:Jp(null),bindingsEnabled:!0,isInCheckNoChangesMode:!1};function Wp(){return N.bindingsEnabled}function w(){return N.lFrame.lView}function K(){return N.lFrame.tView}function Te(){let t=Gp();for(;null!==t&&64===t.type;)t=t.parent;return t}function Gp(){return N.lFrame.currentTNode}function Gt(t,e){const n=N.lFrame;n.currentTNode=t,n.isParent=e}function Pl(){return N.lFrame.isParent}function ps(){return N.isInCheckNoChangesMode}function hs(t){N.isInCheckNoChangesMode=t}function l_(t,e){const n=N.lFrame;n.bindingIndex=n.bindingRootIndex=t,Nl(e)}function Nl(t){N.lFrame.currentDirectiveIndex=t}function Ol(t){N.lFrame.currentQueryIndex=t}function d_(t){const e=t[1];return 2===e.type?e.declTNode:1===e.type?t[6]:null}function Zp(t,e,n){if(n&I.SkipSelf){let i=e,o=t;for(;!(i=i.parent,null!==i||n&I.Host||(i=d_(o),null===i||(o=o[15],10&i.type))););if(null===i)return!1;e=i,t=o}const r=N.lFrame=Xp();return r.currentTNode=e,r.lView=t,!0}function fs(t){const e=Xp(),n=t[1];N.lFrame=e,e.currentTNode=n.firstChild,e.lView=t,e.tView=n,e.contextLView=t,e.bindingIndex=n.bindingStartIndex,e.inI18n=!1}function Xp(){const t=N.lFrame,e=null===t?null:t.child;return null===e?Jp(t):e}function Jp(t){const e={currentTNode:null,isParent:!0,lView:null,tView:null,selectedIndex:-1,contextLView:null,elementDepthCount:0,currentNamespace:null,currentDirectiveIndex:-1,bindingRootIndex:-1,bindingIndex:-1,currentQueryIndex:0,parent:t,child:null,inI18n:!1};return null!==t&&(t.child=e),e}function eh(){const t=N.lFrame;return N.lFrame=t.parent,t.currentTNode=null,t.lView=null,t}const th=eh;function gs(){const t=eh();t.isParent=!0,t.tView=null,t.selectedIndex=-1,t.contextLView=null,t.elementDepthCount=0,t.currentDirectiveIndex=-1,t.currentNamespace=null,t.bindingRootIndex=-1,t.bindingIndex=-1,t.currentQueryIndex=0}function An(t){N.lFrame.selectedIndex=t}function ms(t,e){for(let n=e.directiveStart,r=e.directiveEnd;n<r;n++){const o=t.data[n].type.prototype,{ngAfterContentInit:s,ngAfterContentChecked:a,ngAfterViewInit:l,ngAfterViewChecked:c,ngOnDestroy:d}=o;s&&(t.contentHooks||(t.contentHooks=[])).push(-n,s),a&&((t.contentHooks||(t.contentHooks=[])).push(n,a),(t.contentCheckHooks||(t.contentCheckHooks=[])).push(n,a)),l&&(t.viewHooks||(t.viewHooks=[])).push(-n,l),c&&((t.viewHooks||(t.viewHooks=[])).push(n,c),(t.viewCheckHooks||(t.viewCheckHooks=[])).push(n,c)),null!=d&&(t.destroyHooks||(t.destroyHooks=[])).push(n,d)}}function bs(t,e,n){nh(t,e,3,n)}function ys(t,e,n,r){(3&t[2])===n&&nh(t,e,n,r)}function Fl(t,e){let n=t[2];(3&n)===e&&(n&=2047,n+=1,t[2]=n)}function nh(t,e,n,r){const o=null!=r?r:-1,s=e.length-1;let a=0;for(let l=void 0!==r?65535&t[18]:0;l<s;l++)if("number"==typeof e[l+1]){if(a=e[l],null!=r&&a>=r)break}else e[l]<0&&(t[18]+=65536),(a<o||-1==o)&&(v_(t,n,e,l),t[18]=(4294901760&t[18])+l+2),l++}function v_(t,e,n,r){const i=n[r]<0,o=n[r+1],a=t[i?-n[r]:n[r]];if(i){if(t[2]>>11<t[18]>>16&&(3&t[2])===e){t[2]+=2048;try{o.call(a)}finally{}}}else try{o.call(a)}finally{}}class Vi{constructor(e,n,r){this.factory=e,this.resolving=!1,this.canSeeViewProviders=n,this.injectImpl=r}}function vs(t,e,n){const r=ge(t);let i=0;for(;i<n.length;){const o=n[i];if("number"==typeof o){if(0!==o)break;i++;const s=n[i++],a=n[i++],l=n[i++];r?t.setAttribute(e,a,l,s):e.setAttributeNS(s,a,l)}else{const s=o,a=n[++i];Ll(s)?r&&t.setProperty(e,s,a):r?t.setAttribute(e,s,a):e.setAttribute(s,a),i++}}return i}function rh(t){return 3===t||4===t||6===t}function Ll(t){return 64===t.charCodeAt(0)}function ws(t,e){if(null!==e&&0!==e.length)if(null===t||0===t.length)t=e.slice();else{let n=-1;for(let r=0;r<e.length;r++){const i=e[r];"number"==typeof i?n=i:0===n||ih(t,n,i,null,-1===n||2===n?e[++r]:null)}}return t}function ih(t,e,n,r,i){let o=0,s=t.length;if(-1===e)s=-1;else for(;o<t.length;){const a=t[o++];if("number"==typeof a){if(a===e){s=-1;break}if(a>e){s=o-1;break}}}for(;o<t.length;){const a=t[o];if("number"==typeof a)break;if(a===n){if(null===r)return void(null!==i&&(t[o+1]=i));if(r===t[o+1])return void(t[o+2]=i)}o++,null!==r&&o++,null!==i&&o++}-1!==s&&(t.splice(s,0,e),o=s+1),t.splice(o++,0,n),null!==r&&t.splice(o++,0,r),null!==i&&t.splice(o++,0,i)}function oh(t){return-1!==t}function Ir(t){return 32767&t}function Ar(t,e){let n=function(t){return t>>16}(t),r=e;for(;n>0;)r=r[15],n--;return r}let Vl=!0;function _s(t){const e=Vl;return Vl=t,e}let T_=0;function Ui(t,e){const n=Ul(t,e);if(-1!==n)return n;const r=e[1];r.firstCreatePass&&(t.injectorIndex=e.length,Bl(r.data,t),Bl(e,null),Bl(r.blueprint,null));const i=Cs(t,e),o=t.injectorIndex;if(oh(i)){const s=Ir(i),a=Ar(i,e),l=a[1].data;for(let c=0;c<8;c++)e[o+c]=a[s+c]|l[s+c]}return e[o+8]=i,o}function Bl(t,e){t.push(0,0,0,0,0,0,0,0,e)}function Ul(t,e){return-1===t.injectorIndex||t.parent&&t.parent.injectorIndex===t.injectorIndex||null===e[t.injectorIndex+8]?-1:t.injectorIndex}function Cs(t,e){if(t.parent&&-1!==t.parent.injectorIndex)return t.parent.injectorIndex;let n=0,r=null,i=e;for(;null!==i;){const o=i[1],s=o.type;if(r=2===s?o.declTNode:1===s?i[6]:null,null===r)return-1;if(n++,i=i[15],-1!==r.injectorIndex)return r.injectorIndex|n<<16}return-1}function Ds(t,e,n){!function(t,e,n){let r;"string"==typeof n?r=n.charCodeAt(0)||0:n.hasOwnProperty(ki)&&(r=n[ki]),null==r&&(r=n[ki]=T_++);const i=255&r;e.data[t+(i>>5)]|=1<<i}(t,e,n)}function lh(t,e,n){if(n&I.Optional)return t;rs(e,"NodeInjector")}function ch(t,e,n,r){if(n&I.Optional&&void 0===r&&(r=null),0==(n&(I.Self|I.Host))){const i=t[9],o=Sn(void 0);try{return i?i.get(e,r,n&I.Optional):Ap(e,r,n&I.Optional)}finally{Sn(o)}}return lh(r,e,n)}function dh(t,e,n,r=I.Default,i){if(null!==t){const o=function(t){if("string"==typeof t)return t.charCodeAt(0)||0;const e=t.hasOwnProperty(ki)?t[ki]:void 0;return"number"==typeof e?e>=0?255&e:M_:e}(n);if("function"==typeof o){if(!Zp(e,t,r))return r&I.Host?lh(i,n,r):ch(e,n,r,i);try{const s=o(r);if(null!=s||r&I.Optional)return s;rs(n)}finally{th()}}else if("number"==typeof o){let s=null,a=Ul(t,e),l=-1,c=r&I.Host?e[16][6]:null;for((-1===a||r&I.SkipSelf)&&(l=-1===a?Cs(t,e):e[a+8],-1!==l&&hh(r,!1)?(s=e[1],a=Ir(l),e=Ar(l,e)):a=-1);-1!==a;){const d=e[1];if(ph(o,a,d.data)){const u=I_(a,e,n,s,r,c);if(u!==uh)return u}l=e[a+8],-1!==l&&hh(r,e[1].data[a+8]===c)&&ph(o,a,e)?(s=d,a=Ir(l),e=Ar(l,e)):a=-1}}}return ch(e,n,r,i)}const uh={};function M_(){return new Pr(Te(),w())}function I_(t,e,n,r,i,o){const s=e[1],a=s.data[t+8],d=function(t,e,n,r,i){const o=t.providerIndexes,s=e.data,a=1048575&o,l=t.directiveStart,d=o>>20,p=i?a+d:t.directiveEnd;for(let h=r?a:a+d;h<p;h++){const f=s[h];if(h<l&&n===f||h>=l&&f.type===n)return h}if(i){const h=s[l];if(h&&At(h)&&h.type===n)return l}return null}(a,s,n,null==r?cs(a)&&Vl:r!=s&&0!=(3&a.type),i&I.Host&&o===a);return null!==d?zi(e,s,d,a):uh}function zi(t,e,n,r){let i=t[n];const o=e.data;if(function(t){return t instanceof Vi}(i)){const s=i;s.resolving&&function(t,e){throw new $n("200",`Circular dependency in DI detected for ${t}`)}(He(o[n]));const a=_s(s.canSeeViewProviders);s.resolving=!0;const l=s.injectImpl?Sn(s.injectImpl):null;Zp(t,r,I.Default);try{i=t[n]=s.factory(void 0,o,t,r),e.firstCreatePass&&n>=r.directiveStart&&function(t,e,n){const{ngOnChanges:r,ngOnInit:i,ngDoCheck:o}=e.type.prototype;if(r){const s=Lp(e);(n.preOrderHooks||(n.preOrderHooks=[])).push(t,s),(n.preOrderCheckHooks||(n.preOrderCheckHooks=[])).push(t,s)}i&&(n.preOrderHooks||(n.preOrderHooks=[])).push(0-t,i),o&&((n.preOrderHooks||(n.preOrderHooks=[])).push(t,o),(n.preOrderCheckHooks||(n.preOrderCheckHooks=[])).push(t,o))}(n,o[n],e)}finally{null!==l&&Sn(l),_s(a),s.resolving=!1,th()}}return i}function ph(t,e,n){return!!(n[e+(t>>5)]&1<<t)}function hh(t,e){return!(t&I.Self||t&I.Host&&e)}class Pr{constructor(e,n){this._tNode=e,this._lView=n}get(e,n){return dh(this._tNode,this._lView,e,void 0,n)}}const Nr="__parameters__";function er(t,e,n){return xn(()=>{const r=function(t){return function(...n){if(t){const r=t(...n);for(const i in r)this[i]=r[i]}}}(e);function i(...o){if(this instanceof i)return r.apply(this,o),this;const s=new i(...o);return a.annotation=s,a;function a(l,c,d){const u=l.hasOwnProperty(Nr)?l[Nr]:Object.defineProperty(l,Nr,{value:[]})[Nr];for(;u.length<=d;)u.push(null);return(u[d]=u[d]||[]).push(s),l}}return n&&(i.prototype=Object.create(n.prototype)),i.prototype.ngMetadataName=t,i.annotationCls=i,i})}class Y{constructor(e,n){this._desc=e,this.ngMetadataName="InjectionToken",this.\u0275prov=void 0,"number"==typeof n?this.__NG_ELEMENT_ID__=n:void 0!==n&&(this.\u0275prov=q({token:this,providedIn:n.providedIn||"root",factory:n.factory}))}toString(){return`InjectionToken ${this._desc}`}}const k_=new Y("AnalyzeForEntryComponents"),Ts=Function;function Yt(t,e){t.forEach(n=>Array.isArray(n)?Yt(n,e):e(n))}function xs(t,e,n){e>=t.length?t.push(n):t.splice(e,0,n)}function tr(t,e){return e>=t.length-1?t.pop():t.splice(e,1)[0]}const Gi={},Wl="__NG_DI_FLAG__",Fr="ngTempTokenPath",H_=/\n/gm,Gl="__source",Kl=Z({provide:String,useValue:Z});let Ki;function jr(t){const e=Ki;return Ki=t,e}function W_(t,e=I.Default){if(void 0===Ki)throw new Error("inject() must be called from an injection context");return null===Ki?Ap(t,void 0,e):Ki.get(t,e&I.Optional?null:void 0,e)}function T(t,e=I.Default){return(yl||W_)(M(t),e)}function nr(t){const e=[];for(let n=0;n<t.length;n++){const r=M(t[n]);if(Array.isArray(r)){if(0===r.length)throw new Error("Arguments array must have arguments.");let i,o=I.Default;for(let s=0;s<r.length;s++){const a=r[s],l=G_(a);"number"==typeof l?-1===l?i=a.token:o|=l:i=a}e.push(T(i,o))}else e.push(T(r))}return e}function Yi(t,e){return t[Wl]=e,t.prototype[Wl]=e,t}function G_(t){return t[Wl]}function vh(t,e,n,r){const i=t[Fr];throw e[Gl]&&i.unshift(e[Gl]),t.message=function(t,e,n,r=null){t=t&&"\n"===t.charAt(0)&&"\u0275"==t.charAt(1)?t.substr(2):t;let i=z(e);if(Array.isArray(e))i=e.map(z).join(" -> ");else if("object"==typeof e){let o=[];for(let s in e)if(e.hasOwnProperty(s)){let a=e[s];o.push(s+":"+("string"==typeof a?JSON.stringify(a):z(a)))}i=`{${o.join(", ")}}`}return`${n}${r?"("+r+")":""}[${i}]: ${t.replace(H_,"\n  ")}`}("\n"+t.message,i,n,r),t.ngTokenPath=i,t[Fr]=null,t}const Lr=Yi(er("Inject",t=>({token:t})),-1),et=Yi(er("Optional"),8),Nn=Yi(er("SkipSelf"),4),Lh="__ngContext__";function Le(t,e){t[Lh]=e}function ic(t){const e=function(t){return t[Lh]||null}(t);return e?Array.isArray(e)?e:e.lView:null}function ks(t){return t.ngOriginalError}function BC(t,...e){t.error(...e)}class ir{constructor(){this._console=console}handleError(e){const n=this._findOriginalError(e),r=this._findContext(e),i=function(t){return t&&t.ngErrorLogger||BC}(e);i(this._console,"ERROR",e),n&&i(this._console,"ORIGINAL ERROR",n),r&&i(this._console,"ERROR CONTEXT",r)}_findContext(e){return e?function(t){return t.ngDebugContext}(e)||this._findContext(ks(e)):null}_findOriginalError(e){let n=e&&ks(e);for(;n&&ks(n);)n=ks(n);return n||null}}const Gh=(()=>("undefined"!=typeof requestAnimationFrame&&requestAnimationFrame||setTimeout).bind(X))();function Xt(t){return t instanceof Function?t():t}var lt=(()=>((lt=lt||{})[lt.Important=1]="Important",lt[lt.DashCase=2]="DashCase",lt))();function ac(t,e){return undefined(t,e)}function to(t){const e=t[3];return It(e)?e[3]:e}function lc(t){return Xh(t[13])}function cc(t){return Xh(t[4])}function Xh(t){for(;null!==t&&!It(t);)t=t[4];return t}function zr(t,e,n,r,i){if(null!=r){let o,s=!1;It(r)?o=r:Wt(r)&&(s=!0,r=r[0]);const a=we(r);0===t&&null!==n?null==i?of(e,n,a):or(e,n,a,i||null,!0):1===t&&null!==n?or(e,n,a,i||null,!0):2===t?function(t,e,n){const r=Fs(t,e);r&&function(t,e,n,r){ge(t)?t.removeChild(e,n,r):e.removeChild(n)}(t,r,e,n)}(e,a,s):3===t&&e.destroyNode(a),null!=o&&function(t,e,n,r,i){const o=n[7];o!==we(n)&&zr(e,t,r,o,i);for(let a=10;a<n.length;a++){const l=n[a];no(l[1],l,t,e,r,o)}}(e,t,o,n,i)}}function uc(t,e,n){return ge(t)?t.createElement(e,n):null===n?t.createElement(e):t.createElementNS(n,e)}function ef(t,e){const n=t[9],r=n.indexOf(e),i=e[3];1024&e[2]&&(e[2]&=-1025,Al(i,-1)),n.splice(r,1)}function pc(t,e){if(t.length<=10)return;const n=10+e,r=t[n];if(r){const i=r[17];null!==i&&i!==t&&ef(i,r),e>0&&(t[n-1][4]=r[4]);const o=tr(t,10+e);!function(t,e){no(t,e,e[V],2,null,null),e[0]=null,e[6]=null}(r[1],r);const s=o[19];null!==s&&s.detachView(o[1]),r[3]=null,r[4]=null,r[2]&=-129}return r}function tf(t,e){if(!(256&e[2])){const n=e[V];ge(n)&&n.destroyNode&&no(t,e,n,3,null,null),function(t){let e=t[13];if(!e)return hc(t[1],t);for(;e;){let n=null;if(Wt(e))n=e[13];else{const r=e[10];r&&(n=r)}if(!n){for(;e&&!e[4]&&e!==t;)Wt(e)&&hc(e[1],e),e=e[3];null===e&&(e=t),Wt(e)&&hc(e[1],e),n=e&&e[4]}e=n}}(e)}}function hc(t,e){if(!(256&e[2])){e[2]&=-129,e[2]|=256,function(t,e){let n;if(null!=t&&null!=(n=t.destroyHooks))for(let r=0;r<n.length;r+=2){const i=e[n[r]];if(!(i instanceof Vi)){const o=n[r+1];if(Array.isArray(o))for(let s=0;s<o.length;s+=2){const a=i[o[s]],l=o[s+1];try{l.call(a)}finally{}}else try{o.call(i)}finally{}}}}(t,e),function(t,e){const n=t.cleanup,r=e[7];let i=-1;if(null!==n)for(let o=0;o<n.length-1;o+=2)if("string"==typeof n[o]){const s=n[o+1],a="function"==typeof s?s(e):we(e[s]),l=r[i=n[o+2]],c=n[o+3];"boolean"==typeof c?a.removeEventListener(n[o],l,c):c>=0?r[i=c]():r[i=-c].unsubscribe(),o+=2}else{const s=r[i=n[o+1]];n[o].call(s)}if(null!==r){for(let o=i+1;o<r.length;o++)r[o]();e[7]=null}}(t,e),1===e[1].type&&ge(e[V])&&e[V].destroy();const n=e[17];if(null!==n&&It(e[3])){n!==e[3]&&ef(n,e);const r=e[19];null!==r&&r.detachView(t)}}}function nf(t,e,n){return function(t,e,n){let r=e;for(;null!==r&&40&r.type;)r=(e=r).parent;if(null===r)return n[0];if(2&r.flags){const i=t.data[r.directiveStart].encapsulation;if(i===Ee.None||i===Ee.Emulated)return null}return bt(r,n)}(t,e.parent,n)}function or(t,e,n,r,i){ge(t)?t.insertBefore(e,n,r,i):e.insertBefore(n,r,i)}function of(t,e,n){ge(t)?t.appendChild(e,n):e.appendChild(n)}function sf(t,e,n,r,i){null!==r?or(t,e,n,r,i):of(t,e,n)}function Fs(t,e){return ge(t)?t.parentNode(e):e.parentNode}let cf=function(t,e,n){return 40&t.type?bt(t,n):null};function js(t,e,n,r){const i=nf(t,r,e),o=e[V],a=function(t,e,n){return cf(t,e,n)}(r.parent||e[6],r,e);if(null!=i)if(Array.isArray(n))for(let l=0;l<n.length;l++)sf(o,i,n[l],a,!1);else sf(o,i,n,a,!1)}function Ls(t,e){if(null!==e){const n=e.type;if(3&n)return bt(e,t);if(4&n)return gc(-1,t[e.index]);if(8&n){const r=e.child;if(null!==r)return Ls(t,r);{const i=t[e.index];return It(i)?gc(-1,i):we(i)}}if(32&n)return ac(e,t)()||we(t[e.index]);{const r=uf(t,e);return null!==r?Array.isArray(r)?r[0]:Ls(to(t[16]),r):Ls(t,e.next)}}return null}function uf(t,e){return null!==e?t[16][6].projection[e.projection]:null}function gc(t,e){const n=10+t+1;if(n<e.length){const r=e[n],i=r[1].firstChild;if(null!==i)return Ls(r,i)}return e[7]}function mc(t,e,n,r,i,o,s){for(;null!=n;){const a=r[n.index],l=n.type;if(s&&0===e&&(a&&Le(we(a),r),n.flags|=4),64!=(64&n.flags))if(8&l)mc(t,e,n.child,r,i,o,!1),zr(e,t,i,a,o);else if(32&l){const c=ac(n,r);let d;for(;d=c();)zr(e,t,i,d,o);zr(e,t,i,a,o)}else 16&l?hf(t,e,r,n,i,o):zr(e,t,i,a,o);n=s?n.projectionNext:n.next}}function no(t,e,n,r,i,o){mc(n,r,t.firstChild,e,i,o,!1)}function hf(t,e,n,r,i,o){const s=n[16],l=s[6].projection[r.projection];if(Array.isArray(l))for(let c=0;c<l.length;c++)zr(e,t,i,l[c],o);else mc(t,e,l,s[3],i,o,!0)}function ff(t,e,n){ge(t)?t.setAttribute(e,"style",n):e.style.cssText=n}function bc(t,e,n){ge(t)?""===n?t.removeAttribute(e,"class"):t.setAttribute(e,"class",n):e.className=n}function gf(t,e,n){let r=t.length;for(;;){const i=t.indexOf(e,n);if(-1===i)return i;if(0===i||t.charCodeAt(i-1)<=32){const o=e.length;if(i+o===r||t.charCodeAt(i+o)<=32)return i}n=i+1}}const mf="ng-template";function pD(t,e,n){let r=0;for(;r<t.length;){let i=t[r++];if(n&&"class"===i){if(i=t[r],-1!==gf(i.toLowerCase(),e,0))return!0}else if(1===i){for(;r<t.length&&"string"==typeof(i=t[r++]);)if(i.toLowerCase()===e)return!0;return!1}}return!1}function bf(t){return 4===t.type&&t.value!==mf}function hD(t,e,n){return e===(4!==t.type||n?t.value:mf)}function fD(t,e,n){let r=4;const i=t.attrs||[],o=function(t){for(let e=0;e<t.length;e++)if(rh(t[e]))return e;return t.length}(i);let s=!1;for(let a=0;a<e.length;a++){const l=e[a];if("number"!=typeof l){if(!s)if(4&r){if(r=2|1&r,""!==l&&!hD(t,l,n)||""===l&&1===e.length){if(Pt(r))return!1;s=!0}}else{const c=8&r?l:e[++a];if(8&r&&null!==t.attrs){if(!pD(t.attrs,c,n)){if(Pt(r))return!1;s=!0}continue}const u=gD(8&r?"class":l,i,bf(t),n);if(-1===u){if(Pt(r))return!1;s=!0;continue}if(""!==c){let p;p=u>o?"":i[u+1].toLowerCase();const h=8&r?p:null;if(h&&-1!==gf(h,c,0)||2&r&&c!==p){if(Pt(r))return!1;s=!0}}}}else{if(!s&&!Pt(r)&&!Pt(l))return!1;if(s&&Pt(l))continue;s=!1,r=l|1&r}}return Pt(r)||s}function Pt(t){return 0==(1&t)}function gD(t,e,n,r){if(null===e)return-1;let i=0;if(r||!n){let o=!1;for(;i<e.length;){const s=e[i];if(s===t)return i;if(3===s||6===s)o=!0;else{if(1===s||2===s){let a=e[++i];for(;"string"==typeof a;)a=e[++i];continue}if(4===s)break;if(0===s){i+=4;continue}}i+=o?1:2}return-1}return function(t,e){let n=t.indexOf(4);if(n>-1)for(n++;n<t.length;){const r=t[n];if("number"==typeof r)return-1;if(r===e)return n;n++}return-1}(e,t)}function yf(t,e,n=!1){for(let r=0;r<e.length;r++)if(fD(t,e[r],n))return!0;return!1}function vf(t,e){return t?":not("+e.trim()+")":e}function wD(t){let e=t[0],n=1,r=2,i="",o=!1;for(;n<t.length;){let s=t[n];if("string"==typeof s)if(2&r){const a=t[++n];i+="["+s+(a.length>0?'="'+a+'"':"")+"]"}else 8&r?i+="."+s:4&r&&(i+=" "+s);else""!==i&&!Pt(s)&&(e+=vf(o,i),i=""),r=s,o=o||!Pt(r);n++}return""!==i&&(e+=vf(o,i)),e}const O={};function Af(t,e){const n=t.contentQueries;if(null!==n)for(let r=0;r<n.length;r+=2){const i=n[r],o=n[r+1];if(-1!==o){const s=t.data[o];Ol(i),s.contentQueries(2,e[o],o)}}}function ro(t,e,n,r,i,o,s,a,l,c){const d=e.blueprint.slice();return d[0]=i,d[2]=140|r,$p(d),d[3]=d[15]=t,d[8]=n,d[10]=s||t&&t[10],d[V]=a||t&&t[V],d[12]=l||t&&t[12]||null,d[9]=c||t&&t[9]||null,d[6]=o,d[16]=2==e.type?t[16]:d,d}function qr(t,e,n,r,i){let o=t.data[e];if(null===o)o=function(t,e,n,r,i){const o=Gp(),s=Pl(),l=t.data[e]=function(t,e,n,r,i,o){return{type:n,index:r,insertBeforeIndex:null,injectorIndex:e?e.injectorIndex:-1,directiveStart:-1,directiveEnd:-1,directiveStylingLast:-1,propertyBindings:null,flags:0,providerIndexes:0,value:i,attrs:o,mergedAttrs:null,localNames:null,initialInputs:void 0,inputs:null,outputs:null,tViews:null,next:null,projectionNext:null,child:null,parent:e,projection:null,styles:null,stylesWithoutHost:null,residualStyles:void 0,classes:null,classesWithoutHost:null,residualClasses:void 0,classBindings:0,styleBindings:0}}(0,s?o:o&&o.parent,n,e,r,i);return null===t.firstChild&&(t.firstChild=l),null!==o&&(s?null==o.child&&null!==l.parent&&(o.child=l):null===o.next&&(o.next=l)),l}(t,e,n,r,i),N.lFrame.inI18n&&(o.flags|=64);else if(64&o.type){o.type=n,o.value=r,o.attrs=i;const s=function(){const t=N.lFrame,e=t.currentTNode;return t.isParent?e:e.parent}();o.injectorIndex=null===s?-1:s.injectorIndex}return Gt(o,!0),o}function Hr(t,e,n,r){if(0===n)return-1;const i=e.length;for(let o=0;o<n;o++)e.push(r),t.blueprint.push(r),t.data.push(null);return i}function io(t,e,n){fs(e);try{const r=t.viewQuery;null!==r&&Fc(1,r,n);const i=t.template;null!==i&&Pf(t,e,i,1,n),t.firstCreatePass&&(t.firstCreatePass=!1),t.staticContentQueries&&Af(t,e),t.staticViewQueries&&Fc(2,t.viewQuery,n);const o=t.components;null!==o&&function(t,e){for(let n=0;n<e.length;n++)eE(t,e[n])}(e,o)}catch(r){throw t.firstCreatePass&&(t.incompleteFirstPass=!0,t.firstCreatePass=!1),r}finally{e[2]&=-5,gs()}}function $r(t,e,n,r){const i=e[2];if(256==(256&i))return;fs(e);const o=ps();try{$p(e),function(t){N.lFrame.bindingIndex=t}(t.bindingStartIndex),null!==n&&Pf(t,e,n,2,r);const s=3==(3&i);if(!o)if(s){const c=t.preOrderCheckHooks;null!==c&&bs(e,c,null)}else{const c=t.preOrderHooks;null!==c&&ys(e,c,0,null),Fl(e,0)}if(function(t){for(let e=lc(t);null!==e;e=cc(e)){if(!e[2])continue;const n=e[9];for(let r=0;r<n.length;r++){const i=n[r],o=i[3];0==(1024&i[2])&&Al(o,1),i[2]|=1024}}}(e),function(t){for(let e=lc(t);null!==e;e=cc(e))for(let n=10;n<e.length;n++){const r=e[n],i=r[1];Il(r)&&$r(i,r,i.template,r[8])}}(e),null!==t.contentQueries&&Af(t,e),!o)if(s){const c=t.contentCheckHooks;null!==c&&bs(e,c)}else{const c=t.contentHooks;null!==c&&ys(e,c,1),Fl(e,1)}!function(t,e){const n=t.hostBindingOpCodes;if(null!==n)try{for(let r=0;r<n.length;r++){const i=n[r];if(i<0)An(~i);else{const o=i,s=n[++r],a=n[++r];l_(s,o),a(2,e[o])}}}finally{An(-1)}}(t,e);const a=t.components;null!==a&&function(t,e){for(let n=0;n<e.length;n++)JD(t,e[n])}(e,a);const l=t.viewQuery;if(null!==l&&Fc(2,l,r),!o)if(s){const c=t.viewCheckHooks;null!==c&&bs(e,c)}else{const c=t.viewHooks;null!==c&&ys(e,c,2),Fl(e,2)}!0===t.firstUpdatePass&&(t.firstUpdatePass=!1),o||(e[2]&=-73),1024&e[2]&&(e[2]&=-1025,Al(e[3],-1))}finally{gs()}}function kD(t,e,n,r){const i=e[10],o=!ps(),s=function(t){return 4==(4&t[2])}(e);try{o&&!s&&i.begin&&i.begin(),s&&io(t,e,r),$r(t,e,n,r)}finally{o&&!s&&i.end&&i.end()}}function Pf(t,e,n,r,i){const o=N.lFrame.selectedIndex,s=2&r;try{An(-1),s&&e.length>20&&function(t,e,n,r){if(!r)if(3==(3&e[2])){const o=t.preOrderCheckHooks;null!==o&&bs(e,o,n)}else{const o=t.preOrderHooks;null!==o&&ys(e,o,0,n)}An(n)}(t,e,20,ps()),n(r,i)}finally{An(o)}}function Nf(t){const e=t.tView;return null===e||e.incompleteFirstPass?t.tView=zs(1,null,t.template,t.decls,t.vars,t.directiveDefs,t.pipeDefs,t.viewQuery,t.schemas,t.consts):e}function zs(t,e,n,r,i,o,s,a,l,c){const d=20+r,u=d+i,p=function(t,e){const n=[];for(let r=0;r<e;r++)n.push(r<t?null:O);return n}(d,u),h="function"==typeof c?c():c;return p[1]={type:t,blueprint:p,template:n,queries:null,viewQuery:a,declTNode:e,data:p.slice().fill(null,d),bindingStartIndex:d,expandoStartIndex:u,hostBindingOpCodes:null,firstCreatePass:!0,firstUpdatePass:!0,staticViewQueries:!1,staticContentQueries:!1,preOrderHooks:null,preOrderCheckHooks:null,contentHooks:null,contentCheckHooks:null,viewHooks:null,viewCheckHooks:null,destroyHooks:null,cleanup:null,contentQueries:null,components:null,directiveRegistry:"function"==typeof o?o():o,pipeRegistry:"function"==typeof s?s():s,firstChild:null,schemas:l,consts:h,incompleteFirstPass:!1}}function jf(t,e,n){for(let r in t)if(t.hasOwnProperty(r)){const i=t[r];(n=null===n?{}:n).hasOwnProperty(r)?n[r].push(e,i):n[r]=[e,i]}return n}function Vf(t,e,n,r,i,o){const s=o.hostBindings;if(s){let a=t.hostBindingOpCodes;null===a&&(a=t.hostBindingOpCodes=[]);const l=~e.index;(function(t){let e=t.length;for(;e>0;){const n=t[--e];if("number"==typeof n&&n<0)return n}return 0})(a)!=l&&a.push(l),a.push(r,i,s)}}function Bf(t,e){null!==t.hostBindings&&t.hostBindings(1,e)}function Uf(t,e){e.flags|=2,(t.components||(t.components=[])).push(e.index)}function GD(t,e,n){if(n){if(e.exportAs)for(let r=0;r<e.exportAs.length;r++)n[e.exportAs[r]]=t;At(e)&&(n[""]=t)}}function zf(t,e,n){t.flags|=1,t.directiveStart=e,t.directiveEnd=e+n,t.providerIndexes=e}function qf(t,e,n,r,i){t.data[r]=i;const o=i.factory||(i.factory=Xn(i.type)),s=new Vi(o,At(i),null);t.blueprint[r]=s,n[r]=s,Vf(t,e,0,r,Hr(t,n,i.hostVars,O),i)}function KD(t,e,n){const r=bt(e,t),i=Nf(n),o=t[10],s=qs(t,ro(t,i,null,n.onPush?64:16,r,e,o,o.createRenderer(r,n),null,null));t[e.index]=s}function YD(t,e,n,r,i,o){const s=o[e];if(null!==s){const a=r.setInput;for(let l=0;l<s.length;){const c=s[l++],d=s[l++],u=s[l++];null!==a?r.setInput(n,u,c,d):n[d]=u}}}function QD(t,e){let n=null,r=0;for(;r<e.length;){const i=e[r];if(0!==i)if(5!==i){if("number"==typeof i)break;t.hasOwnProperty(i)&&(null===n&&(n=[]),n.push(i,t[i],e[r+1])),r+=2}else r+=2;else r+=4}return n}function JD(t,e){const n=ot(e,t);if(Il(n)){const r=n[1];80&n[2]?$r(r,n,r.template,n[8]):n[5]>0&&Rc(n)}}function Rc(t){for(let r=lc(t);null!==r;r=cc(r))for(let i=10;i<r.length;i++){const o=r[i];if(1024&o[2]){const s=o[1];$r(s,o,s.template,o[8])}else o[5]>0&&Rc(o)}const n=t[1].components;if(null!==n)for(let r=0;r<n.length;r++){const i=ot(n[r],t);Il(i)&&i[5]>0&&Rc(i)}}function eE(t,e){const n=ot(e,t),r=n[1];(function(t,e){for(let n=e.length;n<t.blueprint.length;n++)e.push(t.blueprint[n])})(r,n),io(r,n,n[8])}function qs(t,e){return t[13]?t[14][4]=e:t[13]=e,t[14]=e,e}function Nc(t){for(;t;){t[2]|=64;const e=to(t);if(zw(t)&&!e)return t;t=e}return null}function Oc(t,e,n){const r=e[10];r.begin&&r.begin();try{$r(t,e,t.template,n)}catch(i){throw Yf(e,i),i}finally{r.end&&r.end()}}function $f(t){!function(t){for(let e=0;e<t.components.length;e++){const n=t.components[e],r=ic(n),i=r[1];kD(i,r,i.template,n)}}(t[8])}function Fc(t,e,n){Ol(0),e(t,n)}const oE=(()=>Promise.resolve(null))();function Wf(t){return t[7]||(t[7]=[])}function Gf(t){return t.cleanup||(t.cleanup=[])}function Yf(t,e){const n=t[9],r=n?n.get(ir,null):null;r&&r.handleError(e)}function Hs(t,e,n){let r=n?t.styles:null,i=n?t.classes:null,o=0;if(null!==e)for(let s=0;s<e.length;s++){const a=e[s];"number"==typeof a?o=a:1==o?i=fl(i,a):2==o&&(r=fl(r,a+": "+e[++s]+";"))}n?t.styles=r:t.stylesWithoutHost=r,n?t.classes=i:t.classesWithoutHost=i}const Wr=new Y("INJECTOR",-1);class Zf{get(e,n=Gi){if(n===Gi){const r=new Error(`NullInjectorError: No provider for ${z(e)}!`);throw r.name="NullInjectorError",r}return n}}const oo=new Y("Set Injector scope."),so={},lE={};let jc;function Xf(){return void 0===jc&&(jc=new Zf),jc}function Jf(t,e=null,n=null,r){return new dE(t,n,e||Xf(),r)}class dE{constructor(e,n,r,i=null){this.parent=r,this.records=new Map,this.injectorDefTypes=new Set,this.onDestroy=new Set,this._destroyed=!1;const o=[];n&&Yt(n,a=>this.processProvider(a,e,n)),Yt([e],a=>this.processInjectorType(a,[],o)),this.records.set(Wr,Gr(void 0,this));const s=this.records.get(oo);this.scope=null!=s?s.value:null,this.source=i||("object"==typeof e?null:z(e))}get destroyed(){return this._destroyed}destroy(){this.assertNotDestroyed(),this._destroyed=!0;try{this.onDestroy.forEach(e=>e.ngOnDestroy())}finally{this.records.clear(),this.onDestroy.clear(),this.injectorDefTypes.clear()}}get(e,n=Gi,r=I.Default){this.assertNotDestroyed();const i=jr(this),o=Sn(void 0);try{if(!(r&I.SkipSelf)){let a=this.records.get(e);if(void 0===a){const l=function(t){return"function"==typeof t||"object"==typeof t&&t instanceof Y}(e)&&un(e);a=l&&this.injectableDefInScope(l)?Gr(Lc(e),so):null,this.records.set(e,a)}if(null!=a)return this.hydrate(e,a)}return(r&I.Self?Xf():this.parent).get(e,n=r&I.Optional&&n===Gi?null:n)}catch(s){if("NullInjectorError"===s.name){if((s[Fr]=s[Fr]||[]).unshift(z(e)),i)throw s;return vh(s,e,"R3InjectorError",this.source)}throw s}finally{Sn(o),jr(i)}}_resolveInjectorDefTypes(){this.injectorDefTypes.forEach(e=>this.get(e))}toString(){const e=[];return this.records.forEach((r,i)=>e.push(z(i))),`R3Injector[${e.join(", ")}]`}assertNotDestroyed(){if(this._destroyed)throw new Error("Injector has already been destroyed.")}processInjectorType(e,n,r){if(!(e=M(e)))return!1;let i=Mp(e);const o=null==i&&e.ngModule||void 0,s=void 0===o?e:o,a=-1!==r.indexOf(s);if(void 0!==o&&(i=Mp(o)),null==i)return!1;if(null!=i.imports&&!a){let d;r.push(s);try{Yt(i.imports,u=>{this.processInjectorType(u,n,r)&&(void 0===d&&(d=[]),d.push(u))})}finally{}if(void 0!==d)for(let u=0;u<d.length;u++){const{ngModule:p,providers:h}=d[u];Yt(h,f=>this.processProvider(f,p,h||ee))}}this.injectorDefTypes.add(s);const l=Xn(s)||(()=>new s);this.records.set(s,Gr(l,so));const c=i.providers;if(null!=c&&!a){const d=e;Yt(c,u=>this.processProvider(u,d,c))}return void 0!==o&&void 0!==e.providers}processProvider(e,n,r){let i=Kr(e=M(e))?e:M(e&&e.provide);const o=function(t,e,n){return tg(t)?Gr(void 0,t.useValue):Gr(function(t,e,n){let r;if(Kr(t)){const i=M(t);return Xn(i)||Lc(i)}if(tg(t))r=()=>M(t.useValue);else if(function(t){return!(!t||!t.useFactory)}(t))r=()=>t.useFactory(...nr(t.deps||[]));else if(function(t){return!(!t||!t.useExisting)}(t))r=()=>T(M(t.useExisting));else{const i=M(t&&(t.useClass||t.provide));if(!function(t){return!!t.deps}(t))return Xn(i)||Lc(i);r=()=>new i(...nr(t.deps))}return r}(t),so)}(e);if(Kr(e)||!0!==e.multi)this.records.get(i);else{let s=this.records.get(i);s||(s=Gr(void 0,so,!0),s.factory=()=>nr(s.multi),this.records.set(i,s)),i=e,s.multi.push(e)}this.records.set(i,o)}hydrate(e,n){return n.value===so&&(n.value=lE,n.value=n.factory()),"object"==typeof n.value&&n.value&&function(t){return null!==t&&"object"==typeof t&&"function"==typeof t.ngOnDestroy}(n.value)&&this.onDestroy.add(n.value),n.value}injectableDefInScope(e){if(!e.providedIn)return!1;const n=M(e.providedIn);return"string"==typeof n?"any"===n||n===this.scope:this.injectorDefTypes.has(n)}}function Lc(t){const e=un(t),n=null!==e?e.factory:Xn(t);if(null!==n)return n;if(t instanceof Y)throw new Error(`Token ${z(t)} is missing a \u0275prov definition.`);if(t instanceof Function)return function(t){const e=t.length;if(e>0){const r=function(t,e){const n=[];for(let r=0;r<t;r++)n.push(e);return n}(e,"?");throw new Error(`Can't resolve all parameters for ${z(t)}: (${r.join(", ")}).`)}const n=function(t){const e=t&&(t[is]||t[Ip]);if(e){const n=function(t){if(t.hasOwnProperty("name"))return t.name;const e=(""+t).match(/^function\s*([^\s(]+)/);return null===e?"":e[1]}(t);return console.warn(`DEPRECATED: DI is instantiating a token "${n}" that inherits its @Injectable decorator but does not provide one itself.\nThis will become an error in a future version of Angular. Please add @Injectable() to the "${n}" class.`),e}return null}(t);return null!==n?()=>n.factory(t):()=>new t}(t);throw new Error("unreachable")}function Gr(t,e,n=!1){return{factory:t,value:e,multi:n?[]:void 0}}function tg(t){return null!==t&&"object"==typeof t&&Kl in t}function Kr(t){return"function"==typeof t}const ng=function(t,e,n){return function(t,e=null,n=null,r){const i=Jf(t,e,n,r);return i._resolveInjectorDefTypes(),i}({name:n},e,t,n)};let te=(()=>{class t{static create(n,r){return Array.isArray(n)?ng(n,r,""):ng(n.providers,n.parent,n.name||"")}}return t.THROW_IF_NOT_FOUND=Gi,t.NULL=new Zf,t.\u0275prov=q({token:t,providedIn:"any",factory:()=>T(Wr)}),t.__NG_ELEMENT_ID__=-1,t})();function PE(t,e){ms(ic(t)[1],Te())}let $s=null;function Yr(){if(!$s){const t=X.Symbol;if(t&&t.iterator)$s=t.iterator;else{const e=Object.getOwnPropertyNames(Map.prototype);for(let n=0;n<e.length;++n){const r=e[n];"entries"!==r&&"size"!==r&&Map.prototype[r]===Map.prototype.entries&&($s=r)}}}return $s}function lo(t){return!!$c(t)&&(Array.isArray(t)||!(t instanceof Map)&&Yr()in t)}function $c(t){return null!==t&&("function"==typeof t||"object"==typeof t)}function x(t,e=I.Default){const n=w();return null===n?T(t,e):dh(Te(),n,M(t),e)}function Zc(t,e,n,r,i){const s=i?"class":"style";!function(t,e,n,r,i){for(let o=0;o<n.length;){const s=n[o++],a=n[o++],l=e[s],c=t.data[s];null!==c.setInput?c.setInput(l,i,r,a):l[a]=i}}(t,n,e.inputs[s],s,r)}function _(t,e,n,r){const i=w(),o=K(),s=20+t,a=i[V],l=i[s]=uc(a,e,N.lFrame.currentNamespace),c=o.firstCreatePass?function(t,e,n,r,i,o,s){const a=e.consts,c=qr(e,t,2,i,In(a,o));return function(t,e,n,r){let i=!1;if(Wp()){const o=function(t,e,n){const r=t.directiveRegistry;let i=null;if(r)for(let o=0;o<r.length;o++){const s=r[o];yf(n,s.selectors,!1)&&(i||(i=[]),Ds(Ui(n,e),t,s.type),At(s)?(Uf(t,n),i.unshift(s)):i.push(s))}return i}(t,e,n),s=null===r?null:{"":-1};if(null!==o){i=!0,zf(n,t.data.length,o.length);for(let d=0;d<o.length;d++){const u=o[d];u.providersResolver&&u.providersResolver(u)}let a=!1,l=!1,c=Hr(t,e,o.length,null);for(let d=0;d<o.length;d++){const u=o[d];n.mergedAttrs=ws(n.mergedAttrs,u.hostAttrs),qf(t,n,e,c,u),GD(c,u,s),null!==u.contentQueries&&(n.flags|=8),(null!==u.hostBindings||null!==u.hostAttrs||0!==u.hostVars)&&(n.flags|=128);const p=u.type.prototype;!a&&(p.ngOnChanges||p.ngOnInit||p.ngDoCheck)&&((t.preOrderHooks||(t.preOrderHooks=[])).push(n.index),a=!0),!l&&(p.ngOnChanges||p.ngDoCheck)&&((t.preOrderCheckHooks||(t.preOrderCheckHooks=[])).push(n.index),l=!0),c++}!function(t,e){const r=e.directiveEnd,i=t.data,o=e.attrs,s=[];let a=null,l=null;for(let c=e.directiveStart;c<r;c++){const d=i[c],u=d.inputs,p=null===o||bf(e)?null:QD(u,o);s.push(p),a=jf(u,c,a),l=jf(d.outputs,c,l)}null!==a&&(a.hasOwnProperty("class")&&(e.flags|=16),a.hasOwnProperty("style")&&(e.flags|=32)),e.initialInputs=s,e.inputs=a,e.outputs=l}(t,n)}s&&function(t,e,n){if(e){const r=t.localNames=[];for(let i=0;i<e.length;i+=2){const o=n[e[i+1]];if(null==o)throw new $n("301",`Export of name '${e[i+1]}' not found!`);r.push(e[i],o)}}}(n,r,s)}n.mergedAttrs=ws(n.mergedAttrs,n.attrs)}(e,n,c,In(a,s)),null!==c.attrs&&Hs(c,c.attrs,!1),null!==c.mergedAttrs&&Hs(c,c.mergedAttrs,!0),null!==e.queries&&e.queries.elementStart(e,c),c}(s,o,i,0,e,n,r):o.data[s];Gt(c,!0);const d=c.mergedAttrs;null!==d&&vs(a,l,d);const u=c.classes;null!==u&&bc(a,l,u);const p=c.styles;null!==p&&ff(a,l,p),64!=(64&c.flags)&&js(o,i,l,c),0===N.lFrame.elementDepthCount&&Le(l,i),N.lFrame.elementDepthCount++,ds(c)&&(function(t,e,n){!Wp()||(function(t,e,n,r){const i=n.directiveStart,o=n.directiveEnd;t.firstCreatePass||Ui(n,e),Le(r,e);const s=n.initialInputs;for(let a=i;a<o;a++){const l=t.data[a],c=At(l);c&&KD(e,n,l);const d=zi(e,t,a,n);Le(d,e),null!==s&&YD(0,a-i,d,l,0,s),c&&(ot(n.index,e)[8]=d)}}(t,e,n,bt(n,e)),128==(128&n.flags)&&function(t,e,n){const r=n.directiveStart,i=n.directiveEnd,s=n.index,a=N.lFrame.currentDirectiveIndex;try{An(s);for(let l=r;l<i;l++){const c=t.data[l],d=e[l];Nl(l),(null!==c.hostBindings||0!==c.hostVars||null!==c.hostAttrs)&&Bf(c,d)}}finally{An(-1),Nl(a)}}(t,e,n))}(o,i,c),function(t,e,n){if(Dl(e)){const i=e.directiveEnd;for(let o=e.directiveStart;o<i;o++){const s=t.data[o];s.contentQueries&&s.contentQueries(1,n[o],o)}}}(o,c,i)),null!==r&&function(t,e,n=bt){const r=e.localNames;if(null!==r){let i=e.index+1;for(let o=0;o<r.length;o+=2){const s=r[o+1],a=-1===s?n(e,t):t[s];t[i++]=a}}}(i,c)}function C(){let t=Te();Pl()?N.lFrame.isParent=!1:(t=t.parent,Gt(t,!1));const e=t;N.lFrame.elementDepthCount--;const n=K();n.firstCreatePass&&(ms(n,t),Dl(t)&&n.queries.elementEnd(t)),null!=e.classesWithoutHost&&function(t){return 0!=(16&t.flags)}(e)&&Zc(n,e,w(),e.classesWithoutHost,!0),null!=e.stylesWithoutHost&&function(t){return 0!=(32&t.flags)}(e)&&Zc(n,e,w(),e.stylesWithoutHost,!1)}function Q(t,e,n,r){_(t,e,n,r),C()}function Ks(t){return!!t&&"function"==typeof t.then}const Xc=function(t){return!!t&&"function"==typeof t.subscribe};function le(t,e,n,r){const i=w(),o=K(),s=Te();return function(t,e,n,r,i,o,s,a){const l=ds(r),d=t.firstCreatePass&&Gf(t),u=e[8],p=Wf(e);let h=!0;if(3&r.type||a){const g=bt(r,e),y=a?a(g):g,b=p.length,v=a?S=>a(we(S[r.index])):r.index;if(ge(n)){let S=null;if(!a&&l&&(S=function(t,e,n,r){const i=t.cleanup;if(null!=i)for(let o=0;o<i.length-1;o+=2){const s=i[o];if(s===n&&i[o+1]===r){const a=e[7],l=i[o+2];return a.length>l?a[l]:null}"string"==typeof s&&(o+=2)}return null}(t,e,i,r.index)),null!==S)(S.__ngLastListenerFn__||S).__ngNextListenerFn__=o,S.__ngLastListenerFn__=o,h=!1;else{o=Jc(r,e,u,o,!1);const R=n.listen(y,i,o);p.push(o,R),d&&d.push(i,v,b,b+1)}}else o=Jc(r,e,u,o,!0),y.addEventListener(i,o,s),p.push(o),d&&d.push(i,v,b,s)}else o=Jc(r,e,u,o,!1);const f=r.outputs;let m;if(h&&null!==f&&(m=f[i])){const g=m.length;if(g)for(let y=0;y<g;y+=2){const ce=e[m[y]][m[y+1]].subscribe(o),ze=p.length;p.push(o,ce),d&&d.push(i,r.index,ze,-(ze+1))}}}(o,i,i[V],s,t,e,!!n,r),le}function Gg(t,e,n,r){try{return!1!==n(r)}catch(i){return Yf(t,i),!1}}function Jc(t,e,n,r,i){return function o(s){if(s===Function)return r;const a=2&t.flags?ot(t.index,e):e;0==(32&e[2])&&Nc(a);let l=Gg(e,0,r,s),c=o.__ngNextListenerFn__;for(;c;)l=Gg(e,0,c,s)&&l,c=c.__ngNextListenerFn__;return i&&!1===l&&(s.preventDefault(),s.returnValue=!1),l}}function B(t,e=""){const n=w(),r=K(),i=t+20,o=r.firstCreatePass?qr(r,i,1,e,null):r.data[i],s=n[i]=function(t,e){return ge(t)?t.createText(e):t.createTextNode(e)}(n[V],e);js(r,n,s,o),Gt(o,!1)}const ar=void 0;var sT=["en",[["a","p"],["AM","PM"],ar],[["AM","PM"],ar,ar],[["S","M","T","W","T","F","S"],["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],["Su","Mo","Tu","We","Th","Fr","Sa"]],ar,[["J","F","M","A","M","J","J","A","S","O","N","D"],["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],["January","February","March","April","May","June","July","August","September","October","November","December"]],ar,[["B","A"],["BC","AD"],["Before Christ","Anno Domini"]],0,[6,0],["M/d/yy","MMM d, y","MMMM d, y","EEEE, MMMM d, y"],["h:mm a","h:mm:ss a","h:mm:ss a z","h:mm:ss a zzzz"],["{1}, {0}",ar,"{1} 'at' {0}",ar],[".",",",";","%","+","-","E","\xd7","\u2030","\u221e","NaN",":"],["#,##0.###","#,##0%","\xa4#,##0.00","#E0"],"USD","$","US Dollar",{},"ltr",function(t){const e=Math.floor(Math.abs(t)),n=t.toString().replace(/^[^.]*\.?/,"").length;return 1===e&&0===n?1:5}];let ai={};function Lm(t){return t in ai||(ai[t]=X.ng&&X.ng.common&&X.ng.common.locales&&X.ng.common.locales[t]),ai[t]}var E=(()=>((E=E||{})[E.LocaleId=0]="LocaleId",E[E.DayPeriodsFormat=1]="DayPeriodsFormat",E[E.DayPeriodsStandalone=2]="DayPeriodsStandalone",E[E.DaysFormat=3]="DaysFormat",E[E.DaysStandalone=4]="DaysStandalone",E[E.MonthsFormat=5]="MonthsFormat",E[E.MonthsStandalone=6]="MonthsStandalone",E[E.Eras=7]="Eras",E[E.FirstDayOfWeek=8]="FirstDayOfWeek",E[E.WeekendRange=9]="WeekendRange",E[E.DateFormat=10]="DateFormat",E[E.TimeFormat=11]="TimeFormat",E[E.DateTimeFormat=12]="DateTimeFormat",E[E.NumberSymbols=13]="NumberSymbols",E[E.NumberFormats=14]="NumberFormats",E[E.CurrencyCode=15]="CurrencyCode",E[E.CurrencySymbol=16]="CurrencySymbol",E[E.CurrencyName=17]="CurrencyName",E[E.Currencies=18]="Currencies",E[E.Directionality=19]="Directionality",E[E.PluralCase=20]="PluralCase",E[E.ExtraData=21]="ExtraData",E))();const Qs="en-US";let Vm=Qs;function id(t){rt(t,"Expected localeId to be defined"),"string"==typeof t&&(Vm=t.toLowerCase().replace(/_/g,"-"))}class db{}const pb="ngComponent";class aS{resolveComponentFactory(e){throw function(t){const e=Error(`No component factory found for ${z(t)}. Did you add it to @NgModule.entryComponents?`);return e[pb]=t,e}(e)}}let lr=(()=>{class t{}return t.NULL=new aS,t})();function ta(...t){}function ci(t,e){return new dt(bt(t,e))}const dS=function(){return ci(Te(),w())};let dt=(()=>{class t{constructor(n){this.nativeElement=n}}return t.__NG_ELEMENT_ID__=dS,t})();class na{}let pd=(()=>{class t{}return t.\u0275prov=q({token:t,providedIn:"root",factory:()=>null}),t})();class ia{constructor(e){this.full=e,this.major=e.split(".")[0],this.minor=e.split(".")[1],this.patch=e.split(".").slice(2).join(".")}}const fb=new ia("12.2.6");class gb{constructor(){}supports(e){return lo(e)}create(e){return new mS(e)}}const gS=(t,e)=>e;class mS{constructor(e){this.length=0,this._linkedRecords=null,this._unlinkedRecords=null,this._previousItHead=null,this._itHead=null,this._itTail=null,this._additionsHead=null,this._additionsTail=null,this._movesHead=null,this._movesTail=null,this._removalsHead=null,this._removalsTail=null,this._identityChangesHead=null,this._identityChangesTail=null,this._trackByFn=e||gS}forEachItem(e){let n;for(n=this._itHead;null!==n;n=n._next)e(n)}forEachOperation(e){let n=this._itHead,r=this._removalsHead,i=0,o=null;for(;n||r;){const s=!r||n&&n.currentIndex<bb(r,i,o)?n:r,a=bb(s,i,o),l=s.currentIndex;if(s===r)i--,r=r._nextRemoved;else if(n=n._next,null==s.previousIndex)i++;else{o||(o=[]);const c=a-i,d=l-i;if(c!=d){for(let p=0;p<c;p++){const h=p<o.length?o[p]:o[p]=0,f=h+p;d<=f&&f<c&&(o[p]=h+1)}o[s.previousIndex]=d-c}}a!==l&&e(s,a,l)}}forEachPreviousItem(e){let n;for(n=this._previousItHead;null!==n;n=n._nextPrevious)e(n)}forEachAddedItem(e){let n;for(n=this._additionsHead;null!==n;n=n._nextAdded)e(n)}forEachMovedItem(e){let n;for(n=this._movesHead;null!==n;n=n._nextMoved)e(n)}forEachRemovedItem(e){let n;for(n=this._removalsHead;null!==n;n=n._nextRemoved)e(n)}forEachIdentityChange(e){let n;for(n=this._identityChangesHead;null!==n;n=n._nextIdentityChange)e(n)}diff(e){if(null==e&&(e=[]),!lo(e))throw new Error(`Error trying to diff '${z(e)}'. Only arrays and iterables are allowed`);return this.check(e)?this:null}onDestroy(){}check(e){this._reset();let i,o,s,n=this._itHead,r=!1;if(Array.isArray(e)){this.length=e.length;for(let a=0;a<this.length;a++)o=e[a],s=this._trackByFn(a,o),null!==n&&Object.is(n.trackById,s)?(r&&(n=this._verifyReinsertion(n,o,s,a)),Object.is(n.item,o)||this._addIdentityChange(n,o)):(n=this._mismatch(n,o,s,a),r=!0),n=n._next}else i=0,function(t,e){if(Array.isArray(t))for(let n=0;n<t.length;n++)e(t[n]);else{const n=t[Yr()]();let r;for(;!(r=n.next()).done;)e(r.value)}}(e,a=>{s=this._trackByFn(i,a),null!==n&&Object.is(n.trackById,s)?(r&&(n=this._verifyReinsertion(n,a,s,i)),Object.is(n.item,a)||this._addIdentityChange(n,a)):(n=this._mismatch(n,a,s,i),r=!0),n=n._next,i++}),this.length=i;return this._truncate(n),this.collection=e,this.isDirty}get isDirty(){return null!==this._additionsHead||null!==this._movesHead||null!==this._removalsHead||null!==this._identityChangesHead}_reset(){if(this.isDirty){let e;for(e=this._previousItHead=this._itHead;null!==e;e=e._next)e._nextPrevious=e._next;for(e=this._additionsHead;null!==e;e=e._nextAdded)e.previousIndex=e.currentIndex;for(this._additionsHead=this._additionsTail=null,e=this._movesHead;null!==e;e=e._nextMoved)e.previousIndex=e.currentIndex;this._movesHead=this._movesTail=null,this._removalsHead=this._removalsTail=null,this._identityChangesHead=this._identityChangesTail=null}}_mismatch(e,n,r,i){let o;return null===e?o=this._itTail:(o=e._prev,this._remove(e)),null!==(e=null===this._unlinkedRecords?null:this._unlinkedRecords.get(r,null))?(Object.is(e.item,n)||this._addIdentityChange(e,n),this._reinsertAfter(e,o,i)):null!==(e=null===this._linkedRecords?null:this._linkedRecords.get(r,i))?(Object.is(e.item,n)||this._addIdentityChange(e,n),this._moveAfter(e,o,i)):e=this._addAfter(new bS(n,r),o,i),e}_verifyReinsertion(e,n,r,i){let o=null===this._unlinkedRecords?null:this._unlinkedRecords.get(r,null);return null!==o?e=this._reinsertAfter(o,e._prev,i):e.currentIndex!=i&&(e.currentIndex=i,this._addToMoves(e,i)),e}_truncate(e){for(;null!==e;){const n=e._next;this._addToRemovals(this._unlink(e)),e=n}null!==this._unlinkedRecords&&this._unlinkedRecords.clear(),null!==this._additionsTail&&(this._additionsTail._nextAdded=null),null!==this._movesTail&&(this._movesTail._nextMoved=null),null!==this._itTail&&(this._itTail._next=null),null!==this._removalsTail&&(this._removalsTail._nextRemoved=null),null!==this._identityChangesTail&&(this._identityChangesTail._nextIdentityChange=null)}_reinsertAfter(e,n,r){null!==this._unlinkedRecords&&this._unlinkedRecords.remove(e);const i=e._prevRemoved,o=e._nextRemoved;return null===i?this._removalsHead=o:i._nextRemoved=o,null===o?this._removalsTail=i:o._prevRemoved=i,this._insertAfter(e,n,r),this._addToMoves(e,r),e}_moveAfter(e,n,r){return this._unlink(e),this._insertAfter(e,n,r),this._addToMoves(e,r),e}_addAfter(e,n,r){return this._insertAfter(e,n,r),this._additionsTail=null===this._additionsTail?this._additionsHead=e:this._additionsTail._nextAdded=e,e}_insertAfter(e,n,r){const i=null===n?this._itHead:n._next;return e._next=i,e._prev=n,null===i?this._itTail=e:i._prev=e,null===n?this._itHead=e:n._next=e,null===this._linkedRecords&&(this._linkedRecords=new mb),this._linkedRecords.put(e),e.currentIndex=r,e}_remove(e){return this._addToRemovals(this._unlink(e))}_unlink(e){null!==this._linkedRecords&&this._linkedRecords.remove(e);const n=e._prev,r=e._next;return null===n?this._itHead=r:n._next=r,null===r?this._itTail=n:r._prev=n,e}_addToMoves(e,n){return e.previousIndex===n||(this._movesTail=null===this._movesTail?this._movesHead=e:this._movesTail._nextMoved=e),e}_addToRemovals(e){return null===this._unlinkedRecords&&(this._unlinkedRecords=new mb),this._unlinkedRecords.put(e),e.currentIndex=null,e._nextRemoved=null,null===this._removalsTail?(this._removalsTail=this._removalsHead=e,e._prevRemoved=null):(e._prevRemoved=this._removalsTail,this._removalsTail=this._removalsTail._nextRemoved=e),e}_addIdentityChange(e,n){return e.item=n,this._identityChangesTail=null===this._identityChangesTail?this._identityChangesHead=e:this._identityChangesTail._nextIdentityChange=e,e}}class bS{constructor(e,n){this.item=e,this.trackById=n,this.currentIndex=null,this.previousIndex=null,this._nextPrevious=null,this._prev=null,this._next=null,this._prevDup=null,this._nextDup=null,this._prevRemoved=null,this._nextRemoved=null,this._nextAdded=null,this._nextMoved=null,this._nextIdentityChange=null}}class yS{constructor(){this._head=null,this._tail=null}add(e){null===this._head?(this._head=this._tail=e,e._nextDup=null,e._prevDup=null):(this._tail._nextDup=e,e._prevDup=this._tail,e._nextDup=null,this._tail=e)}get(e,n){let r;for(r=this._head;null!==r;r=r._nextDup)if((null===n||n<=r.currentIndex)&&Object.is(r.trackById,e))return r;return null}remove(e){const n=e._prevDup,r=e._nextDup;return null===n?this._head=r:n._nextDup=r,null===r?this._tail=n:r._prevDup=n,null===this._head}}class mb{constructor(){this.map=new Map}put(e){const n=e.trackById;let r=this.map.get(n);r||(r=new yS,this.map.set(n,r)),r.add(e)}get(e,n){const i=this.map.get(e);return i?i.get(e,n):null}remove(e){const n=e.trackById;return this.map.get(n).remove(e)&&this.map.delete(n),e}get isEmpty(){return 0===this.map.size}clear(){this.map.clear()}}function bb(t,e,n){const r=t.previousIndex;if(null===r)return r;let i=0;return n&&r<n.length&&(i=n[r]),r+e+i}class yb{constructor(){}supports(e){return e instanceof Map||$c(e)}create(){return new vS}}class vS{constructor(){this._records=new Map,this._mapHead=null,this._appendAfter=null,this._previousMapHead=null,this._changesHead=null,this._changesTail=null,this._additionsHead=null,this._additionsTail=null,this._removalsHead=null,this._removalsTail=null}get isDirty(){return null!==this._additionsHead||null!==this._changesHead||null!==this._removalsHead}forEachItem(e){let n;for(n=this._mapHead;null!==n;n=n._next)e(n)}forEachPreviousItem(e){let n;for(n=this._previousMapHead;null!==n;n=n._nextPrevious)e(n)}forEachChangedItem(e){let n;for(n=this._changesHead;null!==n;n=n._nextChanged)e(n)}forEachAddedItem(e){let n;for(n=this._additionsHead;null!==n;n=n._nextAdded)e(n)}forEachRemovedItem(e){let n;for(n=this._removalsHead;null!==n;n=n._nextRemoved)e(n)}diff(e){if(e){if(!(e instanceof Map||$c(e)))throw new Error(`Error trying to diff '${z(e)}'. Only maps and objects are allowed`)}else e=new Map;return this.check(e)?this:null}onDestroy(){}check(e){this._reset();let n=this._mapHead;if(this._appendAfter=null,this._forEach(e,(r,i)=>{if(n&&n.key===i)this._maybeAddToChanges(n,r),this._appendAfter=n,n=n._next;else{const o=this._getOrCreateRecordForKey(i,r);n=this._insertBeforeOrAppend(n,o)}}),n){n._prev&&(n._prev._next=null),this._removalsHead=n;for(let r=n;null!==r;r=r._nextRemoved)r===this._mapHead&&(this._mapHead=null),this._records.delete(r.key),r._nextRemoved=r._next,r.previousValue=r.currentValue,r.currentValue=null,r._prev=null,r._next=null}return this._changesTail&&(this._changesTail._nextChanged=null),this._additionsTail&&(this._additionsTail._nextAdded=null),this.isDirty}_insertBeforeOrAppend(e,n){if(e){const r=e._prev;return n._next=e,n._prev=r,e._prev=n,r&&(r._next=n),e===this._mapHead&&(this._mapHead=n),this._appendAfter=e,e}return this._appendAfter?(this._appendAfter._next=n,n._prev=this._appendAfter):this._mapHead=n,this._appendAfter=n,null}_getOrCreateRecordForKey(e,n){if(this._records.has(e)){const i=this._records.get(e);this._maybeAddToChanges(i,n);const o=i._prev,s=i._next;return o&&(o._next=s),s&&(s._prev=o),i._next=null,i._prev=null,i}const r=new wS(e);return this._records.set(e,r),r.currentValue=n,this._addToAdditions(r),r}_reset(){if(this.isDirty){let e;for(this._previousMapHead=this._mapHead,e=this._previousMapHead;null!==e;e=e._next)e._nextPrevious=e._next;for(e=this._changesHead;null!==e;e=e._nextChanged)e.previousValue=e.currentValue;for(e=this._additionsHead;null!=e;e=e._nextAdded)e.previousValue=e.currentValue;this._changesHead=this._changesTail=null,this._additionsHead=this._additionsTail=null,this._removalsHead=null}}_maybeAddToChanges(e,n){Object.is(n,e.currentValue)||(e.previousValue=e.currentValue,e.currentValue=n,this._addToChanges(e))}_addToAdditions(e){null===this._additionsHead?this._additionsHead=this._additionsTail=e:(this._additionsTail._nextAdded=e,this._additionsTail=e)}_addToChanges(e){null===this._changesHead?this._changesHead=this._changesTail=e:(this._changesTail._nextChanged=e,this._changesTail=e)}_forEach(e,n){e instanceof Map?e.forEach(n):Object.keys(e).forEach(r=>n(e[r],r))}}class wS{constructor(e){this.key=e,this.previousValue=null,this.currentValue=null,this._nextPrevious=null,this._next=null,this._prev=null,this._nextAdded=null,this._nextRemoved=null,this._nextChanged=null}}function vb(){return new mo([new gb])}let mo=(()=>{class t{constructor(n){this.factories=n}static create(n,r){if(null!=r){const i=r.factories.slice();n=n.concat(i)}return new t(n)}static extend(n){return{provide:t,useFactory:r=>t.create(n,r||vb()),deps:[[t,new Nn,new et]]}}find(n){const r=this.factories.find(i=>i.supports(n));if(null!=r)return r;throw new Error(`Cannot find a differ supporting object '${n}' of type '${function(t){return t.name||typeof t}(n)}'`)}}return t.\u0275prov=q({token:t,providedIn:"root",factory:vb}),t})();function wb(){return new di([new yb])}let di=(()=>{class t{constructor(n){this.factories=n}static create(n,r){if(r){const i=r.factories.slice();n=n.concat(i)}return new t(n)}static extend(n){return{provide:t,useFactory:r=>t.create(n,r||wb()),deps:[[t,new Nn,new et]]}}find(n){const r=this.factories.find(i=>i.supports(n));if(r)return r;throw new Error(`Cannot find a differ supporting object '${n}'`)}}return t.\u0275prov=q({token:t,providedIn:"root",factory:wb}),t})();function oa(t,e,n,r,i=!1){for(;null!==n;){const o=e[n.index];if(null!==o&&r.push(we(o)),It(o))for(let a=10;a<o.length;a++){const l=o[a],c=l[1].firstChild;null!==c&&oa(l[1],l,c,r)}const s=n.type;if(8&s)oa(t,e,n.child,r);else if(32&s){const a=ac(n,e);let l;for(;l=a();)r.push(l)}else if(16&s){const a=uf(e,n);if(Array.isArray(a))r.push(...a);else{const l=to(e[16]);oa(l[1],l,a,r,!0)}}n=i?n.projectionNext:n.next}return r}class bo{constructor(e,n){this._lView=e,this._cdRefInjectingView=n,this._appRef=null,this._attachedToViewContainer=!1}get rootNodes(){const e=this._lView,n=e[1];return oa(n,e,n.firstChild,[])}get context(){return this._lView[8]}set context(e){this._lView[8]=e}get destroyed(){return 256==(256&this._lView[2])}destroy(){if(this._appRef)this._appRef.detachView(this);else if(this._attachedToViewContainer){const e=this._lView[3];if(It(e)){const n=e[8],r=n?n.indexOf(this):-1;r>-1&&(pc(e,r),tr(n,r))}this._attachedToViewContainer=!1}tf(this._lView[1],this._lView)}onDestroy(e){!function(t,e,n,r){const i=Wf(e);null===n?i.push(r):(i.push(n),t.firstCreatePass&&Gf(t).push(r,i.length-1))}(this._lView[1],this._lView,null,e)}markForCheck(){Nc(this._cdRefInjectingView||this._lView)}detach(){this._lView[2]&=-129}reattach(){this._lView[2]|=128}detectChanges(){Oc(this._lView[1],this._lView,this.context)}checkNoChanges(){!function(t,e,n){hs(!0);try{Oc(t,e,n)}finally{hs(!1)}}(this._lView[1],this._lView,this.context)}attachToViewContainerRef(){if(this._appRef)throw new Error("This view is already attached directly to the ApplicationRef!");this._attachedToViewContainer=!0}detachFromAppRef(){this._appRef=null,function(t,e){no(t,e,e[V],2,null,null)}(this._lView[1],this._lView)}attachToAppRef(e){if(this._attachedToViewContainer)throw new Error("This view is already attached to a ViewContainer!");this._appRef=e}}class CS extends bo{constructor(e){super(e),this._view=e}detectChanges(){$f(this._view)}checkNoChanges(){!function(t){hs(!0);try{$f(t)}finally{hs(!1)}}(this._view)}get context(){return null}}const ES=function(t){return function(t,e,n){if(cs(t)&&!n){const r=ot(t.index,e);return new bo(r,r)}return 47&t.type?new bo(e[16],e):null}(Te(),w(),16==(16&t))};let hd=(()=>{class t{}return t.__NG_ELEMENT_ID__=ES,t})();const xS=[new yb],IS=new mo([new gb]),AS=new di(xS);class rn{}class _b{}const jS=function(){return function(t,e){let n;const r=e[t.index];if(It(r))n=r;else{let i;if(8&t.type)i=we(r);else{const o=e[V];i=o.createComment("");const s=bt(t,e);or(o,Fs(o,s),i,function(t,e){return ge(t)?t.nextSibling(e):e.nextSibling}(o,s),!1)}e[t.index]=n=function(t,e,n,r){return new Array(t,!0,!1,e,null,0,r,n,null,null)}(r,e,i,t),qs(e,n)}return new Cb(n,t,e)}(Te(),w())};let jt=(()=>{class t{}return t.__NG_ELEMENT_ID__=jS,t})();const VS=jt,Cb=class extends VS{constructor(e,n,r){super(),this._lContainer=e,this._hostTNode=n,this._hostLView=r}get element(){return ci(this._hostTNode,this._hostLView)}get injector(){return new Pr(this._hostTNode,this._hostLView)}get parentInjector(){const e=Cs(this._hostTNode,this._hostLView);if(oh(e)){const n=Ar(e,this._hostLView),r=Ir(e);return new Pr(n[1].data[r+8],n)}return new Pr(null,this._hostLView)}clear(){for(;this.length>0;)this.remove(this.length-1)}get(e){const n=Db(this._lContainer);return null!==n&&n[e]||null}get length(){return this._lContainer.length-10}createEmbeddedView(e,n,r){const i=e.createEmbeddedView(n||{});return this.insert(i,r),i}createComponent(e,n,r,i,o){const s=r||this.parentInjector;if(!o&&null==e.ngModule&&s){const l=s.get(rn,null);l&&(o=l)}const a=e.create(s,i,void 0,o);return this.insert(a.hostView,n),a}insert(e,n){const r=e._lView,i=r[1];if(function(t){return It(t[3])}(r)){const d=this.indexOf(e);if(-1!==d)this.detach(d);else{const u=r[3],p=new Cb(u,u[6],u[3]);p.detach(p.indexOf(e))}}const o=this._adjustIndex(n),s=this._lContainer;!function(t,e,n,r){const i=10+r,o=n.length;r>0&&(n[i-1][4]=e),r<o-10?(e[4]=n[i],xs(n,10+r,e)):(n.push(e),e[4]=null),e[3]=n;const s=e[17];null!==s&&n!==s&&function(t,e){const n=t[9];e[16]!==e[3][3][16]&&(t[2]=!0),null===n?t[9]=[e]:n.push(e)}(s,e);const a=e[19];null!==a&&a.insertView(t),e[2]|=128}(i,r,s,o);const a=gc(o,s),l=r[V],c=Fs(l,s[7]);return null!==c&&function(t,e,n,r,i,o){r[0]=i,r[6]=e,no(t,r,n,1,i,o)}(i,s[6],l,r,c,a),e.attachToViewContainerRef(),xs(fd(s),o,e),e}move(e,n){return this.insert(e,n)}indexOf(e){const n=Db(this._lContainer);return null!==n?n.indexOf(e):-1}remove(e){const n=this._adjustIndex(e,-1),r=pc(this._lContainer,n);r&&(tr(fd(this._lContainer),n),tf(r[1],r))}detach(e){const n=this._adjustIndex(e,-1),r=pc(this._lContainer,n);return r&&null!=tr(fd(this._lContainer),n)?new bo(r):null}_adjustIndex(e,n=0){return null==e?this.length+n:e}};function Db(t){return t[8]}function fd(t){return t[8]||(t[8]=[])}const fi={};class Hb extends lr{constructor(e){super(),this.ngModule=e}resolveComponentFactory(e){const n=$e(e);return new Wb(n,this.ngModule)}}function $b(t){const e=[];for(let n in t)t.hasOwnProperty(n)&&e.push({propName:t[n],templateName:n});return e}const Ox=new Y("SCHEDULER_TOKEN",{providedIn:"root",factory:()=>Gh});class Wb extends db{constructor(e,n){super(),this.componentDef=e,this.ngModule=n,this.componentType=e.type,this.selector=function(t){return t.map(wD).join(",")}(e.selectors),this.ngContentSelectors=e.ngContentSelectors?e.ngContentSelectors:[],this.isBoundToModule=!!n}get inputs(){return $b(this.componentDef.inputs)}get outputs(){return $b(this.componentDef.outputs)}create(e,n,r,i){const o=(i=i||this.ngModule)?function(t,e){return{get:(n,r,i)=>{const o=t.get(n,fi,i);return o!==fi||r===fi?o:e.get(n,r,i)}}}(e,i.injector):e,s=o.get(na,qp),a=o.get(pd,null),l=s.createRenderer(null,this.componentDef),c=this.componentDef.selectors[0][0]||"div",d=r?function(t,e,n){if(ge(t))return t.selectRootElement(e,n===Ee.ShadowDom);let r="string"==typeof e?t.querySelector(e):e;return r.textContent="",r}(l,r,this.componentDef.encapsulation):uc(s.createRenderer(null,this.componentDef),c,function(t){const e=t.toLowerCase();return"svg"===e?"http://www.w3.org/2000/svg":"math"===e?"http://www.w3.org/1998/MathML/":null}(c)),u=this.componentDef.onPush?576:528,p=function(t,e){return{components:[],scheduler:t||Gh,clean:oE,playerHandler:e||null,flags:0}}(),h=zs(0,null,null,1,0,null,null,null,null,null),f=ro(null,h,p,u,null,null,s,l,a,o);let m,g;fs(f);try{const y=function(t,e,n,r,i,o){const s=n[1];n[20]=t;const l=qr(s,20,2,"#host",null),c=l.mergedAttrs=e.hostAttrs;null!==c&&(Hs(l,c,!0),null!==t&&(vs(i,t,c),null!==l.classes&&bc(i,t,l.classes),null!==l.styles&&ff(i,t,l.styles)));const d=r.createRenderer(t,e),u=ro(n,Nf(e),null,e.onPush?64:16,n[20],l,r,d,o||null,null);return s.firstCreatePass&&(Ds(Ui(l,n),s,e.type),Uf(s,l),zf(l,n.length,1)),qs(n,u),n[20]=u}(d,this.componentDef,f,s,l);if(d)if(r)vs(l,d,["ng-version",fb.full]);else{const{attrs:b,classes:v}=function(t){const e=[],n=[];let r=1,i=2;for(;r<t.length;){let o=t[r];if("string"==typeof o)2===i?""!==o&&e.push(o,t[++r]):8===i&&n.push(o);else{if(!Pt(i))break;i=o}r++}return{attrs:e,classes:n}}(this.componentDef.selectors[0]);b&&vs(l,d,b),v&&v.length>0&&bc(l,d,v.join(" "))}if(g=function(t,e){return t.data[e]}(h,20),void 0!==n){const b=g.projection=[];for(let v=0;v<this.ngContentSelectors.length;v++){const S=n[v];b.push(null!=S?Array.from(S):null)}}m=function(t,e,n,r,i){const o=n[1],s=function(t,e,n){const r=Te();t.firstCreatePass&&(n.providersResolver&&n.providersResolver(n),qf(t,r,e,Hr(t,e,1,null),n));const i=zi(e,t,r.directiveStart,r);Le(i,e);const o=bt(r,e);return o&&Le(o,e),i}(o,n,e);if(r.components.push(s),t[8]=s,i&&i.forEach(l=>l(s,e)),e.contentQueries){const l=Te();e.contentQueries(1,s,l.directiveStart)}const a=Te();return!o.firstCreatePass||null===e.hostBindings&&null===e.hostAttrs||(An(a.index),Vf(n[1],a,0,a.directiveStart,a.directiveEnd,e),Bf(e,s)),s}(y,this.componentDef,f,p,[PE]),io(h,f,null)}finally{gs()}return new Lx(this.componentType,m,ci(g,f),f,g)}}class Lx extends class{}{constructor(e,n,r,i,o){super(),this.location=r,this._rootLView=i,this._tNode=o,this.instance=n,this.hostView=this.changeDetectorRef=new CS(i),this.componentType=e}get injector(){return new Pr(this._tNode,this._rootLView)}destroy(){this.hostView.destroy()}onDestroy(e){this.hostView.onDestroy(e)}}const gi=new Map;class Ux extends rn{constructor(e,n){super(),this._parent=n,this._bootstrapComponents=[],this.injector=this,this.destroyCbs=[],this.componentFactoryResolver=new Hb(this);const r=ft(e),i=function(t){return t[kw]||null}(e);i&&id(i),this._bootstrapComponents=Xt(r.bootstrap),this._r3Injector=Jf(e,n,[{provide:rn,useValue:this},{provide:lr,useValue:this.componentFactoryResolver}],z(e)),this._r3Injector._resolveInjectorDefTypes(),this.instance=this.get(e)}get(e,n=te.THROW_IF_NOT_FOUND,r=I.Default){return e===te||e===rn||e===Wr?this:this._r3Injector.get(e,n,r)}destroy(){const e=this._r3Injector;!e.destroyed&&e.destroy(),this.destroyCbs.forEach(n=>n()),this.destroyCbs=null}onDestroy(e){this.destroyCbs.push(e)}}class xd extends _b{constructor(e){super(),this.moduleType=e,null!==ft(e)&&function(t){const e=new Set;!function n(r){const i=ft(r,!0),o=i.id;null!==o&&(function(t,e,n){if(e&&e!==n)throw new Error(`Duplicate module registered for ${t} - ${z(e)} vs ${z(e.name)}`)}(o,gi.get(o),r),gi.set(o,r));const s=Xt(i.imports);for(const a of s)e.has(a)||(e.add(a),n(a))}(t)}(e)}create(e){return new Ux(this.moduleType,e)}}function Md(t){return e=>{setTimeout(t,void 0,e)}}const Dt=class extends ln{constructor(e=!1){super(),this.__isAsync=e}emit(e){super.next(e)}subscribe(e,n,r){var i,o,s;let a=e,l=n||(()=>null),c=r;if(e&&"object"==typeof e){const u=e;a=null===(i=u.next)||void 0===i?void 0:i.bind(u),l=null===(o=u.error)||void 0===o?void 0:o.bind(u),c=null===(s=u.complete)||void 0===s?void 0:s.bind(u)}this.__isAsync&&(l=Md(l),a&&(a=Md(a)),c&&(c=Md(c)));const d=super.subscribe({next:a,error:l,complete:c});return e instanceof fe&&e.add(d),d}};Symbol;const Io=new Y("Application Initializer");let bi=(()=>{class t{constructor(n){this.appInits=n,this.resolve=ta,this.reject=ta,this.initialized=!1,this.done=!1,this.donePromise=new Promise((r,i)=>{this.resolve=r,this.reject=i})}runInitializers(){if(this.initialized)return;const n=[],r=()=>{this.done=!0,this.resolve()};if(this.appInits)for(let i=0;i<this.appInits.length;i++){const o=this.appInits[i]();if(Ks(o))n.push(o);else if(Xc(o)){const s=new Promise((a,l)=>{o.subscribe({complete:a,error:l})});n.push(s)}}Promise.all(n).then(()=>{r()}).catch(i=>{this.reject(i)}),0===n.length&&r(),this.initialized=!0}}return t.\u0275fac=function(n){return new(n||t)(T(Io,8))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();const Ao=new Y("AppId"),qM={provide:Ao,useFactory:function(){return`${zd()}${zd()}${zd()}`},deps:[]};function zd(){return String.fromCharCode(97+Math.floor(25*Math.random()))}const Cy=new Y("Platform Initializer"),qd=new Y("Platform ID"),Dy=new Y("appBootstrapListener");let fa=(()=>{class t{log(n){console.log(n)}warn(n){console.warn(n)}}return t.\u0275fac=function(n){return new(n||t)},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();const jn=new Y("LocaleId"),Ey=new Y("DefaultCurrencyCode");class $M{constructor(e,n){this.ngModuleFactory=e,this.componentFactories=n}}const Hd=function(t){return new xd(t)},WM=Hd,GM=function(t){return Promise.resolve(Hd(t))},Ty=function(t){const e=Hd(t),r=Xt(ft(t).declarations).reduce((i,o)=>{const s=$e(o);return s&&i.push(new Wb(s)),i},[]);return new $M(e,r)},KM=Ty,YM=function(t){return Promise.resolve(Ty(t))};let ur=(()=>{class t{constructor(){this.compileModuleSync=WM,this.compileModuleAsync=GM,this.compileModuleAndAllComponentsSync=KM,this.compileModuleAndAllComponentsAsync=YM}clearCache(){}clearCacheFor(n){}getModuleId(n){}}return t.\u0275fac=function(n){return new(n||t)},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();const XM=(()=>Promise.resolve(0))();function $d(t){"undefined"==typeof Zone?XM.then(()=>{t&&t.apply(null,null)}):Zone.current.scheduleMicroTask("scheduleMicrotask",t)}class _e{constructor({enableLongStackTrace:e=!1,shouldCoalesceEventChangeDetection:n=!1,shouldCoalesceRunChangeDetection:r=!1}){if(this.hasPendingMacrotasks=!1,this.hasPendingMicrotasks=!1,this.isStable=!0,this.onUnstable=new Dt(!1),this.onMicrotaskEmpty=new Dt(!1),this.onStable=new Dt(!1),this.onError=new Dt(!1),"undefined"==typeof Zone)throw new Error("In this configuration Angular requires Zone.js");Zone.assertZonePatched();const i=this;i._nesting=0,i._outer=i._inner=Zone.current,Zone.TaskTrackingZoneSpec&&(i._inner=i._inner.fork(new Zone.TaskTrackingZoneSpec)),e&&Zone.longStackTraceZoneSpec&&(i._inner=i._inner.fork(Zone.longStackTraceZoneSpec)),i.shouldCoalesceEventChangeDetection=!r&&n,i.shouldCoalesceRunChangeDetection=r,i.lastRequestAnimationFrameId=-1,i.nativeRequestAnimationFrame=function(){let t=X.requestAnimationFrame,e=X.cancelAnimationFrame;if("undefined"!=typeof Zone&&t&&e){const n=t[Zone.__symbol__("OriginalDelegate")];n&&(t=n);const r=e[Zone.__symbol__("OriginalDelegate")];r&&(e=r)}return{nativeRequestAnimationFrame:t,nativeCancelAnimationFrame:e}}().nativeRequestAnimationFrame,function(t){const e=()=>{!function(t){t.isCheckStableRunning||-1!==t.lastRequestAnimationFrameId||(t.lastRequestAnimationFrameId=t.nativeRequestAnimationFrame.call(X,()=>{t.fakeTopEventTask||(t.fakeTopEventTask=Zone.root.scheduleEventTask("fakeTopEventTask",()=>{t.lastRequestAnimationFrameId=-1,Gd(t),t.isCheckStableRunning=!0,Wd(t),t.isCheckStableRunning=!1},void 0,()=>{},()=>{})),t.fakeTopEventTask.invoke()}),Gd(t))}(t)};t._inner=t._inner.fork({name:"angular",properties:{isAngularZone:!0},onInvokeTask:(n,r,i,o,s,a)=>{try{return Sy(t),n.invokeTask(i,o,s,a)}finally{(t.shouldCoalesceEventChangeDetection&&"eventTask"===o.type||t.shouldCoalesceRunChangeDetection)&&e(),xy(t)}},onInvoke:(n,r,i,o,s,a,l)=>{try{return Sy(t),n.invoke(i,o,s,a,l)}finally{t.shouldCoalesceRunChangeDetection&&e(),xy(t)}},onHasTask:(n,r,i,o)=>{n.hasTask(i,o),r===i&&("microTask"==o.change?(t._hasPendingMicrotasks=o.microTask,Gd(t),Wd(t)):"macroTask"==o.change&&(t.hasPendingMacrotasks=o.macroTask))},onHandleError:(n,r,i,o)=>(n.handleError(i,o),t.runOutsideAngular(()=>t.onError.emit(o)),!1)})}(i)}static isInAngularZone(){return!0===Zone.current.get("isAngularZone")}static assertInAngularZone(){if(!_e.isInAngularZone())throw new Error("Expected to be in Angular Zone, but it is not!")}static assertNotInAngularZone(){if(_e.isInAngularZone())throw new Error("Expected to not be in Angular Zone, but it is!")}run(e,n,r){return this._inner.run(e,n,r)}runTask(e,n,r,i){const o=this._inner,s=o.scheduleEventTask("NgZoneEvent: "+i,e,eI,ta,ta);try{return o.runTask(s,n,r)}finally{o.cancelTask(s)}}runGuarded(e,n,r){return this._inner.runGuarded(e,n,r)}runOutsideAngular(e){return this._outer.run(e)}}const eI={};function Wd(t){if(0==t._nesting&&!t.hasPendingMicrotasks&&!t.isStable)try{t._nesting++,t.onMicrotaskEmpty.emit(null)}finally{if(t._nesting--,!t.hasPendingMicrotasks)try{t.runOutsideAngular(()=>t.onStable.emit(null))}finally{t.isStable=!0}}}function Gd(t){t.hasPendingMicrotasks=!!(t._hasPendingMicrotasks||(t.shouldCoalesceEventChangeDetection||t.shouldCoalesceRunChangeDetection)&&-1!==t.lastRequestAnimationFrameId)}function Sy(t){t._nesting++,t.isStable&&(t.isStable=!1,t.onUnstable.emit(null))}function xy(t){t._nesting--,Wd(t)}class rI{constructor(){this.hasPendingMicrotasks=!1,this.hasPendingMacrotasks=!1,this.isStable=!0,this.onUnstable=new Dt,this.onMicrotaskEmpty=new Dt,this.onStable=new Dt,this.onError=new Dt}run(e,n,r){return e.apply(n,r)}runGuarded(e,n,r){return e.apply(n,r)}runOutsideAngular(e){return e()}runTask(e,n,r,i){return e.apply(n,r)}}let Kd=(()=>{class t{constructor(n){this._ngZone=n,this._pendingCount=0,this._isZoneStable=!0,this._didWork=!1,this._callbacks=[],this.taskTrackingZone=null,this._watchAngularEvents(),n.run(()=>{this.taskTrackingZone="undefined"==typeof Zone?null:Zone.current.get("TaskTrackingZone")})}_watchAngularEvents(){this._ngZone.onUnstable.subscribe({next:()=>{this._didWork=!0,this._isZoneStable=!1}}),this._ngZone.runOutsideAngular(()=>{this._ngZone.onStable.subscribe({next:()=>{_e.assertNotInAngularZone(),$d(()=>{this._isZoneStable=!0,this._runCallbacksIfReady()})}})})}increasePendingRequestCount(){return this._pendingCount+=1,this._didWork=!0,this._pendingCount}decreasePendingRequestCount(){if(this._pendingCount-=1,this._pendingCount<0)throw new Error("pending async requests below zero");return this._runCallbacksIfReady(),this._pendingCount}isStable(){return this._isZoneStable&&0===this._pendingCount&&!this._ngZone.hasPendingMacrotasks}_runCallbacksIfReady(){if(this.isStable())$d(()=>{for(;0!==this._callbacks.length;){let n=this._callbacks.pop();clearTimeout(n.timeoutId),n.doneCb(this._didWork)}this._didWork=!1});else{let n=this.getPendingTasks();this._callbacks=this._callbacks.filter(r=>!r.updateCb||!r.updateCb(n)||(clearTimeout(r.timeoutId),!1)),this._didWork=!0}}getPendingTasks(){return this.taskTrackingZone?this.taskTrackingZone.macroTasks.map(n=>({source:n.source,creationLocation:n.creationLocation,data:n.data})):[]}addCallback(n,r,i){let o=-1;r&&r>0&&(o=setTimeout(()=>{this._callbacks=this._callbacks.filter(s=>s.timeoutId!==o),n(this._didWork,this.getPendingTasks())},r)),this._callbacks.push({doneCb:n,timeoutId:o,updateCb:i})}whenStable(n,r,i){if(i&&!this.taskTrackingZone)throw new Error('Task tracking zone is required when passing an update callback to whenStable(). Is "zone.js/plugins/task-tracking" loaded?');this.addCallback(n,r,i),this._runCallbacksIfReady()}getPendingRequestCount(){return this._pendingCount}findProviders(n,r,i){return[]}}return t.\u0275fac=function(n){return new(n||t)(T(_e))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})(),My=(()=>{class t{constructor(){this._applications=new Map,Yd.addToWindow(this)}registerApplication(n,r){this._applications.set(n,r)}unregisterApplication(n){this._applications.delete(n)}unregisterAllApplications(){this._applications.clear()}getTestability(n){return this._applications.get(n)||null}getAllTestabilities(){return Array.from(this._applications.values())}getAllRootElements(){return Array.from(this._applications.keys())}findTestabilityInTree(n,r=!0){return Yd.findTestabilityInTree(this,n,r)}}return t.\u0275fac=function(n){return new(n||t)},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();class iI{addToWindow(e){}findTestabilityInTree(e,n,r){return null}}let Yd=new iI,Iy=!0,Ay=!1;let Bt;const Ry=new Y("AllowMultipleToken");class Qd{constructor(e,n){this.name=e,this.token=n}}function Ny(t,e,n=[]){const r=`Platform: ${e}`,i=new Y(r);return(o=[])=>{let s=ky();if(!s||s.injector.get(Ry,!1))if(t)t(n.concat(o).concat({provide:i,useValue:!0}));else{const a=n.concat(o).concat({provide:i,useValue:!0},{provide:oo,useValue:"platform"});!function(t){if(Bt&&!Bt.destroyed&&!Bt.injector.get(Ry,!1))throw new Error("There can be only one platform. Destroy the previous one to create a new one.");Bt=t.get(Oy);const e=t.get(Cy,null);e&&e.forEach(n=>n())}(te.create({providers:a,name:r}))}return function(t){const e=ky();if(!e)throw new Error("No platform exists!");if(!e.injector.get(t,null))throw new Error("A platform with a different configuration has been created. Please destroy it first.");return e}(i)}}function ky(){return Bt&&!Bt.destroyed?Bt:null}let Oy=(()=>{class t{constructor(n){this._injector=n,this._modules=[],this._destroyListeners=[],this._destroyed=!1}bootstrapModuleFactory(n,r){const a=function(t,e){let n;return n="noop"===t?new rI:("zone.js"===t?void 0:t)||new _e({enableLongStackTrace:(Ay=!0,Iy),shouldCoalesceEventChangeDetection:!!(null==e?void 0:e.ngZoneEventCoalescing),shouldCoalesceRunChangeDetection:!!(null==e?void 0:e.ngZoneRunCoalescing)}),n}(r?r.ngZone:void 0,{ngZoneEventCoalescing:r&&r.ngZoneEventCoalescing||!1,ngZoneRunCoalescing:r&&r.ngZoneRunCoalescing||!1}),l=[{provide:_e,useValue:a}];return a.run(()=>{const c=te.create({providers:l,parent:this.injector,name:n.moduleType.name}),d=n.create(c),u=d.injector.get(ir,null);if(!u)throw new Error("No ErrorHandler. Is platform module (BrowserModule) included?");return a.runOutsideAngular(()=>{const p=a.onError.subscribe({next:h=>{u.handleError(h)}});d.onDestroy(()=>{Zd(this._modules,d),p.unsubscribe()})}),function(t,e,n){try{const r=n();return Ks(r)?r.catch(i=>{throw e.runOutsideAngular(()=>t.handleError(i)),i}):r}catch(r){throw e.runOutsideAngular(()=>t.handleError(r)),r}}(u,a,()=>{const p=d.injector.get(bi);return p.runInitializers(),p.donePromise.then(()=>(id(d.injector.get(jn,Qs)||Qs),this._moduleDoBootstrap(d),d))})})}bootstrapModule(n,r=[]){const i=Fy({},r);return function(t,e,n){const r=new xd(n);return Promise.resolve(r)}(0,0,n).then(o=>this.bootstrapModuleFactory(o,i))}_moduleDoBootstrap(n){const r=n.injector.get(yi);if(n._bootstrapComponents.length>0)n._bootstrapComponents.forEach(i=>r.bootstrap(i));else{if(!n.instance.ngDoBootstrap)throw new Error(`The module ${z(n.instance.constructor)} was bootstrapped, but it does not declare "@NgModule.bootstrap" components nor a "ngDoBootstrap" method. Please define one of these.`);n.instance.ngDoBootstrap(r)}this._modules.push(n)}onDestroy(n){this._destroyListeners.push(n)}get injector(){return this._injector}destroy(){if(this._destroyed)throw new Error("The platform has already been destroyed!");this._modules.slice().forEach(n=>n.destroy()),this._destroyListeners.forEach(n=>n()),this._destroyed=!0}get destroyed(){return this._destroyed}}return t.\u0275fac=function(n){return new(n||t)(T(te))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();function Fy(t,e){return Array.isArray(e)?e.reduce(Fy,t):Object.assign(Object.assign({},t),e)}let yi=(()=>{class t{constructor(n,r,i,o,s){this._zone=n,this._injector=r,this._exceptionHandler=i,this._componentFactoryResolver=o,this._initStatus=s,this._bootstrapListeners=[],this._views=[],this._runningTick=!1,this._stable=!0,this.componentTypes=[],this.components=[],this._onMicrotaskEmptySubscription=this._zone.onMicrotaskEmpty.subscribe({next:()=>{this._zone.run(()=>{this.tick()})}});const a=new ue(c=>{this._stable=this._zone.isStable&&!this._zone.hasPendingMacrotasks&&!this._zone.hasPendingMicrotasks,this._zone.runOutsideAngular(()=>{c.next(this._stable),c.complete()})}),l=new ue(c=>{let d;this._zone.runOutsideAngular(()=>{d=this._zone.onStable.subscribe(()=>{_e.assertNotInAngularZone(),$d(()=>{!this._stable&&!this._zone.hasPendingMacrotasks&&!this._zone.hasPendingMicrotasks&&(this._stable=!0,c.next(!0))})})});const u=this._zone.onUnstable.subscribe(()=>{_e.assertInAngularZone(),this._stable&&(this._stable=!1,this._zone.runOutsideAngular(()=>{c.next(!1)}))});return()=>{d.unsubscribe(),u.unsubscribe()}});this.isStable=function(...t){let e=Number.POSITIVE_INFINITY,n=null,r=t[t.length-1];return ts(r)?(n=t.pop(),t.length>1&&"number"==typeof t[t.length-1]&&(e=t.pop())):"number"==typeof r&&(e=t.pop()),null===n&&1===t.length&&t[0]instanceof ue?t[0]:Ni(e)(ul(t,n))}(a,l.pipe(t=>pl()(function(t,e){return function(r){let i;i="function"==typeof t?t:function(){return t};const o=Object.create(r,fw);return o.source=r,o.subjectFactory=i,o}}(vw)(t))))}bootstrap(n,r){if(!this._initStatus.done)throw new Error("Cannot bootstrap as there are still asynchronous initializers running. Bootstrap components in the `ngDoBootstrap` method of the root module.");let i;i=n instanceof db?n:this._componentFactoryResolver.resolveComponentFactory(n),this.componentTypes.push(i.componentType);const o=function(t){return t.isBoundToModule}(i)?void 0:this._injector.get(rn),a=i.create(te.NULL,[],r||i.selector,o),l=a.location.nativeElement,c=a.injector.get(Kd,null),d=c&&a.injector.get(My);return c&&d&&d.registerApplication(l,c),a.onDestroy(()=>{this.detachView(a.hostView),Zd(this.components,a),d&&d.unregisterApplication(l)}),this._loadComponent(a),a}tick(){if(this._runningTick)throw new Error("ApplicationRef.tick is called recursively");try{this._runningTick=!0;for(let n of this._views)n.detectChanges()}catch(n){this._zone.runOutsideAngular(()=>this._exceptionHandler.handleError(n))}finally{this._runningTick=!1}}attachView(n){const r=n;this._views.push(r),r.attachToAppRef(this)}detachView(n){const r=n;Zd(this._views,r),r.detachFromAppRef()}_loadComponent(n){this.attachView(n.hostView),this.tick(),this.components.push(n),this._injector.get(Dy,[]).concat(this._bootstrapListeners).forEach(i=>i(n))}ngOnDestroy(){this._views.slice().forEach(n=>n.destroy()),this._onMicrotaskEmptySubscription.unsubscribe()}get viewCount(){return this._views.length}}return t.\u0275fac=function(n){return new(n||t)(T(_e),T(te),T(ir),T(lr),T(bi))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();function Zd(t,e){const n=t.indexOf(e);n>-1&&t.splice(n,1)}class ma{}class vI{}const wI={factoryPathPrefix:"",factoryPathSuffix:".ngfactory"};let _I=(()=>{class t{constructor(n,r){this._compiler=n,this._config=r||wI}load(n){return this.loadAndCompile(n)}loadAndCompile(n){let[r,i]=n.split("#");return void 0===i&&(i="default"),Pi(255)(r).then(o=>o[i]).then(o=>By(o,r,i)).then(o=>this._compiler.compileModuleAsync(o))}loadFactory(n){let[r,i]=n.split("#"),o="NgFactory";return void 0===i&&(i="default",o=""),Pi(255)(this._config.factoryPathPrefix+r+this._config.factoryPathSuffix).then(s=>s[i+o]).then(s=>By(s,r,i))}}return t.\u0275fac=function(n){return new(n||t)(T(ur),T(vI,8))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();function By(t,e,n){if(!t)throw new Error(`Cannot find '${n}' in '${e}'`);return t}const PI=Ny(null,"core",[{provide:qd,useValue:"unknown"},{provide:Oy,deps:[te]},{provide:My,deps:[]},{provide:fa,deps:[]}]),FI=[{provide:yi,useClass:yi,deps:[_e,te,ir,lr,bi]},{provide:Ox,deps:[_e],useFactory:function(t){let e=[];return t.onStable.subscribe(()=>{for(;e.length;)e.pop()()}),function(n){e.push(n)}}},{provide:bi,useClass:bi,deps:[[new et,Io]]},{provide:ur,useClass:ur,deps:[]},qM,{provide:mo,useFactory:function(){return IS},deps:[]},{provide:di,useFactory:function(){return AS},deps:[]},{provide:jn,useFactory:function(t){return id(t=t||"undefined"!=typeof $localize&&$localize.locale||Qs),t},deps:[[new Lr(jn),new et,new Nn]]},{provide:Ey,useValue:"USD"}];let LI=(()=>{class t{constructor(n){}}return t.\u0275fac=function(n){return new(n||t)(T(yi))},t.\u0275mod=Gn({type:t}),t.\u0275inj=dn({providers:FI}),t})(),Sa=null;function Vn(){return Sa}const ke=new Y("DocumentToken");let fr=(()=>{class t{historyGo(n){throw new Error("Not implemented")}}return t.\u0275fac=function(n){return new(n||t)},t.\u0275prov=q({factory:jA,token:t,providedIn:"platform"}),t})();function jA(){return T(dv)}const LA=new Y("Location Initialized");let dv=(()=>{class t extends fr{constructor(n){super(),this._doc=n,this._init()}_init(){this.location=window.location,this._history=window.history}getBaseHrefFromDOM(){return Vn().getBaseHref(this._doc)}onPopState(n){const r=Vn().getGlobalEventTarget(this._doc,"window");return r.addEventListener("popstate",n,!1),()=>r.removeEventListener("popstate",n)}onHashChange(n){const r=Vn().getGlobalEventTarget(this._doc,"window");return r.addEventListener("hashchange",n,!1),()=>r.removeEventListener("hashchange",n)}get href(){return this.location.href}get protocol(){return this.location.protocol}get hostname(){return this.location.hostname}get port(){return this.location.port}get pathname(){return this.location.pathname}get search(){return this.location.search}get hash(){return this.location.hash}set pathname(n){this.location.pathname=n}pushState(n,r,i){uv()?this._history.pushState(n,r,i):this.location.hash=i}replaceState(n,r,i){uv()?this._history.replaceState(n,r,i):this.location.hash=i}forward(){this._history.forward()}back(){this._history.back()}historyGo(n=0){this._history.go(n)}getState(){return this._history.state}}return t.\u0275fac=function(n){return new(n||t)(T(ke))},t.\u0275prov=q({factory:VA,token:t,providedIn:"platform"}),t})();function uv(){return!!window.history.pushState}function VA(){return new dv(T(ke))}function pu(t,e){if(0==t.length)return e;if(0==e.length)return t;let n=0;return t.endsWith("/")&&n++,e.startsWith("/")&&n++,2==n?t+e.substring(1):1==n?t+e:t+"/"+e}function pv(t){const e=t.match(/#|\?|$/),n=e&&e.index||t.length;return t.slice(0,n-("/"===t[n-1]?1:0))+t.slice(n)}function wn(t){return t&&"?"!==t[0]?"?"+t:t}let _i=(()=>{class t{historyGo(n){throw new Error("Not implemented")}}return t.\u0275fac=function(n){return new(n||t)},t.\u0275prov=q({factory:BA,token:t,providedIn:"root"}),t})();function BA(t){const e=T(ke).location;return new hv(T(fr),e&&e.origin||"")}const hu=new Y("appBaseHref");let hv=(()=>{class t extends _i{constructor(n,r){if(super(),this._platformLocation=n,this._removeListenerFns=[],null==r&&(r=this._platformLocation.getBaseHrefFromDOM()),null==r)throw new Error("No base href set. Please provide a value for the APP_BASE_HREF token or add a base element to the document.");this._baseHref=r}ngOnDestroy(){for(;this._removeListenerFns.length;)this._removeListenerFns.pop()()}onPopState(n){this._removeListenerFns.push(this._platformLocation.onPopState(n),this._platformLocation.onHashChange(n))}getBaseHref(){return this._baseHref}prepareExternalUrl(n){return pu(this._baseHref,n)}path(n=!1){const r=this._platformLocation.pathname+wn(this._platformLocation.search),i=this._platformLocation.hash;return i&&n?`${r}${i}`:r}pushState(n,r,i,o){const s=this.prepareExternalUrl(i+wn(o));this._platformLocation.pushState(n,r,s)}replaceState(n,r,i,o){const s=this.prepareExternalUrl(i+wn(o));this._platformLocation.replaceState(n,r,s)}forward(){this._platformLocation.forward()}back(){this._platformLocation.back()}historyGo(n=0){var r,i;null===(i=(r=this._platformLocation).historyGo)||void 0===i||i.call(r,n)}}return t.\u0275fac=function(n){return new(n||t)(T(fr),T(hu,8))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})(),UA=(()=>{class t extends _i{constructor(n,r){super(),this._platformLocation=n,this._baseHref="",this._removeListenerFns=[],null!=r&&(this._baseHref=r)}ngOnDestroy(){for(;this._removeListenerFns.length;)this._removeListenerFns.pop()()}onPopState(n){this._removeListenerFns.push(this._platformLocation.onPopState(n),this._platformLocation.onHashChange(n))}getBaseHref(){return this._baseHref}path(n=!1){let r=this._platformLocation.hash;return null==r&&(r="#"),r.length>0?r.substring(1):r}prepareExternalUrl(n){const r=pu(this._baseHref,n);return r.length>0?"#"+r:r}pushState(n,r,i,o){let s=this.prepareExternalUrl(i+wn(o));0==s.length&&(s=this._platformLocation.pathname),this._platformLocation.pushState(n,r,s)}replaceState(n,r,i,o){let s=this.prepareExternalUrl(i+wn(o));0==s.length&&(s=this._platformLocation.pathname),this._platformLocation.replaceState(n,r,s)}forward(){this._platformLocation.forward()}back(){this._platformLocation.back()}historyGo(n=0){var r,i;null===(i=(r=this._platformLocation).historyGo)||void 0===i||i.call(r,n)}}return t.\u0275fac=function(n){return new(n||t)(T(fr),T(hu,8))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})(),xa=(()=>{class t{constructor(n,r){this._subject=new Dt,this._urlChangeListeners=[],this._platformStrategy=n;const i=this._platformStrategy.getBaseHref();this._platformLocation=r,this._baseHref=pv(fv(i)),this._platformStrategy.onPopState(o=>{this._subject.emit({url:this.path(!0),pop:!0,state:o.state,type:o.type})})}path(n=!1){return this.normalize(this._platformStrategy.path(n))}getState(){return this._platformLocation.getState()}isCurrentPathEqualTo(n,r=""){return this.path()==this.normalize(n+wn(r))}normalize(n){return t.stripTrailingSlash(function(t,e){return t&&e.startsWith(t)?e.substring(t.length):e}(this._baseHref,fv(n)))}prepareExternalUrl(n){return n&&"/"!==n[0]&&(n="/"+n),this._platformStrategy.prepareExternalUrl(n)}go(n,r="",i=null){this._platformStrategy.pushState(i,"",n,r),this._notifyUrlChangeListeners(this.prepareExternalUrl(n+wn(r)),i)}replaceState(n,r="",i=null){this._platformStrategy.replaceState(i,"",n,r),this._notifyUrlChangeListeners(this.prepareExternalUrl(n+wn(r)),i)}forward(){this._platformStrategy.forward()}back(){this._platformStrategy.back()}historyGo(n=0){var r,i;null===(i=(r=this._platformStrategy).historyGo)||void 0===i||i.call(r,n)}onUrlChange(n){this._urlChangeListeners.push(n),this._urlChangeSubscription||(this._urlChangeSubscription=this.subscribe(r=>{this._notifyUrlChangeListeners(r.url,r.state)}))}_notifyUrlChangeListeners(n="",r){this._urlChangeListeners.forEach(i=>i(n,r))}subscribe(n,r,i){return this._subject.subscribe({next:n,error:r,complete:i})}}return t.\u0275fac=function(n){return new(n||t)(T(_i),T(fr))},t.normalizeQueryParams=wn,t.joinWithSlash=pu,t.stripTrailingSlash=pv,t.\u0275prov=q({factory:zA,token:t,providedIn:"root"}),t})();function zA(){return new xa(T(_i),T(fr))}function fv(t){return t.replace(/\/index.html$/,"")}var Ce=(()=>((Ce=Ce||{})[Ce.Zero=0]="Zero",Ce[Ce.One=1]="One",Ce[Ce.Two=2]="Two",Ce[Ce.Few=3]="Few",Ce[Ce.Many=4]="Many",Ce[Ce.Other=5]="Other",Ce))();const QA=function(t){return function(t){const e=function(t){return t.toLowerCase().replace(/_/g,"-")}(t);let n=Lm(e);if(n)return n;const r=e.split("-")[0];if(n=Lm(r),n)return n;if("en"===r)return sT;throw new Error(`Missing locale data for the locale "${t}".`)}(t)[E.PluralCase]};class ja{}let TP=(()=>{class t extends ja{constructor(n){super(),this.locale=n}getPluralCategory(n,r){switch(QA(r||this.locale)(n)){case Ce.Zero:return"zero";case Ce.One:return"one";case Ce.Two:return"two";case Ce.Few:return"few";case Ce.Many:return"many";default:return"other"}}}return t.\u0275fac=function(n){return new(n||t)(T(jn))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})(),oR=(()=>{class t{}return t.\u0275fac=function(n){return new(n||t)},t.\u0275mod=Gn({type:t}),t.\u0275inj=dn({providers:[{provide:ja,useClass:TP}]}),t})();let Mv=(()=>{class t{}return t.\u0275prov=q({token:t,providedIn:"root",factory:()=>new cR(T(ke),window)}),t})();class cR{constructor(e,n){this.document=e,this.window=n,this.offset=()=>[0,0]}setOffset(e){this.offset=Array.isArray(e)?()=>e:e}getScrollPosition(){return this.supportsScrolling()?[this.window.pageXOffset,this.window.pageYOffset]:[0,0]}scrollToPosition(e){this.supportsScrolling()&&this.window.scrollTo(e[0],e[1])}scrollToAnchor(e){if(!this.supportsScrolling())return;const n=function(t,e){const n=t.getElementById(e)||t.getElementsByName(e)[0];if(n)return n;if("function"==typeof t.createTreeWalker&&t.body&&(t.body.createShadowRoot||t.body.attachShadow)){const r=t.createTreeWalker(t.body,NodeFilter.SHOW_ELEMENT);let i=r.currentNode;for(;i;){const o=i.shadowRoot;if(o){const s=o.getElementById(e)||o.querySelector(`[name="${e}"]`);if(s)return s}i=r.nextNode()}}return null}(this.document,e);n&&(this.scrollToElement(n),this.attemptFocus(n))}setHistoryScrollRestoration(e){if(this.supportScrollRestoration()){const n=this.window.history;n&&n.scrollRestoration&&(n.scrollRestoration=e)}}scrollToElement(e){const n=e.getBoundingClientRect(),r=n.left+this.window.pageXOffset,i=n.top+this.window.pageYOffset,o=this.offset();this.window.scrollTo(r-o[0],i-o[1])}attemptFocus(e){return e.focus(),this.document.activeElement===e}supportScrollRestoration(){try{if(!this.supportsScrolling())return!1;const e=Iv(this.window.history)||Iv(Object.getPrototypeOf(this.window.history));return!(!e||!e.writable&&!e.set)}catch(e){return!1}}supportsScrolling(){try{return!!this.window&&!!this.window.scrollTo&&"pageXOffset"in this.window}catch(e){return!1}}}function Iv(t){return Object.getOwnPropertyDescriptor(t,"scrollRestoration")}class Su extends class extends class{}{constructor(){super(...arguments),this.supportsDOMEvents=!0}}{static makeCurrent(){!function(t){Sa||(Sa=t)}(new Su)}onAndCancel(e,n,r){return e.addEventListener(n,r,!1),()=>{e.removeEventListener(n,r,!1)}}dispatchEvent(e,n){e.dispatchEvent(n)}remove(e){e.parentNode&&e.parentNode.removeChild(e)}createElement(e,n){return(n=n||this.getDefaultDocument()).createElement(e)}createHtmlDocument(){return document.implementation.createHTMLDocument("fakeTitle")}getDefaultDocument(){return document}isElementNode(e){return e.nodeType===Node.ELEMENT_NODE}isShadowRoot(e){return e instanceof DocumentFragment}getGlobalEventTarget(e,n){return"window"===n?window:"document"===n?e:"body"===n?e.body:null}getBaseHref(e){const n=(jo=jo||document.querySelector("base"),jo?jo.getAttribute("href"):null);return null==n?null:function(t){La=La||document.createElement("a"),La.setAttribute("href",t);const e=La.pathname;return"/"===e.charAt(0)?e:`/${e}`}(n)}resetBaseElement(){jo=null}getUserAgent(){return window.navigator.userAgent}getCookie(e){return function(t,e){e=encodeURIComponent(e);for(const n of t.split(";")){const r=n.indexOf("="),[i,o]=-1==r?[n,""]:[n.slice(0,r),n.slice(r+1)];if(i.trim()===e)return decodeURIComponent(o)}return null}(document.cookie,e)}}let La,jo=null;const Av=new Y("TRANSITION_ID"),mR=[{provide:Io,useFactory:function(t,e,n){return()=>{n.get(bi).donePromise.then(()=>{const r=Vn(),i=e.querySelectorAll(`style[ng-transition="${t}"]`);for(let o=0;o<i.length;o++)r.remove(i[o])})}},deps:[Av,ke,te],multi:!0}];class xu{static init(){!function(t){Yd=t}(new xu)}addToWindow(e){X.getAngularTestability=(r,i=!0)=>{const o=e.findTestabilityInTree(r,i);if(null==o)throw new Error("Could not find testability for element.");return o},X.getAllAngularTestabilities=()=>e.getAllTestabilities(),X.getAllAngularRootElements=()=>e.getAllRootElements(),X.frameworkStabilizers||(X.frameworkStabilizers=[]),X.frameworkStabilizers.push(r=>{const i=X.getAllAngularTestabilities();let o=i.length,s=!1;const a=function(l){s=s||l,o--,0==o&&r(s)};i.forEach(function(l){l.whenStable(a)})})}findTestabilityInTree(e,n,r){if(null==n)return null;const i=e.getTestability(n);return null!=i?i:r?Vn().isShadowRoot(n)?this.findTestabilityInTree(e,n.host,!0):this.findTestabilityInTree(e,n.parentElement,!0):null}}let bR=(()=>{class t{build(){return new XMLHttpRequest}}return t.\u0275fac=function(n){return new(n||t)},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();const Lo=new Y("EventManagerPlugins");let Ba=(()=>{class t{constructor(n,r){this._zone=r,this._eventNameToPlugin=new Map,n.forEach(i=>i.manager=this),this._plugins=n.slice().reverse()}addEventListener(n,r,i){return this._findPluginFor(r).addEventListener(n,r,i)}addGlobalEventListener(n,r,i){return this._findPluginFor(r).addGlobalEventListener(n,r,i)}getZone(){return this._zone}_findPluginFor(n){const r=this._eventNameToPlugin.get(n);if(r)return r;const i=this._plugins;for(let o=0;o<i.length;o++){const s=i[o];if(s.supports(n))return this._eventNameToPlugin.set(n,s),s}throw new Error(`No event manager plugin found for event ${n}`)}}return t.\u0275fac=function(n){return new(n||t)(T(Lo),T(_e))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();class Mu{constructor(e){this._doc=e}addGlobalEventListener(e,n,r){const i=Vn().getGlobalEventTarget(this._doc,e);if(!i)throw new Error(`Unsupported event target ${i} for event ${n}`);return this.addEventListener(i,n,r)}}let Rv=(()=>{class t{constructor(){this._stylesSet=new Set}addStyles(n){const r=new Set;n.forEach(i=>{this._stylesSet.has(i)||(this._stylesSet.add(i),r.add(i))}),this.onStylesAdded(r)}onStylesAdded(n){}getAllStyles(){return Array.from(this._stylesSet)}}return t.\u0275fac=function(n){return new(n||t)},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})(),Vo=(()=>{class t extends Rv{constructor(n){super(),this._doc=n,this._hostNodes=new Map,this._hostNodes.set(n.head,[])}_addStylesToHost(n,r,i){n.forEach(o=>{const s=this._doc.createElement("style");s.textContent=o,i.push(r.appendChild(s))})}addHost(n){const r=[];this._addStylesToHost(this._stylesSet,n,r),this._hostNodes.set(n,r)}removeHost(n){const r=this._hostNodes.get(n);r&&r.forEach(Nv),this._hostNodes.delete(n)}onStylesAdded(n){this._hostNodes.forEach((r,i)=>{this._addStylesToHost(n,i,r)})}ngOnDestroy(){this._hostNodes.forEach(n=>n.forEach(Nv))}}return t.\u0275fac=function(n){return new(n||t)(T(ke))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();function Nv(t){Vn().remove(t)}const Iu={svg:"http://www.w3.org/2000/svg",xhtml:"http://www.w3.org/1999/xhtml",xlink:"http://www.w3.org/1999/xlink",xml:"http://www.w3.org/XML/1998/namespace",xmlns:"http://www.w3.org/2000/xmlns/"},Au=/%COMP%/g;function Ua(t,e,n){for(let r=0;r<e.length;r++){let i=e[r];Array.isArray(i)?Ua(t,i,n):(i=i.replace(Au,t),n.push(i))}return n}function Fv(t){return e=>{if("__ngUnwrap__"===e)return t;!1===t(e)&&(e.preventDefault(),e.returnValue=!1)}}let Pu=(()=>{class t{constructor(n,r,i){this.eventManager=n,this.sharedStylesHost=r,this.appId=i,this.rendererByCompId=new Map,this.defaultRenderer=new Ru(n)}createRenderer(n,r){if(!n||!r)return this.defaultRenderer;switch(r.encapsulation){case Ee.Emulated:{let i=this.rendererByCompId.get(r.id);return i||(i=new RR(this.eventManager,this.sharedStylesHost,r,this.appId),this.rendererByCompId.set(r.id,i)),i.applyToHost(n),i}case 1:case Ee.ShadowDom:return new NR(this.eventManager,this.sharedStylesHost,n,r);default:if(!this.rendererByCompId.has(r.id)){const i=Ua(r.id,r.styles,[]);this.sharedStylesHost.addStyles(i),this.rendererByCompId.set(r.id,this.defaultRenderer)}return this.defaultRenderer}}begin(){}end(){}}return t.\u0275fac=function(n){return new(n||t)(T(Ba),T(Vo),T(Ao))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();class Ru{constructor(e){this.eventManager=e,this.data=Object.create(null)}destroy(){}createElement(e,n){return n?document.createElementNS(Iu[n]||n,e):document.createElement(e)}createComment(e){return document.createComment(e)}createText(e){return document.createTextNode(e)}appendChild(e,n){e.appendChild(n)}insertBefore(e,n,r){e&&e.insertBefore(n,r)}removeChild(e,n){e&&e.removeChild(n)}selectRootElement(e,n){let r="string"==typeof e?document.querySelector(e):e;if(!r)throw new Error(`The selector "${e}" did not match any elements`);return n||(r.textContent=""),r}parentNode(e){return e.parentNode}nextSibling(e){return e.nextSibling}setAttribute(e,n,r,i){if(i){n=i+":"+n;const o=Iu[i];o?e.setAttributeNS(o,n,r):e.setAttribute(n,r)}else e.setAttribute(n,r)}removeAttribute(e,n,r){if(r){const i=Iu[r];i?e.removeAttributeNS(i,n):e.removeAttribute(`${r}:${n}`)}else e.removeAttribute(n)}addClass(e,n){e.classList.add(n)}removeClass(e,n){e.classList.remove(n)}setStyle(e,n,r,i){i&(lt.DashCase|lt.Important)?e.style.setProperty(n,r,i&lt.Important?"important":""):e.style[n]=r}removeStyle(e,n,r){r&lt.DashCase?e.style.removeProperty(n):e.style[n]=""}setProperty(e,n,r){e[n]=r}setValue(e,n){e.nodeValue=n}listen(e,n,r){return"string"==typeof e?this.eventManager.addGlobalEventListener(e,n,Fv(r)):this.eventManager.addEventListener(e,n,Fv(r))}}class RR extends Ru{constructor(e,n,r,i){super(e),this.component=r;const o=Ua(i+"-"+r.id,r.styles,[]);n.addStyles(o),this.contentAttr=function(t){return"_ngcontent-%COMP%".replace(Au,t)}(i+"-"+r.id),this.hostAttr=function(t){return"_nghost-%COMP%".replace(Au,t)}(i+"-"+r.id)}applyToHost(e){super.setAttribute(e,this.hostAttr,"")}createElement(e,n){const r=super.createElement(e,n);return super.setAttribute(r,this.contentAttr,""),r}}class NR extends Ru{constructor(e,n,r,i){super(e),this.sharedStylesHost=n,this.hostEl=r,this.shadowRoot=r.attachShadow({mode:"open"}),this.sharedStylesHost.addHost(this.shadowRoot);const o=Ua(i.id,i.styles,[]);for(let s=0;s<o.length;s++){const a=document.createElement("style");a.textContent=o[s],this.shadowRoot.appendChild(a)}}nodeOrShadowRoot(e){return e===this.hostEl?this.shadowRoot:e}destroy(){this.sharedStylesHost.removeHost(this.shadowRoot)}appendChild(e,n){return super.appendChild(this.nodeOrShadowRoot(e),n)}insertBefore(e,n,r){return super.insertBefore(this.nodeOrShadowRoot(e),n,r)}removeChild(e,n){return super.removeChild(this.nodeOrShadowRoot(e),n)}parentNode(e){return this.nodeOrShadowRoot(super.parentNode(this.nodeOrShadowRoot(e)))}}let kR=(()=>{class t extends Mu{constructor(n){super(n)}supports(n){return!0}addEventListener(n,r,i){return n.addEventListener(r,i,!1),()=>this.removeEventListener(n,r,i)}removeEventListener(n,r,i){return n.removeEventListener(r,i)}}return t.\u0275fac=function(n){return new(n||t)(T(ke))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();const Vv=["alt","control","meta","shift"],UR={"\b":"Backspace","\t":"Tab","\x7f":"Delete","\x1b":"Escape",Del:"Delete",Esc:"Escape",Left:"ArrowLeft",Right:"ArrowRight",Up:"ArrowUp",Down:"ArrowDown",Menu:"ContextMenu",Scroll:"ScrollLock",Win:"OS"},Bv={A:"1",B:"2",C:"3",D:"4",E:"5",F:"6",G:"7",H:"8",I:"9",J:"*",K:"+",M:"-",N:".",O:"/","`":"0","\x90":"NumLock"},zR={alt:t=>t.altKey,control:t=>t.ctrlKey,meta:t=>t.metaKey,shift:t=>t.shiftKey};let qR=(()=>{class t extends Mu{constructor(n){super(n)}supports(n){return null!=t.parseEventName(n)}addEventListener(n,r,i){const o=t.parseEventName(r),s=t.eventCallback(o.fullKey,i,this.manager.getZone());return this.manager.getZone().runOutsideAngular(()=>Vn().onAndCancel(n,o.domEventName,s))}static parseEventName(n){const r=n.toLowerCase().split("."),i=r.shift();if(0===r.length||"keydown"!==i&&"keyup"!==i)return null;const o=t._normalizeKey(r.pop());let s="";if(Vv.forEach(l=>{const c=r.indexOf(l);c>-1&&(r.splice(c,1),s+=l+".")}),s+=o,0!=r.length||0===o.length)return null;const a={};return a.domEventName=i,a.fullKey=s,a}static getEventFullKey(n){let r="",i=function(t){let e=t.key;if(null==e){if(e=t.keyIdentifier,null==e)return"Unidentified";e.startsWith("U+")&&(e=String.fromCharCode(parseInt(e.substring(2),16)),3===t.location&&Bv.hasOwnProperty(e)&&(e=Bv[e]))}return UR[e]||e}(n);return i=i.toLowerCase()," "===i?i="space":"."===i&&(i="dot"),Vv.forEach(o=>{o!=i&&zR[o](n)&&(r+=o+".")}),r+=i,r}static eventCallback(n,r,i){return o=>{t.getEventFullKey(o)===n&&i.runGuarded(()=>r(o))}}static _normalizeKey(n){switch(n){case"esc":return"escape";default:return n}}}return t.\u0275fac=function(n){return new(n||t)(T(ke))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();const ZR=Ny(PI,"browser",[{provide:qd,useValue:"browser"},{provide:Cy,useValue:function(){Su.makeCurrent(),xu.init()},multi:!0},{provide:ke,useFactory:function(){return function(t){Sl=t}(document),document},deps:[]}]),XR=[[],{provide:oo,useValue:"root"},{provide:ir,useFactory:function(){return new ir},deps:[]},{provide:Lo,useClass:kR,multi:!0,deps:[ke,_e,qd]},{provide:Lo,useClass:qR,multi:!0,deps:[ke]},[],{provide:Pu,useClass:Pu,deps:[Ba,Vo,Ao]},{provide:na,useExisting:Pu},{provide:Rv,useExisting:Vo},{provide:Vo,useClass:Vo,deps:[ke]},{provide:Kd,useClass:Kd,deps:[_e]},{provide:Ba,useClass:Ba,deps:[Lo,_e]},{provide:class{},useClass:bR,deps:[]},[]];let JR=(()=>{class t{constructor(n){if(n)throw new Error("BrowserModule has already been loaded. If you need access to common directives such as NgIf and NgFor from a lazy loaded module, import CommonModule instead.")}static withServerTransition(n){return{ngModule:t,providers:[{provide:Ao,useValue:n.appId},{provide:Av,useExisting:Ao},mR]}}}return t.\u0275fac=function(n){return new(n||t)(T(t,12))},t.\u0275mod=Gn({type:t}),t.\u0275inj=dn({providers:XR,imports:[oR,LI]}),t})();function L(...t){let e=t[t.length-1];return ts(e)?(t.pop(),al(t,e)):ul(t)}"undefined"!=typeof window&&window;class $t extends ln{constructor(e){super(),this._value=e}get value(){return this.getValue()}_subscribe(e){const n=super._subscribe(e);return n&&!n.closed&&e.next(this._value),n}getValue(){if(this.hasError)throw this.thrownError;if(this.closed)throw new _r;return this._value}next(e){super.next(this._value=e)}}class aN extends de{notifyNext(e,n,r,i,o){this.destination.next(n)}notifyError(e,n){this.destination.error(e)}notifyComplete(e){this.destination.complete()}}class lN extends de{constructor(e,n,r){super(),this.parent=e,this.outerValue=n,this.outerIndex=r,this.index=0}_next(e){this.parent.notifyNext(this.outerValue,e,this.outerIndex,this.index++,this)}_error(e){this.parent.notifyError(e,this),this.unsubscribe()}_complete(){this.parent.notifyComplete(this),this.unsubscribe()}}function cN(t,e,n,r,i=new lN(t,n,r)){if(!i.closed)return e instanceof ue?e.subscribe(i):sl(e)(i)}const zv={};class uN{constructor(e){this.resultSelector=e}call(e,n){return n.subscribe(new pN(e,this.resultSelector))}}class pN extends aN{constructor(e,n){super(e),this.resultSelector=n,this.active=0,this.values=[],this.observables=[]}_next(e){this.values.push(zv),this.observables.push(e)}_complete(){const e=this.observables,n=e.length;if(0===n)this.destination.complete();else{this.active=n,this.toRespond=n;for(let r=0;r<n;r++)this.add(cN(this,e[r],void 0,r))}}notifyComplete(e){0==(this.active-=1)&&this.destination.complete()}notifyNext(e,n,r){const i=this.values,s=this.toRespond?i[r]===zv?--this.toRespond:this.toRespond:0;i[r]=n,0===s&&(this.resultSelector?this._tryResultSelector(i):this.destination.next(i.slice()))}_tryResultSelector(e){let n;try{n=this.resultSelector.apply(this,e)}catch(r){return void this.destination.error(r)}this.destination.next(n)}}const za=(()=>{function t(){return Error.call(this),this.message="no elements in sequence",this.name="EmptyError",this}return t.prototype=Object.create(Error.prototype),t})();function ju(...t){return Ni(1)(L(...t))}const Ci=new ue(t=>t.complete());function Lu(t){return t?function(t){return new ue(e=>t.schedule(()=>e.complete()))}(t):Ci}function qv(t){return new ue(e=>{let n;try{n=t()}catch(i){return void e.error(i)}return(n?qe(n):Lu()).subscribe(e)})}function Un(t,e){return"function"==typeof e?n=>n.pipe(Un((r,i)=>qe(t(r,i)).pipe(oe((o,s)=>e(r,o,i,s))))):n=>n.lift(new gN(t))}class gN{constructor(e){this.project=e}call(e,n){return n.subscribe(new mN(e,this.project))}}class mN extends cl{constructor(e,n){super(e),this.project=n,this.index=0}_next(e){let n;const r=this.index++;try{n=this.project(e,r)}catch(i){return void this.destination.error(i)}this._innerSub(n)}_innerSub(e){const n=this.innerSubscription;n&&n.unsubscribe();const r=new ll(this),i=this.destination;i.add(r),this.innerSubscription=dl(e,r),this.innerSubscription!==r&&i.add(this.innerSubscription)}_complete(){const{innerSubscription:e}=this;(!e||e.closed)&&super._complete(),this.unsubscribe()}_unsubscribe(){this.innerSubscription=void 0}notifyComplete(){this.innerSubscription=void 0,this.isStopped&&super._complete()}notifyNext(e){this.destination.next(e)}}const Hv=(()=>{function t(){return Error.call(this),this.message="argument out of range",this.name="ArgumentOutOfRangeError",this}return t.prototype=Object.create(Error.prototype),t})();function Vu(t){return e=>0===t?Lu():e.lift(new bN(t))}class bN{constructor(e){if(this.total=e,this.total<0)throw new Hv}call(e,n){return n.subscribe(new yN(e,this.total))}}class yN extends de{constructor(e,n){super(e),this.total=n,this.count=0}_next(e){const n=this.total,r=++this.count;r<=n&&(this.destination.next(e),r===n&&(this.destination.complete(),this.unsubscribe()))}}function $v(t,e){let n=!1;return arguments.length>=2&&(n=!0),function(i){return i.lift(new wN(t,e,n))}}class wN{constructor(e,n,r=!1){this.accumulator=e,this.seed=n,this.hasSeed=r}call(e,n){return n.subscribe(new _N(e,this.accumulator,this.seed,this.hasSeed))}}class _N extends de{constructor(e,n,r,i){super(e),this.accumulator=n,this._seed=r,this.hasSeed=i,this.index=0}get seed(){return this._seed}set seed(e){this.hasSeed=!0,this._seed=e}_next(e){if(this.hasSeed)return this._tryNext(e);this.seed=e,this.destination.next(e)}_tryNext(e){const n=this.index++;let r;try{r=this.accumulator(this.seed,e,n)}catch(i){this.destination.error(i)}this.seed=r,this.destination.next(r)}}function Di(t,e){return function(r){return r.lift(new CN(t,e))}}class CN{constructor(e,n){this.predicate=e,this.thisArg=n}call(e,n){return n.subscribe(new DN(e,this.predicate,this.thisArg))}}class DN extends de{constructor(e,n,r){super(e),this.predicate=n,this.thisArg=r,this.count=0}_next(e){let n;try{n=this.predicate.call(this.thisArg,e,this.count++)}catch(r){return void this.destination.error(r)}n&&this.destination.next(e)}}function gr(t){return function(n){const r=new EN(t),i=n.lift(r);return r.caught=i}}class EN{constructor(e){this.selector=e}call(e,n){return n.subscribe(new TN(e,this.selector,this.caught))}}class TN extends cl{constructor(e,n,r){super(e),this.selector=n,this.caught=r}error(e){if(!this.isStopped){let n;try{n=this.selector(e,this.caught)}catch(o){return void super.error(o)}this._unsubscribeAndRecycle();const r=new ll(this);this.add(r);const i=dl(n,r);i!==r&&this.add(i)}}}function Bo(t,e){return Ne(t,e,1)}function Bu(t){return function(n){return 0===t?Lu():n.lift(new SN(t))}}class SN{constructor(e){if(this.total=e,this.total<0)throw new Hv}call(e,n){return n.subscribe(new xN(e,this.total))}}class xN extends de{constructor(e,n){super(e),this.total=n,this.ring=new Array,this.count=0}_next(e){const n=this.ring,r=this.total,i=this.count++;n.length<r?n.push(e):n[i%r]=e}_complete(){const e=this.destination;let n=this.count;if(n>0){const r=this.count>=this.total?this.total:this.count,i=this.ring;for(let o=0;o<r;o++){const s=n++%r;e.next(i[s])}}e.complete()}}function Wv(t=AN){return e=>e.lift(new MN(t))}class MN{constructor(e){this.errorFactory=e}call(e,n){return n.subscribe(new IN(e,this.errorFactory))}}class IN extends de{constructor(e,n){super(e),this.errorFactory=n,this.hasValue=!1}_next(e){this.hasValue=!0,this.destination.next(e)}_complete(){if(this.hasValue)return this.destination.complete();{let e;try{e=this.errorFactory()}catch(n){e=n}this.destination.error(e)}}}function AN(){return new za}function Gv(t=null){return e=>e.lift(new PN(t))}class PN{constructor(e){this.defaultValue=e}call(e,n){return n.subscribe(new RN(e,this.defaultValue))}}class RN extends de{constructor(e,n){super(e),this.defaultValue=n,this.isEmpty=!0}_next(e){this.isEmpty=!1,this.destination.next(e)}_complete(){this.isEmpty&&this.destination.next(this.defaultValue),this.destination.complete()}}function Ei(t,e){const n=arguments.length>=2;return r=>r.pipe(t?Di((i,o)=>t(i,o,r)):es,Vu(1),n?Gv(e):Wv(()=>new za))}function zn(){}function ut(t,e,n){return function(i){return i.lift(new kN(t,e,n))}}class kN{constructor(e,n,r){this.nextOrObserver=e,this.error=n,this.complete=r}call(e,n){return n.subscribe(new ON(e,this.nextOrObserver,this.error,this.complete))}}class ON extends de{constructor(e,n,r,i){super(e),this._tapNext=zn,this._tapError=zn,this._tapComplete=zn,this._tapError=r||zn,this._tapComplete=i||zn,an(n)?(this._context=this,this._tapNext=n):n&&(this._context=n,this._tapNext=n.next||zn,this._tapError=n.error||zn,this._tapComplete=n.complete||zn)}_next(e){try{this._tapNext.call(this._context,e)}catch(n){return void this.destination.error(n)}this.destination.next(e)}_error(e){try{this._tapError.call(this._context,e)}catch(n){return void this.destination.error(n)}this.destination.error(e)}_complete(){try{this._tapComplete.call(this._context)}catch(e){return void this.destination.error(e)}return this.destination.complete()}}class jN{constructor(e){this.callback=e}call(e,n){return n.subscribe(new LN(e,this.callback))}}class LN extends de{constructor(e,n){super(e),this.add(new fe(n))}}class Cn{constructor(e,n){this.id=e,this.url=n}}class Uu extends Cn{constructor(e,n,r="imperative",i=null){super(e,n),this.navigationTrigger=r,this.restoredState=i}toString(){return`NavigationStart(id: ${this.id}, url: '${this.url}')`}}class Uo extends Cn{constructor(e,n,r){super(e,n),this.urlAfterRedirects=r}toString(){return`NavigationEnd(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}')`}}class Kv extends Cn{constructor(e,n,r){super(e,n),this.reason=r}toString(){return`NavigationCancel(id: ${this.id}, url: '${this.url}')`}}class VN extends Cn{constructor(e,n,r){super(e,n),this.error=r}toString(){return`NavigationError(id: ${this.id}, url: '${this.url}', error: ${this.error})`}}class BN extends Cn{constructor(e,n,r,i){super(e,n),this.urlAfterRedirects=r,this.state=i}toString(){return`RoutesRecognized(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state})`}}class UN extends Cn{constructor(e,n,r,i){super(e,n),this.urlAfterRedirects=r,this.state=i}toString(){return`GuardsCheckStart(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state})`}}class zN extends Cn{constructor(e,n,r,i,o){super(e,n),this.urlAfterRedirects=r,this.state=i,this.shouldActivate=o}toString(){return`GuardsCheckEnd(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state}, shouldActivate: ${this.shouldActivate})`}}class qN extends Cn{constructor(e,n,r,i){super(e,n),this.urlAfterRedirects=r,this.state=i}toString(){return`ResolveStart(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state})`}}class HN extends Cn{constructor(e,n,r,i){super(e,n),this.urlAfterRedirects=r,this.state=i}toString(){return`ResolveEnd(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state})`}}class Yv{constructor(e){this.route=e}toString(){return`RouteConfigLoadStart(path: ${this.route.path})`}}class Qv{constructor(e){this.route=e}toString(){return`RouteConfigLoadEnd(path: ${this.route.path})`}}class $N{constructor(e){this.snapshot=e}toString(){return`ChildActivationStart(path: '${this.snapshot.routeConfig&&this.snapshot.routeConfig.path||""}')`}}class WN{constructor(e){this.snapshot=e}toString(){return`ChildActivationEnd(path: '${this.snapshot.routeConfig&&this.snapshot.routeConfig.path||""}')`}}class GN{constructor(e){this.snapshot=e}toString(){return`ActivationStart(path: '${this.snapshot.routeConfig&&this.snapshot.routeConfig.path||""}')`}}class KN{constructor(e){this.snapshot=e}toString(){return`ActivationEnd(path: '${this.snapshot.routeConfig&&this.snapshot.routeConfig.path||""}')`}}class Zv{constructor(e,n,r){this.routerEvent=e,this.position=n,this.anchor=r}toString(){return`Scroll(anchor: '${this.anchor}', position: '${this.position?`${this.position[0]}, ${this.position[1]}`:null}')`}}const $="primary";class YN{constructor(e){this.params=e||{}}has(e){return Object.prototype.hasOwnProperty.call(this.params,e)}get(e){if(this.has(e)){const n=this.params[e];return Array.isArray(n)?n[0]:n}return null}getAll(e){if(this.has(e)){const n=this.params[e];return Array.isArray(n)?n:[n]}return[]}get keys(){return Object.keys(this.params)}}function Ti(t){return new YN(t)}const Xv="ngNavigationCancelingError";function zu(t){const e=Error("NavigationCancelingError: "+t);return e[Xv]=!0,e}function ZN(t,e,n){const r=n.path.split("/");if(r.length>t.length||"full"===n.pathMatch&&(e.hasChildren()||r.length<t.length))return null;const i={};for(let o=0;o<r.length;o++){const s=r[o],a=t[o];if(s.startsWith(":"))i[s.substring(1)]=a;else if(s!==a.path)return null}return{consumed:t.slice(0,r.length),posParams:i}}function on(t,e){const n=t?Object.keys(t):void 0,r=e?Object.keys(e):void 0;if(!n||!r||n.length!=r.length)return!1;let i;for(let o=0;o<n.length;o++)if(i=n[o],!Jv(t[i],e[i]))return!1;return!0}function Jv(t,e){if(Array.isArray(t)&&Array.isArray(e)){if(t.length!==e.length)return!1;const n=[...t].sort(),r=[...e].sort();return n.every((i,o)=>r[o]===i)}return t===e}function e0(t){return Array.prototype.concat.apply([],t)}function t0(t){return t.length>0?t[t.length-1]:null}function Oe(t,e){for(const n in t)t.hasOwnProperty(n)&&e(t[n],n)}function sn(t){return Xc(t)?t:Ks(t)?qe(Promise.resolve(t)):L(t)}const ek={exact:function i0(t,e,n){if(!br(t.segments,e.segments)||!qa(t.segments,e.segments,n)||t.numberOfChildren!==e.numberOfChildren)return!1;for(const r in e.children)if(!t.children[r]||!i0(t.children[r],e.children[r],n))return!1;return!0},subset:o0},n0={exact:function(t,e){return on(t,e)},subset:function(t,e){return Object.keys(e).length<=Object.keys(t).length&&Object.keys(e).every(n=>Jv(t[n],e[n]))},ignored:()=>!0};function r0(t,e,n){return ek[n.paths](t.root,e.root,n.matrixParams)&&n0[n.queryParams](t.queryParams,e.queryParams)&&!("exact"===n.fragment&&t.fragment!==e.fragment)}function o0(t,e,n){return s0(t,e,e.segments,n)}function s0(t,e,n,r){if(t.segments.length>n.length){const i=t.segments.slice(0,n.length);return!(!br(i,n)||e.hasChildren()||!qa(i,n,r))}if(t.segments.length===n.length){if(!br(t.segments,n)||!qa(t.segments,n,r))return!1;for(const i in e.children)if(!t.children[i]||!o0(t.children[i],e.children[i],r))return!1;return!0}{const i=n.slice(0,t.segments.length),o=n.slice(t.segments.length);return!!(br(t.segments,i)&&qa(t.segments,i,r)&&t.children[$])&&s0(t.children[$],e,o,r)}}function qa(t,e,n){return e.every((r,i)=>n0[n](t[i].parameters,r.parameters))}class mr{constructor(e,n,r){this.root=e,this.queryParams=n,this.fragment=r}get queryParamMap(){return this._queryParamMap||(this._queryParamMap=Ti(this.queryParams)),this._queryParamMap}toString(){return ok.serialize(this)}}class G{constructor(e,n){this.segments=e,this.children=n,this.parent=null,Oe(n,(r,i)=>r.parent=this)}hasChildren(){return this.numberOfChildren>0}get numberOfChildren(){return Object.keys(this.children).length}toString(){return Ha(this)}}class zo{constructor(e,n){this.path=e,this.parameters=n}get parameterMap(){return this._parameterMap||(this._parameterMap=Ti(this.parameters)),this._parameterMap}toString(){return d0(this)}}function br(t,e){return t.length===e.length&&t.every((n,r)=>n.path===e[r].path)}class qu{}class a0{parse(e){const n=new fk(e);return new mr(n.parseRootSegment(),n.parseQueryParams(),n.parseFragment())}serialize(e){return`${`/${qo(e.root,!0)}`}${function(t){const e=Object.keys(t).map(n=>{const r=t[n];return Array.isArray(r)?r.map(i=>`${$a(n)}=${$a(i)}`).join("&"):`${$a(n)}=${$a(r)}`}).filter(n=>!!n);return e.length?`?${e.join("&")}`:""}(e.queryParams)}${"string"==typeof e.fragment?`#${function(t){return encodeURI(t)}(e.fragment)}`:""}`}}const ok=new a0;function Ha(t){return t.segments.map(e=>d0(e)).join("/")}function qo(t,e){if(!t.hasChildren())return Ha(t);if(e){const n=t.children[$]?qo(t.children[$],!1):"",r=[];return Oe(t.children,(i,o)=>{o!==$&&r.push(`${o}:${qo(i,!1)}`)}),r.length>0?`${n}(${r.join("//")})`:n}{const n=function(t,e){let n=[];return Oe(t.children,(r,i)=>{i===$&&(n=n.concat(e(r,i)))}),Oe(t.children,(r,i)=>{i!==$&&(n=n.concat(e(r,i)))}),n}(t,(r,i)=>i===$?[qo(t.children[$],!1)]:[`${i}:${qo(r,!1)}`]);return 1===Object.keys(t.children).length&&null!=t.children[$]?`${Ha(t)}/${n[0]}`:`${Ha(t)}/(${n.join("//")})`}}function l0(t){return encodeURIComponent(t).replace(/%40/g,"@").replace(/%3A/gi,":").replace(/%24/g,"$").replace(/%2C/gi,",")}function $a(t){return l0(t).replace(/%3B/gi,";")}function Hu(t){return l0(t).replace(/\(/g,"%28").replace(/\)/g,"%29").replace(/%26/gi,"&")}function Wa(t){return decodeURIComponent(t)}function c0(t){return Wa(t.replace(/\+/g,"%20"))}function d0(t){return`${Hu(t.path)}${function(t){return Object.keys(t).map(e=>`;${Hu(e)}=${Hu(t[e])}`).join("")}(t.parameters)}`}const ck=/^[^\/()?;=#]+/;function Ga(t){const e=t.match(ck);return e?e[0]:""}const dk=/^[^=?&#]+/,pk=/^[^?&#]+/;class fk{constructor(e){this.url=e,this.remaining=e}parseRootSegment(){return this.consumeOptional("/"),""===this.remaining||this.peekStartsWith("?")||this.peekStartsWith("#")?new G([],{}):new G([],this.parseChildren())}parseQueryParams(){const e={};if(this.consumeOptional("?"))do{this.parseQueryParam(e)}while(this.consumeOptional("&"));return e}parseFragment(){return this.consumeOptional("#")?decodeURIComponent(this.remaining):null}parseChildren(){if(""===this.remaining)return{};this.consumeOptional("/");const e=[];for(this.peekStartsWith("(")||e.push(this.parseSegment());this.peekStartsWith("/")&&!this.peekStartsWith("//")&&!this.peekStartsWith("/(");)this.capture("/"),e.push(this.parseSegment());let n={};this.peekStartsWith("/(")&&(this.capture("/"),n=this.parseParens(!0));let r={};return this.peekStartsWith("(")&&(r=this.parseParens(!1)),(e.length>0||Object.keys(n).length>0)&&(r[$]=new G(e,n)),r}parseSegment(){const e=Ga(this.remaining);if(""===e&&this.peekStartsWith(";"))throw new Error(`Empty path url segment cannot have parameters: '${this.remaining}'.`);return this.capture(e),new zo(Wa(e),this.parseMatrixParams())}parseMatrixParams(){const e={};for(;this.consumeOptional(";");)this.parseParam(e);return e}parseParam(e){const n=Ga(this.remaining);if(!n)return;this.capture(n);let r="";if(this.consumeOptional("=")){const i=Ga(this.remaining);i&&(r=i,this.capture(r))}e[Wa(n)]=Wa(r)}parseQueryParam(e){const n=function(t){const e=t.match(dk);return e?e[0]:""}(this.remaining);if(!n)return;this.capture(n);let r="";if(this.consumeOptional("=")){const s=function(t){const e=t.match(pk);return e?e[0]:""}(this.remaining);s&&(r=s,this.capture(r))}const i=c0(n),o=c0(r);if(e.hasOwnProperty(i)){let s=e[i];Array.isArray(s)||(s=[s],e[i]=s),s.push(o)}else e[i]=o}parseParens(e){const n={};for(this.capture("(");!this.consumeOptional(")")&&this.remaining.length>0;){const r=Ga(this.remaining),i=this.remaining[r.length];if("/"!==i&&")"!==i&&";"!==i)throw new Error(`Cannot parse url '${this.url}'`);let o;r.indexOf(":")>-1?(o=r.substr(0,r.indexOf(":")),this.capture(o),this.capture(":")):e&&(o=$);const s=this.parseChildren();n[o]=1===Object.keys(s).length?s[$]:new G([],s),this.consumeOptional("//")}return n}peekStartsWith(e){return this.remaining.startsWith(e)}consumeOptional(e){return!!this.peekStartsWith(e)&&(this.remaining=this.remaining.substring(e.length),!0)}capture(e){if(!this.consumeOptional(e))throw new Error(`Expected "${e}".`)}}class u0{constructor(e){this._root=e}get root(){return this._root.value}parent(e){const n=this.pathFromRoot(e);return n.length>1?n[n.length-2]:null}children(e){const n=$u(e,this._root);return n?n.children.map(r=>r.value):[]}firstChild(e){const n=$u(e,this._root);return n&&n.children.length>0?n.children[0].value:null}siblings(e){const n=Wu(e,this._root);return n.length<2?[]:n[n.length-2].children.map(i=>i.value).filter(i=>i!==e)}pathFromRoot(e){return Wu(e,this._root).map(n=>n.value)}}function $u(t,e){if(t===e.value)return e;for(const n of e.children){const r=$u(t,n);if(r)return r}return null}function Wu(t,e){if(t===e.value)return[e];for(const n of e.children){const r=Wu(t,n);if(r.length)return r.unshift(e),r}return[]}class Dn{constructor(e,n){this.value=e,this.children=n}toString(){return`TreeNode(${this.value})`}}function Ho(t){const e={};return t&&t.children.forEach(n=>e[n.value.outlet]=n),e}class p0 extends u0{constructor(e,n){super(e),this.snapshot=n,Gu(this,e)}toString(){return this.snapshot.toString()}}function h0(t,e){const n=function(t,e){const s=new Ka([],{},{},"",{},$,e,null,t.root,-1,{});return new g0("",new Dn(s,[]))}(t,e),r=new $t([new zo("",{})]),i=new $t({}),o=new $t({}),s=new $t({}),a=new $t(""),l=new qn(r,i,s,a,o,$,e,n.root);return l.snapshot=n.root,new p0(new Dn(l,[]),n)}class qn{constructor(e,n,r,i,o,s,a,l){this.url=e,this.params=n,this.queryParams=r,this.fragment=i,this.data=o,this.outlet=s,this.component=a,this._futureSnapshot=l}get routeConfig(){return this._futureSnapshot.routeConfig}get root(){return this._routerState.root}get parent(){return this._routerState.parent(this)}get firstChild(){return this._routerState.firstChild(this)}get children(){return this._routerState.children(this)}get pathFromRoot(){return this._routerState.pathFromRoot(this)}get paramMap(){return this._paramMap||(this._paramMap=this.params.pipe(oe(e=>Ti(e)))),this._paramMap}get queryParamMap(){return this._queryParamMap||(this._queryParamMap=this.queryParams.pipe(oe(e=>Ti(e)))),this._queryParamMap}toString(){return this.snapshot?this.snapshot.toString():`Future(${this._futureSnapshot})`}}function f0(t,e="emptyOnly"){const n=t.pathFromRoot;let r=0;if("always"!==e)for(r=n.length-1;r>=1;){const i=n[r],o=n[r-1];if(i.routeConfig&&""===i.routeConfig.path)r--;else{if(o.component)break;r--}}return function(t){return t.reduce((e,n)=>({params:Object.assign(Object.assign({},e.params),n.params),data:Object.assign(Object.assign({},e.data),n.data),resolve:Object.assign(Object.assign({},e.resolve),n._resolvedData)}),{params:{},data:{},resolve:{}})}(n.slice(r))}class Ka{constructor(e,n,r,i,o,s,a,l,c,d,u){this.url=e,this.params=n,this.queryParams=r,this.fragment=i,this.data=o,this.outlet=s,this.component=a,this.routeConfig=l,this._urlSegment=c,this._lastPathIndex=d,this._resolve=u}get root(){return this._routerState.root}get parent(){return this._routerState.parent(this)}get firstChild(){return this._routerState.firstChild(this)}get children(){return this._routerState.children(this)}get pathFromRoot(){return this._routerState.pathFromRoot(this)}get paramMap(){return this._paramMap||(this._paramMap=Ti(this.params)),this._paramMap}get queryParamMap(){return this._queryParamMap||(this._queryParamMap=Ti(this.queryParams)),this._queryParamMap}toString(){return`Route(url:'${this.url.map(r=>r.toString()).join("/")}', path:'${this.routeConfig?this.routeConfig.path:""}')`}}class g0 extends u0{constructor(e,n){super(n),this.url=e,Gu(this,n)}toString(){return m0(this._root)}}function Gu(t,e){e.value._routerState=t,e.children.forEach(n=>Gu(t,n))}function m0(t){const e=t.children.length>0?` { ${t.children.map(m0).join(", ")} } `:"";return`${t.value}${e}`}function Ku(t){if(t.snapshot){const e=t.snapshot,n=t._futureSnapshot;t.snapshot=n,on(e.queryParams,n.queryParams)||t.queryParams.next(n.queryParams),e.fragment!==n.fragment&&t.fragment.next(n.fragment),on(e.params,n.params)||t.params.next(n.params),function(t,e){if(t.length!==e.length)return!1;for(let n=0;n<t.length;++n)if(!on(t[n],e[n]))return!1;return!0}(e.url,n.url)||t.url.next(n.url),on(e.data,n.data)||t.data.next(n.data)}else t.snapshot=t._futureSnapshot,t.data.next(t._futureSnapshot.data)}function Yu(t,e){return on(t.params,e.params)&&function(t,e){return br(t,e)&&t.every((n,r)=>on(n.parameters,e[r].parameters))}(t.url,e.url)&&!(!t.parent!=!e.parent)&&(!t.parent||Yu(t.parent,e.parent))}function Ya(t,e,n){if(n&&t.shouldReuseRoute(e.value,n.value.snapshot)){const r=n.value;r._futureSnapshot=e.value;const i=function(t,e,n){return e.children.map(r=>{for(const i of n.children)if(t.shouldReuseRoute(r.value,i.value.snapshot))return Ya(t,r,i);return Ya(t,r)})}(t,e,n);return new Dn(r,i)}{if(t.shouldAttach(e.value)){const o=t.retrieve(e.value);if(null!==o){const s=o.route;return b0(e,s),s}}const r=function(t){return new qn(new $t(t.url),new $t(t.params),new $t(t.queryParams),new $t(t.fragment),new $t(t.data),t.outlet,t.component,t)}(e.value),i=e.children.map(o=>Ya(t,o));return new Dn(r,i)}}function b0(t,e){if(t.value.routeConfig!==e.value.routeConfig)throw new Error("Cannot reattach ActivatedRouteSnapshot created from a different route");if(t.children.length!==e.children.length)throw new Error("Cannot reattach ActivatedRouteSnapshot with a different number of children");e.value._futureSnapshot=t.value;for(let n=0;n<t.children.length;++n)b0(t.children[n],e.children[n])}function Qa(t){return"object"==typeof t&&null!=t&&!t.outlets&&!t.segmentPath}function $o(t){return"object"==typeof t&&null!=t&&t.outlets}function Qu(t,e,n,r,i){let o={};return r&&Oe(r,(s,a)=>{o[a]=Array.isArray(s)?s.map(l=>`${l}`):`${s}`}),new mr(n.root===t?e:y0(n.root,t,e),o,i)}function y0(t,e,n){const r={};return Oe(t.children,(i,o)=>{r[o]=i===e?n:y0(i,e,n)}),new G(t.segments,r)}class v0{constructor(e,n,r){if(this.isAbsolute=e,this.numberOfDoubleDots=n,this.commands=r,e&&r.length>0&&Qa(r[0]))throw new Error("Root segment cannot have matrix parameters");const i=r.find($o);if(i&&i!==t0(r))throw new Error("{outlets:{}} has to be the last command")}toRoot(){return this.isAbsolute&&1===this.commands.length&&"/"==this.commands[0]}}class Zu{constructor(e,n,r){this.segmentGroup=e,this.processChildren=n,this.index=r}}function w0(t,e,n){if(t||(t=new G([],{})),0===t.segments.length&&t.hasChildren())return Za(t,e,n);const r=function(t,e,n){let r=0,i=e;const o={match:!1,pathIndex:0,commandIndex:0};for(;i<t.segments.length;){if(r>=n.length)return o;const s=t.segments[i],a=n[r];if($o(a))break;const l=`${a}`,c=r<n.length-1?n[r+1]:null;if(i>0&&void 0===l)break;if(l&&c&&"object"==typeof c&&void 0===c.outlets){if(!C0(l,c,s))return o;r+=2}else{if(!C0(l,{},s))return o;r++}i++}return{match:!0,pathIndex:i,commandIndex:r}}(t,e,n),i=n.slice(r.commandIndex);if(r.match&&r.pathIndex<t.segments.length){const o=new G(t.segments.slice(0,r.pathIndex),{});return o.children[$]=new G(t.segments.slice(r.pathIndex),t.children),Za(o,0,i)}return r.match&&0===i.length?new G(t.segments,{}):r.match&&!t.hasChildren()?Xu(t,e,n):r.match?Za(t,0,i):Xu(t,e,n)}function Za(t,e,n){if(0===n.length)return new G(t.segments,{});{const r=function(t){return $o(t[0])?t[0].outlets:{[$]:t}}(n),i={};return Oe(r,(o,s)=>{"string"==typeof o&&(o=[o]),null!==o&&(i[s]=w0(t.children[s],e,o))}),Oe(t.children,(o,s)=>{void 0===r[s]&&(i[s]=o)}),new G(t.segments,i)}}function Xu(t,e,n){const r=t.segments.slice(0,e);let i=0;for(;i<n.length;){const o=n[i];if($o(o)){const l=Sk(o.outlets);return new G(r,l)}if(0===i&&Qa(n[0])){r.push(new zo(t.segments[e].path,_0(n[0]))),i++;continue}const s=$o(o)?o.outlets[$]:`${o}`,a=i<n.length-1?n[i+1]:null;s&&a&&Qa(a)?(r.push(new zo(s,_0(a))),i+=2):(r.push(new zo(s,{})),i++)}return new G(r,{})}function Sk(t){const e={};return Oe(t,(n,r)=>{"string"==typeof n&&(n=[n]),null!==n&&(e[r]=Xu(new G([],{}),0,n))}),e}function _0(t){const e={};return Oe(t,(n,r)=>e[r]=`${n}`),e}function C0(t,e,n){return t==n.path&&on(e,n.parameters)}class Mk{constructor(e,n,r,i){this.routeReuseStrategy=e,this.futureState=n,this.currState=r,this.forwardEvent=i}activate(e){const n=this.futureState._root,r=this.currState?this.currState._root:null;this.deactivateChildRoutes(n,r,e),Ku(this.futureState.root),this.activateChildRoutes(n,r,e)}deactivateChildRoutes(e,n,r){const i=Ho(n);e.children.forEach(o=>{const s=o.value.outlet;this.deactivateRoutes(o,i[s],r),delete i[s]}),Oe(i,(o,s)=>{this.deactivateRouteAndItsChildren(o,r)})}deactivateRoutes(e,n,r){const i=e.value,o=n?n.value:null;if(i===o)if(i.component){const s=r.getContext(i.outlet);s&&this.deactivateChildRoutes(e,n,s.children)}else this.deactivateChildRoutes(e,n,r);else o&&this.deactivateRouteAndItsChildren(n,r)}deactivateRouteAndItsChildren(e,n){this.routeReuseStrategy.shouldDetach(e.value.snapshot)?this.detachAndStoreRouteSubtree(e,n):this.deactivateRouteAndOutlet(e,n)}detachAndStoreRouteSubtree(e,n){const r=n.getContext(e.value.outlet);if(r&&r.outlet){const i=r.outlet.detach(),o=r.children.onOutletDeactivated();this.routeReuseStrategy.store(e.value.snapshot,{componentRef:i,route:e,contexts:o})}}deactivateRouteAndOutlet(e,n){const r=n.getContext(e.value.outlet),i=r&&e.value.component?r.children:n,o=Ho(e);for(const s of Object.keys(o))this.deactivateRouteAndItsChildren(o[s],i);r&&r.outlet&&(r.outlet.deactivate(),r.children.onOutletDeactivated(),r.attachRef=null,r.resolver=null,r.route=null)}activateChildRoutes(e,n,r){const i=Ho(n);e.children.forEach(o=>{this.activateRoutes(o,i[o.value.outlet],r),this.forwardEvent(new KN(o.value.snapshot))}),e.children.length&&this.forwardEvent(new WN(e.value.snapshot))}activateRoutes(e,n,r){const i=e.value,o=n?n.value:null;if(Ku(i),i===o)if(i.component){const s=r.getOrCreateContext(i.outlet);this.activateChildRoutes(e,n,s.children)}else this.activateChildRoutes(e,n,r);else if(i.component){const s=r.getOrCreateContext(i.outlet);if(this.routeReuseStrategy.shouldAttach(i.snapshot)){const a=this.routeReuseStrategy.retrieve(i.snapshot);this.routeReuseStrategy.store(i.snapshot,null),s.children.onOutletReAttached(a.contexts),s.attachRef=a.componentRef,s.route=a.route.value,s.outlet&&s.outlet.attach(a.componentRef,a.route.value),D0(a.route)}else{const a=function(t){for(let e=t.parent;e;e=e.parent){const n=e.routeConfig;if(n&&n._loadedConfig)return n._loadedConfig;if(n&&n.component)return null}return null}(i.snapshot),l=a?a.module.componentFactoryResolver:null;s.attachRef=null,s.route=i,s.resolver=l,s.outlet&&s.outlet.activateWith(i,l),this.activateChildRoutes(e,null,s.children)}}else this.activateChildRoutes(e,null,r)}}function D0(t){Ku(t.value),t.children.forEach(D0)}class Ju{constructor(e,n){this.routes=e,this.module=n}}function Hn(t){return"function"==typeof t}function yr(t){return t instanceof mr}const Wo=Symbol("INITIAL_VALUE");function Go(){return Un(t=>function(...t){let e,n;return ts(t[t.length-1])&&(n=t.pop()),"function"==typeof t[t.length-1]&&(e=t.pop()),1===t.length&&fp(t[0])&&(t=t[0]),ul(t,n).lift(new uN(e))}(t.map(e=>e.pipe(Vu(1),function(...t){const e=t[t.length-1];return ts(e)?(t.pop(),n=>ju(t,n,e)):n=>ju(t,n)}(Wo)))).pipe($v((e,n)=>{let r=!1;return n.reduce((i,o,s)=>i!==Wo?i:(o===Wo&&(r=!0),r||!1!==o&&s!==n.length-1&&!yr(o)?i:o),e)},Wo),Di(e=>e!==Wo),oe(e=>yr(e)?e:!0===e),Vu(1)))}let E0=(()=>{class t{}return t.\u0275fac=function(n){return new(n||t)},t.\u0275cmp=Wn({type:t,selectors:[["ng-component"]],decls:1,vars:0,template:function(n,r){1&n&&Q(0,"router-outlet")},directives:function(){return[sp]},encapsulation:2}),t})();function T0(t,e=""){for(let n=0;n<t.length;n++){const r=t[n];Ok(r,Fk(e,r))}}function Ok(t,e){t.children&&T0(t.children,e)}function Fk(t,e){return e?t||e.path?t&&!e.path?`${t}/`:!t&&e.path?e.path:`${t}/${e.path}`:"":t}function ep(t){const e=t.children&&t.children.map(ep),n=e?Object.assign(Object.assign({},t),{children:e}):Object.assign({},t);return!n.component&&(e||n.loadChildren)&&n.outlet&&n.outlet!==$&&(n.component=E0),n}function St(t){return t.outlet||$}function S0(t,e){const n=t.filter(r=>St(r)===e);return n.push(...t.filter(r=>St(r)!==e)),n}const x0={matched:!1,consumedSegments:[],lastChild:0,parameters:{},positionalParamSegments:{}};function Xa(t,e,n){var r;if(""===e.path)return"full"===e.pathMatch&&(t.hasChildren()||n.length>0)?Object.assign({},x0):{matched:!0,consumedSegments:[],lastChild:0,parameters:{},positionalParamSegments:{}};const o=(e.matcher||ZN)(n,t,e);if(!o)return Object.assign({},x0);const s={};Oe(o.posParams,(l,c)=>{s[c]=l.path});const a=o.consumed.length>0?Object.assign(Object.assign({},s),o.consumed[o.consumed.length-1].parameters):s;return{matched:!0,consumedSegments:o.consumed,lastChild:o.consumed.length,parameters:a,positionalParamSegments:null!==(r=o.posParams)&&void 0!==r?r:{}}}function Ja(t,e,n,r,i="corrected"){if(n.length>0&&function(t,e,n){return n.some(r=>el(t,e,r)&&St(r)!==$)}(t,n,r)){const s=new G(e,function(t,e,n,r){const i={};i[$]=r,r._sourceSegment=t,r._segmentIndexShift=e.length;for(const o of n)if(""===o.path&&St(o)!==$){const s=new G([],{});s._sourceSegment=t,s._segmentIndexShift=e.length,i[St(o)]=s}return i}(t,e,r,new G(n,t.children)));return s._sourceSegment=t,s._segmentIndexShift=e.length,{segmentGroup:s,slicedSegments:[]}}if(0===n.length&&function(t,e,n){return n.some(r=>el(t,e,r))}(t,n,r)){const s=new G(t.segments,function(t,e,n,r,i,o){const s={};for(const a of r)if(el(t,n,a)&&!i[St(a)]){const l=new G([],{});l._sourceSegment=t,l._segmentIndexShift="legacy"===o?t.segments.length:e.length,s[St(a)]=l}return Object.assign(Object.assign({},i),s)}(t,e,n,r,t.children,i));return s._sourceSegment=t,s._segmentIndexShift=e.length,{segmentGroup:s,slicedSegments:n}}const o=new G(t.segments,t.children);return o._sourceSegment=t,o._segmentIndexShift=e.length,{segmentGroup:o,slicedSegments:n}}function el(t,e,n){return(!(t.hasChildren()||e.length>0)||"full"!==n.pathMatch)&&""===n.path}function M0(t,e,n,r){return!!(St(t)===r||r!==$&&el(e,n,t))&&("**"===t.path||Xa(e,t,n).matched)}function I0(t,e,n){return 0===e.length&&!t.children[n]}class Ko{constructor(e){this.segmentGroup=e||null}}class A0{constructor(e){this.urlTree=e}}function tl(t){return new ue(e=>e.error(new Ko(t)))}function P0(t){return new ue(e=>e.error(new A0(t)))}function Uk(t){return new ue(e=>e.error(new Error(`Only absolute redirects can have named outlets. redirectTo: '${t}'`)))}class Hk{constructor(e,n,r,i,o){this.configLoader=n,this.urlSerializer=r,this.urlTree=i,this.config=o,this.allowRedirects=!0,this.ngModule=e.get(rn)}apply(){const e=Ja(this.urlTree.root,[],[],this.config).segmentGroup,n=new G(e.segments,e.children);return this.expandSegmentGroup(this.ngModule,this.config,n,$).pipe(oe(o=>this.createUrlTree(tp(o),this.urlTree.queryParams,this.urlTree.fragment))).pipe(gr(o=>{if(o instanceof A0)return this.allowRedirects=!1,this.match(o.urlTree);throw o instanceof Ko?this.noMatchError(o):o}))}match(e){return this.expandSegmentGroup(this.ngModule,this.config,e.root,$).pipe(oe(i=>this.createUrlTree(tp(i),e.queryParams,e.fragment))).pipe(gr(i=>{throw i instanceof Ko?this.noMatchError(i):i}))}noMatchError(e){return new Error(`Cannot match any routes. URL Segment: '${e.segmentGroup}'`)}createUrlTree(e,n,r){const i=e.segments.length>0?new G([],{[$]:e}):e;return new mr(i,n,r)}expandSegmentGroup(e,n,r,i){return 0===r.segments.length&&r.hasChildren()?this.expandChildren(e,n,r).pipe(oe(o=>new G([],o))):this.expandSegment(e,r,n,r.segments,i,!0)}expandChildren(e,n,r){const i=[];for(const o of Object.keys(r.children))"primary"===o?i.unshift(o):i.push(o);return qe(i).pipe(Bo(o=>{const s=r.children[o],a=S0(n,o);return this.expandSegmentGroup(e,a,s,o).pipe(oe(l=>({segment:l,outlet:o})))}),$v((o,s)=>(o[s.outlet]=s.segment,o),{}),function(t,e){const n=arguments.length>=2;return r=>r.pipe(t?Di((i,o)=>t(i,o,r)):es,Bu(1),n?Gv(e):Wv(()=>new za))}())}expandSegment(e,n,r,i,o,s){return qe(r).pipe(Bo(a=>this.expandSegmentAgainstRoute(e,n,r,a,i,o,s).pipe(gr(c=>{if(c instanceof Ko)return L(null);throw c}))),Ei(a=>!!a),gr((a,l)=>{if(a instanceof za||"EmptyError"===a.name){if(I0(n,i,o))return L(new G([],{}));throw new Ko(n)}throw a}))}expandSegmentAgainstRoute(e,n,r,i,o,s,a){return M0(i,n,o,s)?void 0===i.redirectTo?this.matchSegmentAgainstRoute(e,n,i,o,s):a&&this.allowRedirects?this.expandSegmentAgainstRouteUsingRedirect(e,n,r,i,o,s):tl(n):tl(n)}expandSegmentAgainstRouteUsingRedirect(e,n,r,i,o,s){return"**"===i.path?this.expandWildCardWithParamsAgainstRouteUsingRedirect(e,r,i,s):this.expandRegularSegmentAgainstRouteUsingRedirect(e,n,r,i,o,s)}expandWildCardWithParamsAgainstRouteUsingRedirect(e,n,r,i){const o=this.applyRedirectCommands([],r.redirectTo,{});return r.redirectTo.startsWith("/")?P0(o):this.lineralizeSegments(r,o).pipe(Ne(s=>{const a=new G(s,{});return this.expandSegment(e,a,n,s,i,!1)}))}expandRegularSegmentAgainstRouteUsingRedirect(e,n,r,i,o,s){const{matched:a,consumedSegments:l,lastChild:c,positionalParamSegments:d}=Xa(n,i,o);if(!a)return tl(n);const u=this.applyRedirectCommands(l,i.redirectTo,d);return i.redirectTo.startsWith("/")?P0(u):this.lineralizeSegments(i,u).pipe(Ne(p=>this.expandSegment(e,n,r,p.concat(o.slice(c)),s,!1)))}matchSegmentAgainstRoute(e,n,r,i,o){if("**"===r.path)return r.loadChildren?(r._loadedConfig?L(r._loadedConfig):this.configLoader.load(e.injector,r)).pipe(oe(p=>(r._loadedConfig=p,new G(i,{})))):L(new G(i,{}));const{matched:s,consumedSegments:a,lastChild:l}=Xa(n,r,i);if(!s)return tl(n);const c=i.slice(l);return this.getChildConfig(e,r,i).pipe(Ne(u=>{const p=u.module,h=u.routes,{segmentGroup:f,slicedSegments:m}=Ja(n,a,c,h),g=new G(f.segments,f.children);if(0===m.length&&g.hasChildren())return this.expandChildren(p,h,g).pipe(oe(S=>new G(a,S)));if(0===h.length&&0===m.length)return L(new G(a,{}));const y=St(r)===o;return this.expandSegment(p,g,h,m,y?$:o,!0).pipe(oe(v=>new G(a.concat(v.segments),v.children)))}))}getChildConfig(e,n,r){return n.children?L(new Ju(n.children,e)):n.loadChildren?void 0!==n._loadedConfig?L(n._loadedConfig):this.runCanLoadGuards(e.injector,n,r).pipe(Ne(i=>i?this.configLoader.load(e.injector,n).pipe(oe(o=>(n._loadedConfig=o,o))):function(t){return new ue(e=>e.error(zu(`Cannot load children because the guard of the route "path: '${t.path}'" returned false`)))}(n))):L(new Ju([],e))}runCanLoadGuards(e,n,r){const i=n.canLoad;return i&&0!==i.length?L(i.map(s=>{const a=e.get(s);let l;if(function(t){return t&&Hn(t.canLoad)}(a))l=a.canLoad(n,r);else{if(!Hn(a))throw new Error("Invalid CanLoad guard");l=a(n,r)}return sn(l)})).pipe(Go(),ut(s=>{if(!yr(s))return;const a=zu(`Redirecting to "${this.urlSerializer.serialize(s)}"`);throw a.url=s,a}),oe(s=>!0===s)):L(!0)}lineralizeSegments(e,n){let r=[],i=n.root;for(;;){if(r=r.concat(i.segments),0===i.numberOfChildren)return L(r);if(i.numberOfChildren>1||!i.children[$])return Uk(e.redirectTo);i=i.children[$]}}applyRedirectCommands(e,n,r){return this.applyRedirectCreatreUrlTree(n,this.urlSerializer.parse(n),e,r)}applyRedirectCreatreUrlTree(e,n,r,i){const o=this.createSegmentGroup(e,n.root,r,i);return new mr(o,this.createQueryParams(n.queryParams,this.urlTree.queryParams),n.fragment)}createQueryParams(e,n){const r={};return Oe(e,(i,o)=>{if("string"==typeof i&&i.startsWith(":")){const a=i.substring(1);r[o]=n[a]}else r[o]=i}),r}createSegmentGroup(e,n,r,i){const o=this.createSegments(e,n.segments,r,i);let s={};return Oe(n.children,(a,l)=>{s[l]=this.createSegmentGroup(e,a,r,i)}),new G(o,s)}createSegments(e,n,r,i){return n.map(o=>o.path.startsWith(":")?this.findPosParam(e,o,i):this.findOrReturn(o,r))}findPosParam(e,n,r){const i=r[n.path.substring(1)];if(!i)throw new Error(`Cannot redirect to '${e}'. Cannot find '${n.path}'.`);return i}findOrReturn(e,n){let r=0;for(const i of n){if(i.path===e.path)return n.splice(r),i;r++}return e}}function tp(t){const e={};for(const r of Object.keys(t.children)){const o=tp(t.children[r]);(o.segments.length>0||o.hasChildren())&&(e[r]=o)}return function(t){if(1===t.numberOfChildren&&t.children[$]){const e=t.children[$];return new G(t.segments.concat(e.segments),e.children)}return t}(new G(t.segments,e))}class R0{constructor(e){this.path=e,this.route=this.path[this.path.length-1]}}class nl{constructor(e,n){this.component=e,this.route=n}}function Gk(t,e,n){const r=t._root;return Yo(r,e?e._root:null,n,[r.value])}function rl(t,e,n){const r=function(t){if(!t)return null;for(let e=t.parent;e;e=e.parent){const n=e.routeConfig;if(n&&n._loadedConfig)return n._loadedConfig}return null}(e);return(r?r.module.injector:n).get(t)}function Yo(t,e,n,r,i={canDeactivateChecks:[],canActivateChecks:[]}){const o=Ho(e);return t.children.forEach(s=>{(function(t,e,n,r,i={canDeactivateChecks:[],canActivateChecks:[]}){const o=t.value,s=e?e.value:null,a=n?n.getContext(t.value.outlet):null;if(s&&o.routeConfig===s.routeConfig){const l=function(t,e,n){if("function"==typeof n)return n(t,e);switch(n){case"pathParamsChange":return!br(t.url,e.url);case"pathParamsOrQueryParamsChange":return!br(t.url,e.url)||!on(t.queryParams,e.queryParams);case"always":return!0;case"paramsOrQueryParamsChange":return!Yu(t,e)||!on(t.queryParams,e.queryParams);case"paramsChange":default:return!Yu(t,e)}}(s,o,o.routeConfig.runGuardsAndResolvers);l?i.canActivateChecks.push(new R0(r)):(o.data=s.data,o._resolvedData=s._resolvedData),Yo(t,e,o.component?a?a.children:null:n,r,i),l&&a&&a.outlet&&a.outlet.isActivated&&i.canDeactivateChecks.push(new nl(a.outlet.component,s))}else s&&Qo(e,a,i),i.canActivateChecks.push(new R0(r)),Yo(t,null,o.component?a?a.children:null:n,r,i)})(s,o[s.value.outlet],n,r.concat([s.value]),i),delete o[s.value.outlet]}),Oe(o,(s,a)=>Qo(s,n.getContext(a),i)),i}function Qo(t,e,n){const r=Ho(t),i=t.value;Oe(r,(o,s)=>{Qo(o,i.component?e?e.children.getContext(s):null:e,n)}),n.canDeactivateChecks.push(new nl(i.component&&e&&e.outlet&&e.outlet.isActivated?e.outlet.component:null,i))}class s2{}function N0(t){return new ue(e=>e.error(t))}class l2{constructor(e,n,r,i,o,s){this.rootComponentType=e,this.config=n,this.urlTree=r,this.url=i,this.paramsInheritanceStrategy=o,this.relativeLinkResolution=s}recognize(){const e=Ja(this.urlTree.root,[],[],this.config.filter(s=>void 0===s.redirectTo),this.relativeLinkResolution).segmentGroup,n=this.processSegmentGroup(this.config,e,$);if(null===n)return null;const r=new Ka([],Object.freeze({}),Object.freeze(Object.assign({},this.urlTree.queryParams)),this.urlTree.fragment,{},$,this.rootComponentType,null,this.urlTree.root,-1,{}),i=new Dn(r,n),o=new g0(this.url,i);return this.inheritParamsAndData(o._root),o}inheritParamsAndData(e){const n=e.value,r=f0(n,this.paramsInheritanceStrategy);n.params=Object.freeze(r.params),n.data=Object.freeze(r.data),e.children.forEach(i=>this.inheritParamsAndData(i))}processSegmentGroup(e,n,r){return 0===n.segments.length&&n.hasChildren()?this.processChildren(e,n):this.processSegment(e,n,n.segments,r)}processChildren(e,n){const r=[];for(const o of Object.keys(n.children)){const s=n.children[o],a=S0(e,o),l=this.processSegmentGroup(a,s,o);if(null===l)return null;r.push(...l)}const i=k0(r);return function(t){t.sort((e,n)=>e.value.outlet===$?-1:n.value.outlet===$?1:e.value.outlet.localeCompare(n.value.outlet))}(i),i}processSegment(e,n,r,i){for(const o of e){const s=this.processSegmentAgainstRoute(o,n,r,i);if(null!==s)return s}return I0(n,r,i)?[]:null}processSegmentAgainstRoute(e,n,r,i){if(e.redirectTo||!M0(e,n,r,i))return null;let o,s=[],a=[];if("**"===e.path){const h=r.length>0?t0(r).parameters:{};o=new Ka(r,h,Object.freeze(Object.assign({},this.urlTree.queryParams)),this.urlTree.fragment,j0(e),St(e),e.component,e,O0(n),F0(n)+r.length,L0(e))}else{const h=Xa(n,e,r);if(!h.matched)return null;s=h.consumedSegments,a=r.slice(h.lastChild),o=new Ka(s,h.parameters,Object.freeze(Object.assign({},this.urlTree.queryParams)),this.urlTree.fragment,j0(e),St(e),e.component,e,O0(n),F0(n)+s.length,L0(e))}const l=function(t){return t.children?t.children:t.loadChildren?t._loadedConfig.routes:[]}(e),{segmentGroup:c,slicedSegments:d}=Ja(n,s,a,l.filter(h=>void 0===h.redirectTo),this.relativeLinkResolution);if(0===d.length&&c.hasChildren()){const h=this.processChildren(l,c);return null===h?null:[new Dn(o,h)]}if(0===l.length&&0===d.length)return[new Dn(o,[])];const u=St(e)===i,p=this.processSegment(l,c,d,u?$:i);return null===p?null:[new Dn(o,p)]}}function u2(t){const e=t.value.routeConfig;return e&&""===e.path&&void 0===e.redirectTo}function k0(t){const e=[],n=new Set;for(const r of t){if(!u2(r)){e.push(r);continue}const i=e.find(o=>r.value.routeConfig===o.value.routeConfig);void 0!==i?(i.children.push(...r.children),n.add(i)):e.push(r)}for(const r of n){const i=k0(r.children);e.push(new Dn(r.value,i))}return e.filter(r=>!n.has(r))}function O0(t){let e=t;for(;e._sourceSegment;)e=e._sourceSegment;return e}function F0(t){let e=t,n=e._segmentIndexShift?e._segmentIndexShift:0;for(;e._sourceSegment;)e=e._sourceSegment,n+=e._segmentIndexShift?e._segmentIndexShift:0;return n-1}function j0(t){return t.data||{}}function L0(t){return t.resolve||{}}function np(t){return Un(e=>{const n=t(e);return n?qe(n).pipe(oe(()=>e)):L(e)})}class v2 extends class{shouldDetach(e){return!1}store(e,n){}shouldAttach(e){return!1}retrieve(e){return null}shouldReuseRoute(e,n){return e.routeConfig===n.routeConfig}}{}const rp=new Y("ROUTES");class V0{constructor(e,n,r,i){this.loader=e,this.compiler=n,this.onLoadStartListener=r,this.onLoadEndListener=i}load(e,n){if(n._loader$)return n._loader$;this.onLoadStartListener&&this.onLoadStartListener(n);const i=this.loadModuleFactory(n.loadChildren).pipe(oe(o=>{this.onLoadEndListener&&this.onLoadEndListener(n);const s=o.create(e);return new Ju(e0(s.injector.get(rp,void 0,I.Self|I.Optional)).map(ep),s)}),gr(o=>{throw n._loader$=void 0,o}));return n._loader$=new Tp(i,()=>new ln).pipe(pl()),n._loader$}loadModuleFactory(e){return"string"==typeof e?qe(this.loader.load(e)):sn(e()).pipe(Ne(n=>n instanceof _b?L(n):qe(this.compiler.compileModuleAsync(n))))}}class w2{constructor(){this.outlet=null,this.route=null,this.resolver=null,this.children=new Si,this.attachRef=null}}class Si{constructor(){this.contexts=new Map}onChildOutletCreated(e,n){const r=this.getOrCreateContext(e);r.outlet=n,this.contexts.set(e,r)}onChildOutletDestroyed(e){const n=this.getContext(e);n&&(n.outlet=null)}onOutletDeactivated(){const e=this.contexts;return this.contexts=new Map,e}onOutletReAttached(e){this.contexts=e}getOrCreateContext(e){let n=this.getContext(e);return n||(n=new w2,this.contexts.set(e,n)),n}getContext(e){return this.contexts.get(e)||null}}class C2{shouldProcessUrl(e){return!0}extract(e){return e}merge(e,n){return e}}function D2(t){throw t}function E2(t,e,n){return e.parse("/")}function B0(t,e){return L(null)}const T2={paths:"exact",fragment:"ignored",matrixParams:"ignored",queryParams:"exact"},S2={paths:"subset",fragment:"ignored",matrixParams:"ignored",queryParams:"subset"};let Re=(()=>{class t{constructor(n,r,i,o,s,a,l,c){this.rootComponentType=n,this.urlSerializer=r,this.rootContexts=i,this.location=o,this.config=c,this.lastSuccessfulNavigation=null,this.currentNavigation=null,this.disposed=!1,this.lastLocationChangeInfo=null,this.navigationId=0,this.currentPageId=0,this.isNgZoneEnabled=!1,this.events=new ln,this.errorHandler=D2,this.malformedUriErrorHandler=E2,this.navigated=!1,this.lastSuccessfulId=-1,this.hooks={beforePreactivation:B0,afterPreactivation:B0},this.urlHandlingStrategy=new C2,this.routeReuseStrategy=new v2,this.onSameUrlNavigation="ignore",this.paramsInheritanceStrategy="emptyOnly",this.urlUpdateStrategy="deferred",this.relativeLinkResolution="corrected",this.canceledNavigationResolution="replace",this.ngModule=s.get(rn),this.console=s.get(fa);const p=s.get(_e);this.isNgZoneEnabled=p instanceof _e&&_e.isInAngularZone(),this.resetConfig(c),this.currentUrlTree=new mr(new G([],{}),{},null),this.rawUrlTree=this.currentUrlTree,this.browserUrlTree=this.currentUrlTree,this.configLoader=new V0(a,l,h=>this.triggerEvent(new Yv(h)),h=>this.triggerEvent(new Qv(h))),this.routerState=h0(this.currentUrlTree,this.rootComponentType),this.transitions=new $t({id:0,targetPageId:0,currentUrlTree:this.currentUrlTree,currentRawUrl:this.currentUrlTree,extractedUrl:this.urlHandlingStrategy.extract(this.currentUrlTree),urlAfterRedirects:this.urlHandlingStrategy.extract(this.currentUrlTree),rawUrl:this.currentUrlTree,extras:{},resolve:null,reject:null,promise:Promise.resolve(!0),source:"imperative",restoredState:null,currentSnapshot:this.routerState.snapshot,targetSnapshot:null,currentRouterState:this.routerState,targetRouterState:null,guards:{canActivateChecks:[],canDeactivateChecks:[]},guardsResult:null}),this.navigations=this.setupNavigations(this.transitions),this.processNavigations()}get browserPageId(){var n;return null===(n=this.location.getState())||void 0===n?void 0:n.\u0275routerPageId}setupNavigations(n){const r=this.events;return n.pipe(Di(i=>0!==i.id),oe(i=>Object.assign(Object.assign({},i),{extractedUrl:this.urlHandlingStrategy.extract(i.rawUrl)})),Un(i=>{let o=!1,s=!1;return L(i).pipe(ut(a=>{this.currentNavigation={id:a.id,initialUrl:a.currentRawUrl,extractedUrl:a.extractedUrl,trigger:a.source,extras:a.extras,previousNavigation:this.lastSuccessfulNavigation?Object.assign(Object.assign({},this.lastSuccessfulNavigation),{previousNavigation:null}):null}}),Un(a=>{const l=this.browserUrlTree.toString(),c=!this.navigated||a.extractedUrl.toString()!==l||l!==this.currentUrlTree.toString();if(("reload"===this.onSameUrlNavigation||c)&&this.urlHandlingStrategy.shouldProcessUrl(a.rawUrl))return il(a.source)&&(this.browserUrlTree=a.extractedUrl),L(a).pipe(Un(u=>{const p=this.transitions.getValue();return r.next(new Uu(u.id,this.serializeUrl(u.extractedUrl),u.source,u.restoredState)),p!==this.transitions.getValue()?Ci:Promise.resolve(u)}),function(t,e,n,r){return Un(i=>function(t,e,n,r,i){return new Hk(t,e,n,r,i).apply()}(t,e,n,i.extractedUrl,r).pipe(oe(o=>Object.assign(Object.assign({},i),{urlAfterRedirects:o}))))}(this.ngModule.injector,this.configLoader,this.urlSerializer,this.config),ut(u=>{this.currentNavigation=Object.assign(Object.assign({},this.currentNavigation),{finalUrl:u.urlAfterRedirects})}),function(t,e,n,r,i){return Ne(o=>function(t,e,n,r,i="emptyOnly",o="legacy"){try{const s=new l2(t,e,n,r,i,o).recognize();return null===s?N0(new s2):L(s)}catch(s){return N0(s)}}(t,e,o.urlAfterRedirects,n(o.urlAfterRedirects),r,i).pipe(oe(s=>Object.assign(Object.assign({},o),{targetSnapshot:s}))))}(this.rootComponentType,this.config,u=>this.serializeUrl(u),this.paramsInheritanceStrategy,this.relativeLinkResolution),ut(u=>{"eager"===this.urlUpdateStrategy&&(u.extras.skipLocationChange||this.setBrowserUrl(u.urlAfterRedirects,u),this.browserUrlTree=u.urlAfterRedirects);const p=new BN(u.id,this.serializeUrl(u.extractedUrl),this.serializeUrl(u.urlAfterRedirects),u.targetSnapshot);r.next(p)}));if(c&&this.rawUrlTree&&this.urlHandlingStrategy.shouldProcessUrl(this.rawUrlTree)){const{id:p,extractedUrl:h,source:f,restoredState:m,extras:g}=a,y=new Uu(p,this.serializeUrl(h),f,m);r.next(y);const b=h0(h,this.rootComponentType).snapshot;return L(Object.assign(Object.assign({},a),{targetSnapshot:b,urlAfterRedirects:h,extras:Object.assign(Object.assign({},g),{skipLocationChange:!1,replaceUrl:!1})}))}return this.rawUrlTree=a.rawUrl,this.browserUrlTree=a.urlAfterRedirects,a.resolve(null),Ci}),np(a=>{const{targetSnapshot:l,id:c,extractedUrl:d,rawUrl:u,extras:{skipLocationChange:p,replaceUrl:h}}=a;return this.hooks.beforePreactivation(l,{navigationId:c,appliedUrlTree:d,rawUrlTree:u,skipLocationChange:!!p,replaceUrl:!!h})}),ut(a=>{const l=new UN(a.id,this.serializeUrl(a.extractedUrl),this.serializeUrl(a.urlAfterRedirects),a.targetSnapshot);this.triggerEvent(l)}),oe(a=>Object.assign(Object.assign({},a),{guards:Gk(a.targetSnapshot,a.currentSnapshot,this.rootContexts)})),function(t,e){return Ne(n=>{const{targetSnapshot:r,currentSnapshot:i,guards:{canActivateChecks:o,canDeactivateChecks:s}}=n;return 0===s.length&&0===o.length?L(Object.assign(Object.assign({},n),{guardsResult:!0})):function(t,e,n,r){return qe(t).pipe(Ne(i=>function(t,e,n,r,i){const o=e&&e.routeConfig?e.routeConfig.canDeactivate:null;return o&&0!==o.length?L(o.map(a=>{const l=rl(a,e,i);let c;if(function(t){return t&&Hn(t.canDeactivate)}(l))c=sn(l.canDeactivate(t,e,n,r));else{if(!Hn(l))throw new Error("Invalid CanDeactivate guard");c=sn(l(t,e,n,r))}return c.pipe(Ei())})).pipe(Go()):L(!0)}(i.component,i.route,n,e,r)),Ei(i=>!0!==i,!0))}(s,r,i,t).pipe(Ne(a=>a&&function(t){return"boolean"==typeof t}(a)?function(t,e,n,r){return qe(e).pipe(Bo(i=>ju(function(t,e){return null!==t&&e&&e(new $N(t)),L(!0)}(i.route.parent,r),function(t,e){return null!==t&&e&&e(new GN(t)),L(!0)}(i.route,r),function(t,e,n){const r=e[e.length-1],o=e.slice(0,e.length-1).reverse().map(s=>function(t){const e=t.routeConfig?t.routeConfig.canActivateChild:null;return e&&0!==e.length?{node:t,guards:e}:null}(s)).filter(s=>null!==s).map(s=>qv(()=>L(s.guards.map(l=>{const c=rl(l,s.node,n);let d;if(function(t){return t&&Hn(t.canActivateChild)}(c))d=sn(c.canActivateChild(r,t));else{if(!Hn(c))throw new Error("Invalid CanActivateChild guard");d=sn(c(r,t))}return d.pipe(Ei())})).pipe(Go())));return L(o).pipe(Go())}(t,i.path,n),function(t,e,n){const r=e.routeConfig?e.routeConfig.canActivate:null;return r&&0!==r.length?L(r.map(o=>qv(()=>{const s=rl(o,e,n);let a;if(function(t){return t&&Hn(t.canActivate)}(s))a=sn(s.canActivate(e,t));else{if(!Hn(s))throw new Error("Invalid CanActivate guard");a=sn(s(e,t))}return a.pipe(Ei())}))).pipe(Go()):L(!0)}(t,i.route,n))),Ei(i=>!0!==i,!0))}(r,o,t,e):L(a)),oe(a=>Object.assign(Object.assign({},n),{guardsResult:a})))})}(this.ngModule.injector,a=>this.triggerEvent(a)),ut(a=>{if(yr(a.guardsResult)){const c=zu(`Redirecting to "${this.serializeUrl(a.guardsResult)}"`);throw c.url=a.guardsResult,c}const l=new zN(a.id,this.serializeUrl(a.extractedUrl),this.serializeUrl(a.urlAfterRedirects),a.targetSnapshot,!!a.guardsResult);this.triggerEvent(l)}),Di(a=>!!a.guardsResult||(this.restoreHistory(a),this.cancelNavigationTransition(a,""),!1)),np(a=>{if(a.guards.canActivateChecks.length)return L(a).pipe(ut(l=>{const c=new qN(l.id,this.serializeUrl(l.extractedUrl),this.serializeUrl(l.urlAfterRedirects),l.targetSnapshot);this.triggerEvent(c)}),Un(l=>{let c=!1;return L(l).pipe(function(t,e){return Ne(n=>{const{targetSnapshot:r,guards:{canActivateChecks:i}}=n;if(!i.length)return L(n);let o=0;return qe(i).pipe(Bo(s=>function(t,e,n,r){return function(t,e,n,r){const i=Object.keys(t);if(0===i.length)return L({});const o={};return qe(i).pipe(Ne(s=>function(t,e,n,r){const i=rl(t,e,r);return sn(i.resolve?i.resolve(e,n):i(e,n))}(t[s],e,n,r).pipe(ut(a=>{o[s]=a}))),Bu(1),Ne(()=>Object.keys(o).length===i.length?L(o):Ci))}(t._resolve,t,e,r).pipe(oe(o=>(t._resolvedData=o,t.data=Object.assign(Object.assign({},t.data),f0(t,n).resolve),null)))}(s.route,r,t,e)),ut(()=>o++),Bu(1),Ne(s=>o===i.length?L(n):Ci))})}(this.paramsInheritanceStrategy,this.ngModule.injector),ut({next:()=>c=!0,complete:()=>{c||(this.restoreHistory(l),this.cancelNavigationTransition(l,"At least one route resolver didn't emit any value."))}}))}),ut(l=>{const c=new HN(l.id,this.serializeUrl(l.extractedUrl),this.serializeUrl(l.urlAfterRedirects),l.targetSnapshot);this.triggerEvent(c)}))}),np(a=>{const{targetSnapshot:l,id:c,extractedUrl:d,rawUrl:u,extras:{skipLocationChange:p,replaceUrl:h}}=a;return this.hooks.afterPreactivation(l,{navigationId:c,appliedUrlTree:d,rawUrlTree:u,skipLocationChange:!!p,replaceUrl:!!h})}),oe(a=>{const l=function(t,e,n){const r=Ya(t,e._root,n?n._root:void 0);return new p0(r,e)}(this.routeReuseStrategy,a.targetSnapshot,a.currentRouterState);return Object.assign(Object.assign({},a),{targetRouterState:l})}),ut(a=>{this.currentUrlTree=a.urlAfterRedirects,this.rawUrlTree=this.urlHandlingStrategy.merge(a.urlAfterRedirects,a.rawUrl),this.routerState=a.targetRouterState,"deferred"===this.urlUpdateStrategy&&(a.extras.skipLocationChange||this.setBrowserUrl(this.rawUrlTree,a),this.browserUrlTree=a.urlAfterRedirects)}),((t,e,n)=>oe(r=>(new Mk(e,r.targetRouterState,r.currentRouterState,n).activate(t),r)))(this.rootContexts,this.routeReuseStrategy,a=>this.triggerEvent(a)),ut({next(){o=!0},complete(){o=!0}}),function(t){return e=>e.lift(new jN(t))}(()=>{if(!o&&!s){const a=`Navigation ID ${i.id} is not equal to the current navigation id ${this.navigationId}`;"replace"===this.canceledNavigationResolution?(this.restoreHistory(i),this.cancelNavigationTransition(i,a)):this.cancelNavigationTransition(i,a)}this.currentNavigation=null}),gr(a=>{if(s=!0,function(t){return t&&t[Xv]}(a)){const l=yr(a.url);l||(this.navigated=!0,this.restoreHistory(i,!0));const c=new Kv(i.id,this.serializeUrl(i.extractedUrl),a.message);r.next(c),l?setTimeout(()=>{const d=this.urlHandlingStrategy.merge(a.url,this.rawUrlTree),u={skipLocationChange:i.extras.skipLocationChange,replaceUrl:"eager"===this.urlUpdateStrategy||il(i.source)};this.scheduleNavigation(d,"imperative",null,u,{resolve:i.resolve,reject:i.reject,promise:i.promise})},0):i.resolve(!1)}else{this.restoreHistory(i,!0);const l=new VN(i.id,this.serializeUrl(i.extractedUrl),a);r.next(l);try{i.resolve(this.errorHandler(a))}catch(c){i.reject(c)}}return Ci}))}))}resetRootComponentType(n){this.rootComponentType=n,this.routerState.root.component=this.rootComponentType}getTransition(){const n=this.transitions.value;return n.urlAfterRedirects=this.browserUrlTree,n}setTransition(n){this.transitions.next(Object.assign(Object.assign({},this.getTransition()),n))}initialNavigation(){this.setUpLocationChangeListener(),0===this.navigationId&&this.navigateByUrl(this.location.path(!0),{replaceUrl:!0})}setUpLocationChangeListener(){this.locationSubscription||(this.locationSubscription=this.location.subscribe(n=>{const r=this.extractLocationChangeInfoFromEvent(n);this.shouldScheduleNavigation(this.lastLocationChangeInfo,r)&&setTimeout(()=>{const{source:i,state:o,urlTree:s}=r,a={replaceUrl:!0};if(o){const l=Object.assign({},o);delete l.navigationId,delete l.\u0275routerPageId,0!==Object.keys(l).length&&(a.state=l)}this.scheduleNavigation(s,i,o,a)},0),this.lastLocationChangeInfo=r}))}extractLocationChangeInfoFromEvent(n){var r;return{source:"popstate"===n.type?"popstate":"hashchange",urlTree:this.parseUrl(n.url),state:(null===(r=n.state)||void 0===r?void 0:r.navigationId)?n.state:null,transitionId:this.getTransition().id}}shouldScheduleNavigation(n,r){if(!n)return!0;const i=r.urlTree.toString()===n.urlTree.toString();return r.transitionId!==n.transitionId||!i||!("hashchange"===r.source&&"popstate"===n.source||"popstate"===r.source&&"hashchange"===n.source)}get url(){return this.serializeUrl(this.currentUrlTree)}getCurrentNavigation(){return this.currentNavigation}triggerEvent(n){this.events.next(n)}resetConfig(n){T0(n),this.config=n.map(ep),this.navigated=!1,this.lastSuccessfulId=-1}ngOnDestroy(){this.dispose()}dispose(){this.transitions.complete(),this.locationSubscription&&(this.locationSubscription.unsubscribe(),this.locationSubscription=void 0),this.disposed=!0}createUrlTree(n,r={}){const{relativeTo:i,queryParams:o,fragment:s,queryParamsHandling:a,preserveFragment:l}=r,c=i||this.routerState.root,d=l?this.currentUrlTree.fragment:s;let u=null;switch(a){case"merge":u=Object.assign(Object.assign({},this.currentUrlTree.queryParams),o);break;case"preserve":u=this.currentUrlTree.queryParams;break;default:u=o||null}return null!==u&&(u=this.removeEmptyProps(u)),function(t,e,n,r,i){if(0===n.length)return Qu(e.root,e.root,e,r,i);const o=function(t){if("string"==typeof t[0]&&1===t.length&&"/"===t[0])return new v0(!0,0,t);let e=0,n=!1;const r=t.reduce((i,o,s)=>{if("object"==typeof o&&null!=o){if(o.outlets){const a={};return Oe(o.outlets,(l,c)=>{a[c]="string"==typeof l?l.split("/"):l}),[...i,{outlets:a}]}if(o.segmentPath)return[...i,o.segmentPath]}return"string"!=typeof o?[...i,o]:0===s?(o.split("/").forEach((a,l)=>{0==l&&"."===a||(0==l&&""===a?n=!0:".."===a?e++:""!=a&&i.push(a))}),i):[...i,o]},[]);return new v0(n,e,r)}(n);if(o.toRoot())return Qu(e.root,new G([],{}),e,r,i);const s=function(t,e,n){if(t.isAbsolute)return new Zu(e.root,!0,0);if(-1===n.snapshot._lastPathIndex){const o=n.snapshot._urlSegment;return new Zu(o,o===e.root,0)}const r=Qa(t.commands[0])?0:1;return function(t,e,n){let r=t,i=e,o=n;for(;o>i;){if(o-=i,r=r.parent,!r)throw new Error("Invalid number of '../'");i=r.segments.length}return new Zu(r,!1,i-o)}(n.snapshot._urlSegment,n.snapshot._lastPathIndex+r,t.numberOfDoubleDots)}(o,e,t),a=s.processChildren?Za(s.segmentGroup,s.index,o.commands):w0(s.segmentGroup,s.index,o.commands);return Qu(s.segmentGroup,a,e,r,i)}(c,this.currentUrlTree,n,u,null!=d?d:null)}navigateByUrl(n,r={skipLocationChange:!1}){const i=yr(n)?n:this.parseUrl(n),o=this.urlHandlingStrategy.merge(i,this.rawUrlTree);return this.scheduleNavigation(o,"imperative",null,r)}navigate(n,r={skipLocationChange:!1}){return function(t){for(let e=0;e<t.length;e++){const n=t[e];if(null==n)throw new Error(`The requested path contains ${n} segment at index ${e}`)}}(n),this.navigateByUrl(this.createUrlTree(n,r),r)}serializeUrl(n){return this.urlSerializer.serialize(n)}parseUrl(n){let r;try{r=this.urlSerializer.parse(n)}catch(i){r=this.malformedUriErrorHandler(i,this.urlSerializer,n)}return r}isActive(n,r){let i;if(i=!0===r?Object.assign({},T2):!1===r?Object.assign({},S2):r,yr(n))return r0(this.currentUrlTree,n,i);const o=this.parseUrl(n);return r0(this.currentUrlTree,o,i)}removeEmptyProps(n){return Object.keys(n).reduce((r,i)=>{const o=n[i];return null!=o&&(r[i]=o),r},{})}processNavigations(){this.navigations.subscribe(n=>{this.navigated=!0,this.lastSuccessfulId=n.id,this.currentPageId=n.targetPageId,this.events.next(new Uo(n.id,this.serializeUrl(n.extractedUrl),this.serializeUrl(this.currentUrlTree))),this.lastSuccessfulNavigation=this.currentNavigation,n.resolve(!0)},n=>{this.console.warn(`Unhandled Navigation Error: ${n}`)})}scheduleNavigation(n,r,i,o,s){var a,l;if(this.disposed)return Promise.resolve(!1);const c=this.getTransition(),d=il(r)&&c&&!il(c.source),h=(this.lastSuccessfulId===c.id||this.currentNavigation?c.rawUrl:c.urlAfterRedirects).toString()===n.toString();if(d&&h)return Promise.resolve(!0);let f,m,g;s?(f=s.resolve,m=s.reject,g=s.promise):g=new Promise((v,S)=>{f=v,m=S});const y=++this.navigationId;let b;return"computed"===this.canceledNavigationResolution?(0===this.currentPageId&&(i=this.location.getState()),b=i&&i.\u0275routerPageId?i.\u0275routerPageId:o.replaceUrl||o.skipLocationChange?null!==(a=this.browserPageId)&&void 0!==a?a:0:(null!==(l=this.browserPageId)&&void 0!==l?l:0)+1):b=0,this.setTransition({id:y,targetPageId:b,source:r,restoredState:i,currentUrlTree:this.currentUrlTree,currentRawUrl:this.rawUrlTree,rawUrl:n,extras:o,resolve:f,reject:m,promise:g,currentSnapshot:this.routerState.snapshot,currentRouterState:this.routerState}),g.catch(v=>Promise.reject(v))}setBrowserUrl(n,r){const i=this.urlSerializer.serialize(n),o=Object.assign(Object.assign({},r.extras.state),this.generateNgRouterState(r.id,r.targetPageId));this.location.isCurrentPathEqualTo(i)||r.extras.replaceUrl?this.location.replaceState(i,"",o):this.location.go(i,"",o)}restoreHistory(n,r=!1){var i,o;if("computed"===this.canceledNavigationResolution){const s=this.currentPageId-n.targetPageId;"popstate"!==n.source&&"eager"!==this.urlUpdateStrategy&&this.currentUrlTree!==(null===(i=this.currentNavigation)||void 0===i?void 0:i.finalUrl)||0===s?this.currentUrlTree===(null===(o=this.currentNavigation)||void 0===o?void 0:o.finalUrl)&&0===s&&(this.resetState(n),this.browserUrlTree=n.currentUrlTree,this.resetUrlToCurrentUrlTree()):this.location.historyGo(s)}else"replace"===this.canceledNavigationResolution&&(r&&this.resetState(n),this.resetUrlToCurrentUrlTree())}resetState(n){this.routerState=n.currentRouterState,this.currentUrlTree=n.currentUrlTree,this.rawUrlTree=this.urlHandlingStrategy.merge(this.currentUrlTree,n.rawUrl)}resetUrlToCurrentUrlTree(){this.location.replaceState(this.urlSerializer.serialize(this.rawUrlTree),"",this.generateNgRouterState(this.lastSuccessfulId,this.currentPageId))}cancelNavigationTransition(n,r){const i=new Kv(n.id,this.serializeUrl(n.extractedUrl),r);this.triggerEvent(i),n.resolve(!1)}generateNgRouterState(n,r){return"computed"===this.canceledNavigationResolution?{navigationId:n,\u0275routerPageId:r}:{navigationId:n}}}return t.\u0275fac=function(n){return new(n||t)(T(Ts),T(qu),T(Si),T(xa),T(te),T(ma),T(ur),T(void 0))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();function il(t){return"imperative"!==t}let sp=(()=>{class t{constructor(n,r,i,o,s){this.parentContexts=n,this.location=r,this.resolver=i,this.changeDetector=s,this.activated=null,this._activatedRoute=null,this.activateEvents=new Dt,this.deactivateEvents=new Dt,this.name=o||$,n.onChildOutletCreated(this.name,this)}ngOnDestroy(){this.parentContexts.onChildOutletDestroyed(this.name)}ngOnInit(){if(!this.activated){const n=this.parentContexts.getContext(this.name);n&&n.route&&(n.attachRef?this.attach(n.attachRef,n.route):this.activateWith(n.route,n.resolver||null))}}get isActivated(){return!!this.activated}get component(){if(!this.activated)throw new Error("Outlet is not activated");return this.activated.instance}get activatedRoute(){if(!this.activated)throw new Error("Outlet is not activated");return this._activatedRoute}get activatedRouteData(){return this._activatedRoute?this._activatedRoute.snapshot.data:{}}detach(){if(!this.activated)throw new Error("Outlet is not activated");this.location.detach();const n=this.activated;return this.activated=null,this._activatedRoute=null,n}attach(n,r){this.activated=n,this._activatedRoute=r,this.location.insert(n.hostView)}deactivate(){if(this.activated){const n=this.component;this.activated.destroy(),this.activated=null,this._activatedRoute=null,this.deactivateEvents.emit(n)}}activateWith(n,r){if(this.isActivated)throw new Error("Cannot activate an already activated outlet");this._activatedRoute=n;const s=(r=r||this.resolver).resolveComponentFactory(n._futureSnapshot.routeConfig.component),a=this.parentContexts.getOrCreateContext(this.name).children,l=new A2(n,a,this.location.injector);this.activated=this.location.createComponent(s,this.location.length,l),this.changeDetector.markForCheck(),this.activateEvents.emit(this.activated.instance)}}return t.\u0275fac=function(n){return new(n||t)(x(Si),x(jt),x(lr),function(t){return function(t,e){if("class"===e)return t.classes;if("style"===e)return t.styles;const n=t.attrs;if(n){const r=n.length;let i=0;for(;i<r;){const o=n[i];if(rh(o))break;if(0===o)i+=2;else if("number"==typeof o)for(i++;i<r&&"string"==typeof n[i];)i++;else{if(o===e)return n[i+1];i+=2}}}return null}(Te(),t)}("name"),x(hd))},t.\u0275dir=Fe({type:t,selectors:[["router-outlet"]],outputs:{activateEvents:"activate",deactivateEvents:"deactivate"},exportAs:["outlet"]}),t})();class A2{constructor(e,n,r){this.route=e,this.childContexts=n,this.parent=r}get(e,n){return e===qn?this.route:e===Si?this.childContexts:this.parent.get(e,n)}}class U0{}class z0{preload(e,n){return L(null)}}let q0=(()=>{class t{constructor(n,r,i,o,s){this.router=n,this.injector=o,this.preloadingStrategy=s,this.loader=new V0(r,i,c=>n.triggerEvent(new Yv(c)),c=>n.triggerEvent(new Qv(c)))}setUpPreloading(){this.subscription=this.router.events.pipe(Di(n=>n instanceof Uo),Bo(()=>this.preload())).subscribe(()=>{})}preload(){const n=this.injector.get(rn);return this.processRoutes(n,this.router.config)}ngOnDestroy(){this.subscription&&this.subscription.unsubscribe()}processRoutes(n,r){const i=[];for(const o of r)if(o.loadChildren&&!o.canLoad&&o._loadedConfig){const s=o._loadedConfig;i.push(this.processRoutes(s.module,s.routes))}else o.loadChildren&&!o.canLoad?i.push(this.preloadConfig(n,o)):o.children&&i.push(this.processRoutes(n,o.children));return qe(i).pipe(Ni(),oe(o=>{}))}preloadConfig(n,r){return this.preloadingStrategy.preload(r,()=>(r._loadedConfig?L(r._loadedConfig):this.loader.load(n.injector,r)).pipe(Ne(o=>(r._loadedConfig=o,this.processRoutes(o.module,o.routes)))))}}return t.\u0275fac=function(n){return new(n||t)(T(Re),T(ma),T(ur),T(te),T(U0))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})(),ap=(()=>{class t{constructor(n,r,i={}){this.router=n,this.viewportScroller=r,this.options=i,this.lastId=0,this.lastSource="imperative",this.restoredId=0,this.store={},i.scrollPositionRestoration=i.scrollPositionRestoration||"disabled",i.anchorScrolling=i.anchorScrolling||"disabled"}init(){"disabled"!==this.options.scrollPositionRestoration&&this.viewportScroller.setHistoryScrollRestoration("manual"),this.routerEventsSubscription=this.createScrollEvents(),this.scrollEventsSubscription=this.consumeScrollEvents()}createScrollEvents(){return this.router.events.subscribe(n=>{n instanceof Uu?(this.store[this.lastId]=this.viewportScroller.getScrollPosition(),this.lastSource=n.navigationTrigger,this.restoredId=n.restoredState?n.restoredState.navigationId:0):n instanceof Uo&&(this.lastId=n.id,this.scheduleScrollEvent(n,this.router.parseUrl(n.urlAfterRedirects).fragment))})}consumeScrollEvents(){return this.router.events.subscribe(n=>{n instanceof Zv&&(n.position?"top"===this.options.scrollPositionRestoration?this.viewportScroller.scrollToPosition([0,0]):"enabled"===this.options.scrollPositionRestoration&&this.viewportScroller.scrollToPosition(n.position):n.anchor&&"enabled"===this.options.anchorScrolling?this.viewportScroller.scrollToAnchor(n.anchor):"disabled"!==this.options.scrollPositionRestoration&&this.viewportScroller.scrollToPosition([0,0]))})}scheduleScrollEvent(n,r){this.router.triggerEvent(new Zv(n,"popstate"===this.lastSource?this.store[this.restoredId]:null,r))}ngOnDestroy(){this.routerEventsSubscription&&this.routerEventsSubscription.unsubscribe(),this.scrollEventsSubscription&&this.scrollEventsSubscription.unsubscribe()}}return t.\u0275fac=function(n){return new(n||t)(T(Re),T(Mv),T(void 0))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();const vr=new Y("ROUTER_CONFIGURATION"),H0=new Y("ROUTER_FORROOT_GUARD"),R2=[xa,{provide:qu,useClass:a0},{provide:Re,useFactory:function(t,e,n,r,i,o,s,a={},l,c){const d=new Re(null,t,e,n,r,i,o,e0(s));return l&&(d.urlHandlingStrategy=l),c&&(d.routeReuseStrategy=c),function(t,e){t.errorHandler&&(e.errorHandler=t.errorHandler),t.malformedUriErrorHandler&&(e.malformedUriErrorHandler=t.malformedUriErrorHandler),t.onSameUrlNavigation&&(e.onSameUrlNavigation=t.onSameUrlNavigation),t.paramsInheritanceStrategy&&(e.paramsInheritanceStrategy=t.paramsInheritanceStrategy),t.relativeLinkResolution&&(e.relativeLinkResolution=t.relativeLinkResolution),t.urlUpdateStrategy&&(e.urlUpdateStrategy=t.urlUpdateStrategy)}(a,d),a.enableTracing&&d.events.subscribe(u=>{var p,h;null===(p=console.group)||void 0===p||p.call(console,`Router Event: ${u.constructor.name}`),console.log(u.toString()),console.log(u),null===(h=console.groupEnd)||void 0===h||h.call(console)}),d},deps:[qu,Si,xa,te,ma,ur,rp,vr,[class{},new et],[class{},new et]]},Si,{provide:qn,useFactory:function(t){return t.routerState.root},deps:[Re]},{provide:ma,useClass:_I},q0,z0,class{preload(e,n){return n().pipe(gr(()=>L(null)))}},{provide:vr,useValue:{enableTracing:!1}}];function N2(){return new Qd("Router",Re)}let $0=(()=>{class t{constructor(n,r){}static forRoot(n,r){return{ngModule:t,providers:[R2,W0(n),{provide:H0,useFactory:F2,deps:[[Re,new et,new Nn]]},{provide:vr,useValue:r||{}},{provide:_i,useFactory:O2,deps:[fr,[new Lr(hu),new et],vr]},{provide:ap,useFactory:k2,deps:[Re,Mv,vr]},{provide:U0,useExisting:r&&r.preloadingStrategy?r.preloadingStrategy:z0},{provide:Qd,multi:!0,useFactory:N2},[lp,{provide:Io,multi:!0,useFactory:B2,deps:[lp]},{provide:G0,useFactory:U2,deps:[lp]},{provide:Dy,multi:!0,useExisting:G0}]]}}static forChild(n){return{ngModule:t,providers:[W0(n)]}}}return t.\u0275fac=function(n){return new(n||t)(T(H0,8),T(Re,8))},t.\u0275mod=Gn({type:t}),t.\u0275inj=dn({}),t})();function k2(t,e,n){return n.scrollOffset&&e.setOffset(n.scrollOffset),new ap(t,e,n)}function O2(t,e,n={}){return n.useHash?new UA(t,e):new hv(t,e)}function F2(t){return"guarded"}function W0(t){return[{provide:k_,multi:!0,useValue:t},{provide:rp,multi:!0,useValue:t}]}let lp=(()=>{class t{constructor(n){this.injector=n,this.initNavigation=!1,this.destroyed=!1,this.resultOfPreactivationDone=new ln}appInitializer(){return this.injector.get(LA,Promise.resolve(null)).then(()=>{if(this.destroyed)return Promise.resolve(!0);let r=null;const i=new Promise(a=>r=a),o=this.injector.get(Re),s=this.injector.get(vr);return"disabled"===s.initialNavigation?(o.setUpLocationChangeListener(),r(!0)):"enabled"===s.initialNavigation||"enabledBlocking"===s.initialNavigation?(o.hooks.afterPreactivation=()=>this.initNavigation?L(null):(this.initNavigation=!0,r(!0),this.resultOfPreactivationDone),o.initialNavigation()):r(!0),i})}bootstrapListener(n){const r=this.injector.get(vr),i=this.injector.get(q0),o=this.injector.get(ap),s=this.injector.get(Re),a=this.injector.get(yi);n===a.components[0]&&(("enabledNonBlocking"===r.initialNavigation||void 0===r.initialNavigation)&&s.initialNavigation(),i.setUpPreloading(),o.init(),s.resetRootComponentType(a.componentTypes[0]),this.resultOfPreactivationDone.next(null),this.resultOfPreactivationDone.complete())}ngOnDestroy(){this.destroyed=!0}}return t.\u0275fac=function(n){return new(n||t)(T(te))},t.\u0275prov=q({token:t,factory:t.\u0275fac}),t})();function B2(t){return t.appInitializer.bind(t)}function U2(t){return t.bootstrapListener.bind(t)}const G0=new Y("Router Initializer");let q2=(()=>{class t{constructor(n){this.router=n}ngOnInit(){"es"==this.router.parseUrl(this.router.url).queryParamMap.get("lang")&&this.setSpanish()}goToMenu(n){let r=this.router.parseUrl(this.router.url),i=this.router.createUrlTree(["menu",n]);i.queryParams.lang=r.queryParamMap.get("lang"),this.router.navigateByUrl(i)}goToBlog(n){let r=this.router.parseUrl(this.router.url),i=this.router.createUrlTree(["blog","Caso",n]);i.queryParams.lang=r.queryParamMap.get("lang"),this.router.navigateByUrl(i)}setSpanish(){document.querySelectorAll(".case").forEach(n=>n.innerHTML="Caso"),document.querySelectorAll(".more").forEach(n=>n.innerHTML="Ver m\xe1s"),document.getElementById("case1").innerText="Bienes ra\xedces en Ames Iowa",document.getElementById("case2").innerText="Enfermedad card\xedaca",document.getElementById("case3").innerText="Segmentaci\xf3n de clientes",document.getElementById("title11").innerText="Preparaci\xf3n de datos",document.getElementById("title12").innerText="Proyectos con missing values, outliers, transformaciones, selecci\xf3n de atributos, etc.",document.getElementById("title13").innerText="Algoritmos lineales",document.getElementById("title14").innerText="Ejercicios con Regresi\xf3n Lineal, Regresi\xf3n Log\xedstica y Linear Discriminant Analysis.",document.getElementById("title15").innerText="Algoritmos no lineales",document.getElementById("title16").innerText="Problemas con \xc1rboles de Decisi\xf3n, Naive Bayes, Support Vector Machines y K-Nearest Neighbors.",document.getElementById("title17").innerText="No supervisado",document.getElementById("title18").innerText="Estudios con m\xe9todos de clustering y Principal Component Analysis.",document.getElementById("title19").innerText="Ensambles",document.getElementById("title110").innerText="An\xe1lisis de algoritmos Bagging, Random Forest, Boosting y Ada Boost."}}return t.\u0275fac=function(n){return new(n||t)(x(Re))},t.\u0275cmp=Wn({type:t,selectors:[["app-home"]],decls:86,vars:0,consts:[["id","myCarousel","data-bs-ride","carousel",1,"carousel","slide"],[1,"carousel-indicators"],["type","button","data-bs-target","#myCarousel","data-bs-slide-to","0","aria-current","true","aria-label","Slide 1",1,"active"],["type","button","data-bs-target","#myCarousel","data-bs-slide-to","1","aria-label","Slide 2"],["type","button","data-bs-target","#myCarousel","data-bs-slide-to","2","aria-label","Slide 3"],[1,"carousel-inner"],["id","bg1",1,"carousel-item","active"],[1,"carousel-caption","text-start"],[1,"case"],["id","case1"],[1,"btn","btn-lg","btn-primary","more",3,"click"],["id","bg2",1,"carousel-item"],["id","case2"],["id","bg3",1,"carousel-item"],["id","case3"],["type","button","data-bs-target","#myCarousel","data-bs-slide","prev",1,"carousel-control-prev"],["aria-hidden","true",1,"carousel-control-prev-icon"],[1,"visually-hidden"],["type","button","data-bs-target","#myCarousel","data-bs-slide","next",1,"carousel-control-next"],["aria-hidden","true",1,"carousel-control-next-icon"],[1,"container","marketing"],[1,"row"],[1,"col-lg-4"],["id","title11"],["id","title12",1,"menuText"],[1,"menuText"],[1,"btn","btn-primary","more",3,"click"],[1,"mobile"],["id","title13"],["id","title14",1,"menuText"],["id","title15"],["id","title16",1,"menuText"],["id","title17"],["id","title18",1,"menuText"],["id","title19"],["id","title110",1,"menuText"]],template:function(n,r){1&n&&(_(0,"div",0),_(1,"div",1),Q(2,"button",2),Q(3,"button",3),Q(4,"button",4),C(),_(5,"div",5),_(6,"div",6),_(7,"div",7),_(8,"h4",8),B(9,"Study Cases"),C(),_(10,"h1",9),B(11,"Ames housing"),C(),_(12,"button",10),le("click",function(){return r.goToBlog("Bienes ra\xedces en Ames Iowa")}),B(13,"Show more"),C(),C(),C(),_(14,"div",11),_(15,"div",7),_(16,"h4",8),B(17,"Case"),C(),_(18,"h1",12),B(19,"Heart disease"),C(),_(20,"button",10),le("click",function(){return r.goToBlog("Enfermedad card\xedaca")}),B(21,"Show more"),C(),C(),C(),_(22,"div",13),_(23,"div",7),_(24,"h4",8),B(25,"Case"),C(),_(26,"h1",14),B(27,"Customer segmentation"),C(),_(28,"button",10),le("click",function(){return r.goToBlog("Segmentaci\xf3n de clientes")}),B(29,"Show more"),C(),C(),C(),C(),_(30,"button",15),Q(31,"span",16),_(32,"span",17),B(33,"Previous"),C(),C(),_(34,"button",18),Q(35,"span",19),_(36,"span",17),B(37,"Next"),C(),C(),C(),_(38,"div",20),_(39,"div",21),Q(40,"hr"),_(41,"div",22),_(42,"h4",23),B(43,"Data preparation"),C(),_(44,"p",24),B(45,"Projects with missing values, outliers, transformations, attribute selection, etc."),C(),_(46,"p",25),_(47,"button",26),le("click",function(){return r.goToMenu("Preparaci\xf3n de datos")}),B(48," Show more"),C(),C(),C(),Q(49,"hr",27),_(50,"div",22),_(51,"h4",28),B(52,"Linear algorithms"),C(),_(53,"p",29),B(54,"Exercises with Linear Regression, Logistic Regression and Linear Discriminant Analysis."),C(),_(55,"p",25),_(56,"button",26),le("click",function(){return r.goToMenu("Algoritmos lineales")}),B(57," Show more"),C(),C(),C(),Q(58,"hr",27),_(59,"div",22),_(60,"h4",30),B(61,"Non linear algorithms"),C(),_(62,"p",31),B(63,"Problems with Decision Trees, Naive Bayes, Support Vector Machines and K-Nearest Neighbors."),C(),_(64,"p",25),_(65,"button",26),le("click",function(){return r.goToMenu("Algoritmos no lineales")}),B(66," Show more"),C(),C(),C(),Q(67,"hr",27),_(68,"div",22),_(69,"h4",32),B(70,"Unsupervised"),C(),_(71,"p",33),B(72,"Studies with clustering methods and Principal Component Analysis."),C(),_(73,"p",25),_(74,"button",26),le("click",function(){return r.goToMenu("No supervisado")}),B(75," Show more"),C(),C(),C(),Q(76,"hr",27),_(77,"div",22),_(78,"h4",34),B(79,"Ensembles"),C(),_(80,"p",35),B(81,"Analysis of Bagging, Random Forest, Boosting and Ada Boost algorithms."),C(),_(82,"p",25),_(83,"button",26),le("click",function(){return r.goToBlog("Ensambles")}),B(84," Show more"),C(),C(),C(),Q(85,"hr"),C(),C())},styles:["button[_ngcontent-%COMP%]{text-shadow:-2px 0 black,0 2px black,2px 0 black,0 -2px black;padding:4px}#bg1[_ngcontent-%COMP%]{background-image:url(ames-0.ec9f9731527e0f72024d.jpg);background-size:cover}#bg2[_ngcontent-%COMP%]{background-image:url(cardiaca-0.faea1652cac487186b48.jpg);background-size:cover}#bg3[_ngcontent-%COMP%]{background-image:url(customer-seg-0.828f6a6e74c2cdce3e1d.jpg);background-size:cover}@media (min-width: 991px){.bd-placeholder-img-lg[_ngcontent-%COMP%]{font-size:3.5rem}.mobile[_ngcontent-%COMP%]{display:none}}@media (max-width: 991px){.mobile[_ngcontent-%COMP%]{display:block}p.menuText[_ngcontent-%COMP%]{font-size:small;margin-bottom:5px}h4[_ngcontent-%COMP%]{font-size:large}hr[_ngcontent-%COMP%]{margin:5px}#myCarousel[_ngcontent-%COMP%]{margin:5px}}.row[_ngcontent-%COMP%]{max-width:100%}.carousel-control-prev-icon[_ngcontent-%COMP%]{text-shadow:-2px 0 black,0 2px black,2px 0 black,0 -2px black;border-radius:50px;background:black}.carousel-control-next-icon[_ngcontent-%COMP%]{text-shadow:-2px 0 black,0 2px black,2px 0 black,0 -2px black;border-radius:50px;background:black}body[_ngcontent-%COMP%]{padding-top:3rem;padding-bottom:3rem;color:#5a5a5a}.carousel[_ngcontent-%COMP%]{margin-bottom:2rem}.carousel-caption[_ngcontent-%COMP%]{bottom:3rem;z-index:10}.carousel-item[_ngcontent-%COMP%]{height:24rem;width:100%}.carousel-item[_ngcontent-%COMP%] > img[_ngcontent-%COMP%]{position:absolute;top:0;left:0;min-width:100%;height:32rem}.marketing[_ngcontent-%COMP%]   .col-lg-4[_ngcontent-%COMP%]{margin-top:.5rem;margin-bottom:.5rem;text-align:center}.marketing[_ngcontent-%COMP%]   h4[_ngcontent-%COMP%]{color:#fff;font-weight:800;margin-bottom:1px}.marketing[_ngcontent-%COMP%]   .col-lg-4[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{margin-right:.75rem;margin-left:.75rem}.col-lg-4[_ngcontent-%COMP%]{padding-left:2%;padding-right:2%}.featurette-divider[_ngcontent-%COMP%]{margin:1rem 10;padding-bottom:5 rem}.featurette-heading[_ngcontent-%COMP%]{font-weight:300;line-height:1;letter-spacing:-.05rem}h1[_ngcontent-%COMP%]{text-shadow:-2px 0 black,0 2px black,2px 0 black,0 -2px black;font-weight:800}h4[_ngcontent-%COMP%]{text-shadow:-2px 0 black,0 2px black,2px 0 black,0 -2px black;font-weight:800}@media (min-width: 40em){.carousel-caption[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{margin-bottom:1.25rem;font-size:1.25rem;line-height:1.4}.featurette-heading[_ngcontent-%COMP%]{font-size:50px}}@media (min-width: 62em){.featurette-heading[_ngcontent-%COMP%]{margin-top:7rem}}.bd-placeholder-img[_ngcontent-%COMP%]{font-size:1.125rem;text-anchor:middle;-webkit-user-select:none;user-select:none}.principal-images[_ngcontent-%COMP%]{flex-shrink:0;min-width:100%;min-height:100%}h2[_ngcontent-%COMP%]{text-align:center;color:#fff}p.menuText[_ngcontent-%COMP%]{text-align:center}p[_ngcontent-%COMP%]{color:#fff}hr[_ngcontent-%COMP%]{color:#fff}"]}),t})();const H2=JSON.parse('[{"titulo":"Preparaci\xf3n de datos","title":"Data preparation","descripcion":"Proyectos con missing values, outliers, transformaciones, selecci\xf3n de atributos, etc.","description":"Projects with missing values, outliers, transformations, attribute selection, etc.","contenido":"Pre procesamiento de datos en Wine con Python>>Data pre processing y c\xe1lculo de probabilidades de sucesos sobre el dataset Titanic con Python>>Normalizaci\xf3n sobre dataset Wine en RapidMiner>>Selecci\xf3n de atributos para dataset Sonar>>Tutoriales de preparaci\xf3n de datos en RapidMiner","content":"Data pre-processing in Wine with Python>>Data pre-processing and calculation of probabilities of events on the Titanic dataset with Python>>Normalization on Wine dataset in RapidMiner>>Selection of attributes for Sonar dataset>>Data preparation tutorials in RapidMiner"},{"titulo":"Algoritmos lineales","title":"Linear algorithms","descripcion":"Ejercicios con Regresi\xf3n Lineal, Regresi\xf3n Log\xedstica y Linear Discriminant Analysis.","description":"Exercises with Linear Regression, Logistic Regression and Linear Discriminant Analysis.","contenido":"Regresi\xf3n lineal simple en hoja de c\xe1lculo>>Regresi\xf3n log\xedstica en planilla electr\xf3nica>>LDA en hoja de c\xe1lculo>>Titanic>>Deportes","content":"Simple Linear Regression in spreadsheet>>Logistic Regression in spreadsheet>>LDA in spreadsheet>>Titanic dataset>>Sports"},{"titulo":"Algoritmos no lineales","title":"Non linear algorithms","descripcion":"Problemas con \xc1rboles de Decisi\xf3n, Naive Bayes, Support Vector Machines y K-Nearest Neighbors.","description":"Problems with Decision Trees, Naive Bayes, Support Vector Machines and K-Nearest Neighbors.","contenido":"CART para clasificaci\xf3n binaria simple>>\xc1rboles de decisi\xf3n en KNIME>>Comparaci\xf3n de Decision Trees en herramientas de ML>>Naive Bayes en planilla electr\xf3nica>>SVM lineal con descenso de sub gradiente>>SVM no lineal en RapidMiner>>KNN en hoja de c\xe1lculo y RapidMiner","content":"CART for Simple Binary Classification>>Decision Trees in KNIME>>Comparison of Decision Trees in ML Tools>>Naive Bayes in spreadsheet>>Linear SVM with Sub-Gradient Descent>>Nonlinear SVM in RapidMiner>>KNN in Sheet calculation and RapidMiner"},{"titulo":"No supervisado","title":"Unsupervised","descripcion":"Estudios con m\xe9todos de clustering y Principal Component Analysis.","description":"Studies with clustering methods and Principal Component Analysis.","contenido":"DBSCAN en Wine>>PCA para dataset Cereals","content":"DBSCAN in Wine>>PCA for Cereals dataset"}]'),$2=JSON.parse('[{"unidad":"Preparaci\xf3n de datos","tarea":"Normalizaci\xf3n sobre dataset Wine en RapidMiner","descripcion":"En RapidMiner se realiz\xf3 una normalizaci\xf3n sobre el dataset Wine y se compar\xf3 la performance de aplicar Naive Bayes usando el dataset normalizado y no normalizado con un split con el 70% de ejemplos para training y 30% para test en ambos casos.","unit":"Data preparation","task":"Normalization on Wine dataset in RapidMiner","description":"In RapidMiner, a normalization was performed on the Wine dataset and the performance of applying Naive Bayes was compared using the normalized and non-normalized dataset with a split with 70% of examples for training and 30% for testing in both cases."},{"unidad":"Preparaci\xf3n de datos","tarea":"Tutoriales de preparaci\xf3n de datos en RapidMiner","descripcion":"En este ejercicio se hicieron tutoriales de modelado, scoring, split y validaci\xf3n, cross validation y visual model comparison para la herramienta RapidMiner.","unit":"Data preparation","task":"Data preparation tutorials in RapidMiner","description":"In this exercise, modeling, scoring, split and validation, cross validation and visual model comparison tutorials were made for the RapidMiner tool."},{"unidad":"Preparaci\xf3n de datos","tarea":"Pre procesamiento de datos en Wine con Python","descripcion":"Para este caso se aplican t\xe9cnicas de pre procesamiento como normalizaci\xf3n, estandarizaci\xf3n y split en datos de training y test. Luego se obtienen estad\xedsticas para el dataset Wine utiliando funciones de Python con SciKitLearn y la librer\xeda Pandas.","unit":"Data preparation","task":"Data pre-processing in Wine with Python","description":"For this case, pre-processing techniques such as normalization, standardization and split are applied to training and test data. Statistics for the Wine dataset are then obtained using Python functions with SciKitLearn and the Pandas library."},{"unidad":"Preparaci\xf3n de datos","tarea":"Data pre processing y c\xe1lculo de probabilidades de sucesos sobre el dataset Titanic con Python","descripcion":"Sobre el dataset Titanic se realiz\xf3 un tratamiento de missing values con reemplazo, remoci\xf3n, sustituci\xf3n por categor\xeda \xfanica y selecci\xf3n de atributos. Posteriormente se realizaron consultas probabil\xedsticas sobre los datos.","unit":"Data preparation","task":"Data pre-processing and calculation of probabilities of events on the Titanic dataset with Python","description":"On the Titanic dataset, a missing value treatment was carried out with replacement, removal, substitution by single category and selection of attributes. Subsequently, probabilistic queries were made on the data."},{"unidad":"Preparaci\xf3n de datos","tarea":"Selecci\xf3n de atributos para dataset Sonar","descripcion":"Ejercicio con selecci\xf3n de atributos en RapidMiner sobre el dataset Sonar.","unit":"Data preparation","task":"Selection of attributes for Sonar dataset","description":"Exercise with attribute selection in RapidMiner on the Sonar dataset."},{"unidad":"Algoritmos lineales","tarea":"Titanic","descripcion":"En este caso se hace un estudio de preparaci\xf3n de datos para el dataset Titanic y se crean modelos de regresi\xf3n log\xedstica equivalentes para RapidMiner y SciKitLearn.","unit":"Linear algorithms","task":"Titanic dataset","description":"In this case, a data preparation study is done for the Titanic dataset and equivalent logistic regression models are created for RapidMiner and SciKitLearn."},{"unidad":"Algoritmos lineales","tarea":"Deportes","descripcion":"Utilizaremos An\xe1lisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificaci\xf3n a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.","unit":"Linear algorithms","task":"Sports","description":"We will use Linear Discriminant Analysis (LDA) with Python\'s Sckit-learn and RapidMiner to train and classify from the Sports dataset to finally compare the predictions made by both models."},{"unidad":"Algoritmos lineales","tarea":"Regresi\xf3n lineal simple en hoja de c\xe1lculo","descripcion":"Utilizando una planilla electr\xf3nica se gener\xf3 un modelo de regresi\xf3n linear simple calculando los coeficientes en el primer ejercicio y por descenso de gradiente en el segundo ejercicio. Para ambos casos se usaron los modelos para hacer predicciones.","unit":"Linear algorithms","task":"Simple Linear Regression in spreadsheet","description":"Using an electronic spreadsheet, a simple linear regression model was generated by calculating the coefficients in the first exercise and by gradient descent in the second exercise. For both cases the models were used to make predictions."},{"unidad":"Algoritmos lineales","tarea":"Regresi\xf3n log\xedstica en planilla electr\xf3nica","descripcion":"En una planilla electr\xf3nica se visualizar\xe1 una funci\xf3n log\xedstica y se genarar\xe1n modelos de regresi\xf3n log\xedstica con descenso de gradiente estoc\xe1stico.","unit":"Linear algorithms","task":"Logistic Regression in spreadsheet","description":"A logistic function will be visualized in an electronic spreadsheet and logistic regression models will be generated with stochastic gradient descent."},{"unidad":"Algoritmos lineales","tarea":"LDA en hoja de c\xe1lculo","descripcion":"Empleando una planilla electr\xf3nica se generar\xe1 un modelo de an\xe1lisis de discriminante lineal LDA y con \xe9l se har\xe1 la predicci\xf3n de clase para un conjunto de datos con un atributo y una salida con dos clases.","unit":"Linear algorithms","task":"LDA in spreadsheet","description":"Using an electronic spreadsheet, an LDA linear discriminant analysis model will be generated and with it the class prediction will be made for a data set with one attribute and an output with two classes."},{"unidad":"Algoritmos lineales","tarea":"Regresi\xf3n lineal con descenso de gradiente","descripcion":"En este ejercicio se usa una planilla electr\xf3nica para minimizar una funci\xf3n siguiendo los gradientes de la funci\xf3n de costo y luego armar un modelo de regresi\xf3n lineal.","unit":"Linear algorithms","task":"Comparaci\xf3n de LDA con Python y RapidMiner para el dataset Sports","description":"Se realizan ejercicios con modelos de LDA para en Python y RapidMiner y se comparan los resultados obtenidos."},{"unidad":"Algoritmos no lineales","tarea":"CART para clasificaci\xf3n binaria simple","descripcion":"Emplearemos un modelo CART para un problema de clasificaci\xf3n binaria simple con dos variables de entrada X1 y X2 y una variable de salida Y.","unit":"Non linear algorithms","task":"CART for Simple Binary Classification","description":"We will use a CART model for a simple binary classification problem with two input variables X1 and X2 and one output variable Y."},{"unidad":"Algoritmos no lineales","tarea":"\xc1rboles de decisi\xf3n en KNIME","descripcion":"Estudiaremos la construcci\xf3n de un modelo de \xe1rbol de decisi\xf3n de regresi\xf3n simple en la herramienta KNIME.","unit":"Non linear algorithms","task":"Decision Trees in KNIME","description":"We will study the construction of a simple regression decision tree model in the KNIME tool."},{"unidad":"Algoritmos no lineales","tarea":"SVM lineal con descenso de sub gradiente","descripcion":"En una hoja de c\xe1lculo analizaremos el algoritmo de Support Vector Machines lineal con descenso de sub gradiente y lo utilizaremos para realizar predicciones.","unit":"Non linear algorithms","task":"Linear SVM with Sub-Gradient Descent","description":"In a spreadsheet, we will discuss the Sub-Gradient Descent Linear Support Vector Machines algorithm and use it to make predictions."},{"unidad":"Algoritmos no lineales","tarea":"SVM no lineal en RapidMiner","descripcion":"En este ejercicio se analizar\xe1 el componente SVM de RapidMiner y se lo utilizar\xe1 para resolver un problema no separable linealmente.","unit":"Non linear algorithms","task":"Nonlinear SVM in RapidMiner","description":"In this exercise you will explore the SVM component of RapidMiner and use it to solve a linearly non-separable problem."},{"unidad":"Algoritmos no lineales","tarea":"Comparaci\xf3n de Decision Trees en herramientas de ML","descripcion":"Comparaci\xf3n de \xe1rboles de decisi\xf3n entre RapidMiner y Weka aplicados al dataset Iris. Adem\xe1s se analiza el componente mencionado en Azure ML Studio, KNIME y Python Sci kit learn.","unit":"Non linear algorithms","task":"Comparison of Decision Trees in ML Tools","description":"Comparison of decision trees between RapidMiner and Weka applied to the Iris dataset. In addition, the mentioned component is analyzed in Azure ML Studio, KNIME and Python Sci kit learn."},{"unidad":"Algoritmos no lineales","tarea":"Naive Bayes en planilla electr\xf3nica","descripcion":"Implementaci\xf3n de un modelo de Naive Bayes en una planilla electr\xf3nica.","unit":"Non linear algorithms","task":"Naive Bayes in spreadsheet","description":"Implementation of a Naive Bayes model in an electronic spreadsheet."},{"unidad":"Algoritmos no lineales","tarea":"KNN en hoja de c\xe1lculo y RapidMiner","descripcion":"Empleo de KNN para realizar predicciones en una hoja de c\xe1lculo y RapidMiner.","unit":"Non linear algorithms","task":"KNN in Sheet calculation and RapidMiner","description":"Using KNN to make predictions in a spreadsheet and RapidMiner."},{"unidad":"No supervisado","tarea":"DBSCAN en Wine","descripcion":"Aplicamos el algoritmo de clustering DBSCAN para encontrar agrupaciones en el dataet Wine.","unit":"Unsupervised","task":"DBSCAN in Wine","description":"We apply the DBSCAN clustering algorithm to find clusters in the Wine dataset."},{"unidad":"No supervisado","tarea":"PCA para dataset Cereals","descripcion":"Estudio del algoritmo PCA utilizando los datos de Cereals","unit":"Unsupervised","task":"PCA for Cereals dataset","description":"Study of the PCA algorithm using Cereals data"}]');let W2=(()=>{class t{constructor(n,r){this.router=n,this.route=r,this.title="json-file-read-angular",this.MenusList=H2,this.DescripcionList=$2}ngOnInit(){var r,n=0;let i,o,s;var a,l,d=this.route.snapshot.paramMap.get("idMenu");let u=document.getElementById("name"),p=document.getElementById("description"),h=document.getElementById("menus");var f=this.router;let m=this.MenusList.find(b=>b.titulo===d),y=this.router.parseUrl(this.router.url).queryParamMap.get("lang");for(s=null==m?void 0:m.contenido,l=s.split(">>").map(b=>b.trim()),"en"==y&&(r=null==m?void 0:m.title,i=null==m?void 0:m.description,o=null==m?void 0:m.content,a=o.split(">>").map(b=>b.trim())),"es"==y&&(r=null==m?void 0:m.titulo,i=null==m?void 0:m.descripcion,o=null==m?void 0:m.contenido,a=o.split(">>").map(b=>b.trim())),u.innerText=r,u.style.color="white",u.style.marginBottom="10px",u.style.fontWeight="700",u.style.fontSize=window.matchMedia("(min-width: 768px)").matches?"xxx-large":"xx-large",p.innerText=i,p.style.fontSize=window.matchMedia("(min-width: 768px)").matches?"medium":"small";n<a.length;){let b,v,S,R,ce,ze;"en"==y&&(ze=l[n]),"es"==y&&(ze=a[n]),b=document.createElement("div"),b.setAttribute("class","articulo"),b.style.marginTop="15px",b.style.marginBottom="15px",b.style.padding="18px",b.style.border="solid rgba(255, 255, 255, .25) thin",v=document.createElement("h4"),v.textContent=a[n],v.style.color="white",window.matchMedia("(min-width: 768px)").matches&&(v.style.fontSize="large"),R=document.createElement("p"),R.style.color="white","en"==y&&(ce=this.DescripcionList.find(pt=>pt.unit==(null==m?void 0:m.title)&&pt.task==a[n]),R.textContent=ce.description),"es"==y&&(ce=this.DescripcionList.find(pt=>pt.unidad==(null==m?void 0:m.titulo)&&pt.tarea==a[n]),R.textContent=ce.descripcion),R.setAttribute("align","center"),R.style.fontSize=window.matchMedia("(min-width: 768px)").matches?"medium":"small",S=document.createElement("button"),"en"==y&&(S.textContent="Show more"),"es"==y&&(S.textContent="Ver m\xe1s"),S.setAttribute("class","btn btn-primary"),S.addEventListener("click",function(){let pt=f.parseUrl(f.url),En=f.createUrlTree(["blog",d,ze]);En.queryParams.lang=pt.queryParamMap.get("lang"),f.navigateByUrl(En)}),b.appendChild(v),b.appendChild(R),b.appendChild(S),h.appendChild(b),n++}}}return t.\u0275fac=function(n){return new(n||t)(x(Re),x(qn))},t.\u0275cmp=Wn({type:t,selectors:[["app-menu"]],decls:9,vars:0,consts:[[1,"container"],["id","name"],["id","descCont"],["id","description"],["id","menus",1,"grid"]],template:function(n,r){1&n&&(_(0,"div",0),_(1,"div"),Q(2,"h3",1),_(3,"div",2),Q(4,"p",3),C(),Q(5,"hr"),Q(6,"div",4),Q(7,"hr"),Q(8,"br"),C(),C())},styles:[".marketing[_ngcontent-%COMP%]   .col-lg-4[_ngcontent-%COMP%]{margin-bottom:1.5rem;text-align:center}.marketing[_ngcontent-%COMP%]   h[_ngcontent-%COMP%]{font-weight:400}.marketing.col-lg-4[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{margin-right:.75rem;margin-left:.75rem}.container[_ngcontent-%COMP%]{display:flex;margin-top:6%;justify-content:center;align-items:center;text-align:center;width:50%}@media (max-width: 1079px){.container[_ngcontent-%COMP%]{width:85%;margin-top:15%}}@media (max-width: 765px){.container[_ngcontent-%COMP%]{width:95%;margin-top:20%}}#menus[_ngcontent-%COMP%]{margin:auto;padding:2px;inline-size:100%;overflow-wrap:break-word}#descCont[_ngcontent-%COMP%]{margin:auto;width:60%;inline-size:100%;overflow-wrap:break-word}h1[_ngcontent-%COMP%]{color:#fff;font-weight:700}p[_ngcontent-%COMP%]{color:#fff}hr[_ngcontent-%COMP%]{color:#fff;margin:0%}"]}),t})();const G2=JSON.parse('[{"unidad":"Preparaci\xf3n de datos","unit":"Data preparation","titulo":"Normalizaci\xf3n sobre dataset Wine en RapidMiner","title":"Normalization over Wine dataset in RapidMiner","descripcion":"En RapidMiner se realiz\xf3 una normalizaci\xf3n sobre el dataset Wine y se compar\xf3 la performance de aplicar Naive Bayes usando el dataset normalizado y no normalizado con un split con el 70% de ejemplos para training y 30% para test en ambos casos.","description":"In RapidMiner a normalization was performed on the Wine dataset and the performance of applying Naive Bayes was compared using the normalized and non-normalized dataset with a split with 70% of examples for training and 30% for testing in both cases .","contenido":"p>>- Se cre\xf3 un proceso en RapidMiner con dos canales, uno que utiliza el dataset normalizado con transforamci\xf3n Z y uno que no.=,=p>>- En ambos canales se realiz\xf3 un split con 70% de datos para entrenamiento y 30% para test. =,=p>>- Para cada caso se entren\xf3 un modelo de Naive Bayes para clasificaci\xf3n por lo cual hubo que convertir el atriburto Wine de num\xe9rico a polin\xf3mico.=,=p>>- Se le midi\xf3 performance del modelo.=,=br=,=h3>>Resultados=,=img>>UT2-PD1-3.jpg>>100%>>400=,=br=,=a>>Descargar modelo>>UT2 PD1 ej2.rmp>>d=,=br=,=br=,=p>>Caso normalizado=,=img>>UT2-PD1-4.jpg>>100%>>200=,=br=,=br=,=p>>Caso sin normalizar=,=img>>UT2-PD1-5.jpg>>100%>>200=,=br=,=br=,=p>>Se obtuvo un apenas poco de mayor presici\xf3n para el caso del dataset normalizado lo cual se debe al algoritmo utilizado. Naive Bayes calcula frecuencias para hallar las clases de predicci\xf3n y al estandarizar los datos, las frecuencias se mantienen de una forma similar.","content":"p>>- A process was created in RapidMiner with two channels, one that uses the normalized dataset with Z transformation and one that does not.=,=p>>- A split was performed on both channels with 70% of data for training and 30% for tests. =,=p>>- For each case, a Naive Bayes model was trained for classification, for which the Wine attribute had to be converted from numeric to polynomial.=,=p>>- The performance of the model was measured.=,= br=,=h3>>Results=,=img>>UT2-PD1-3.jpg>>100%>>400=,=br=,=a>>Download model>>UT2 PD1 ej2.rmp>>d =,=br=,=br=,=p>>Normalized case=,=img>>UT2-PD1-4.jpg>>100%>>200=,=br=,=br=,=p>> Non-normalized case=,=img>>UT2-PD1-5.jpg>>100%>>200=,=br=,=br=,=p>>A little better precision was obtained for the dataset case normalized which is due to the algorithm used. Naive Bayes computes frequencies to find the prediction classes, and by standardizing the data, the frequencies remain similar."},{"unidad":"Preparaci\xf3n de datos","unit":"Data preparation","titulo":"Tutoriales de preparaci\xf3n de datos en RapidMiner","title":"Data preparation tutorials in RapidMiner","descripcion":"En este ejercicio se hicieron tutoriales de missing values, normalizaci\xf3n, detecci\xf3n de outliers, modelado, scoring, split y validaci\xf3n, cross validation y visual model comparison para la herramienta RapidMiner.","description":"In this exercise, modeling, scoring, split and validation, cross validation and visual model comparison tutorials were made for the RapidMiner tool.","contenido":"h3>> Tutorial Handling Missing Values =,=p>>=,=img>>UT2-PD1-1.jpg>>100%>>200=,=br=,=a>>Descargar archivo>>Tutorial Handling Missing Values.rmp>>d =,=br=,=br=,=h3>> Tutorial Normalization and Outlier detection =,=img>>UT2-PD1-2.jpg>>100%>>150=,=br=,=a>>Descargar archivo>>Tutorial Normalization and Outlier detection.rmp>>d=,=br=,=br=,=h3>> Modeling =,=img>>UT2-PD2-1.jpg>>100%>>300 =,=br=,=a>>Descargar archivo>>Tutorial Modeling.rmp>>d=,=br=,=br=,=br=,=h3>> Scoring =,=img>>UT2-PD2-2.jpg>>100%>>250 =,=br=,=a>>Descargar archivo>>Tutorial Scoring.rmp>>d=,=br=,=br=,=br=,=h3>> Test Splits and Validation =,=img>>UT2-PD2-3.jpg>>100%>>175 =,=br=,=a>>Descargar archivo>>Tutorial Test Splits and Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Cross Validation =,=img>>UT2-PD2-4-1.jpg>>100%>>200=,=br=,=br=,=img>>UT2-PD2-4-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>> Tutorial Cross Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Visual Model Comparison =,=img>>UT2-PD2-5-1.jpg>>100%>>125=,=br=,=br=,=img>>UT2-PD2-5-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>>Tutorial Visual Model Comparison.rmp>>d","content":"h3>> Tutorial Handling Missing Values =,=p>>=,=img>>UT2-PD1-1.jpg>>100%>>200=,=br=,=a>>Descargar archivo>>Tutorial Handling Missing Values.rmp>>d =,=br=,=br=,=h3>> Tutorial Normalization and Outlier detection =,=img>>UT2-PD1-2.jpg>>100%>>150=,=br=,=a>>Descargar archivo>>Tutorial Normalization and Outlier detection.rmp>>d=,=br=,=br=,=h3>> Modeling =,=img>>UT2-PD2-1.jpg>>100%>>300 =,=br=,=a>>Descargar archivo>>Tutorial Modeling.rmp>>d=,=br=,=br=,=br=,=h3>> Scoring =,=img>>UT2-PD2-2.jpg>>100%>>250 =,=br=,=a>>Descargar archivo>>Tutorial Scoring.rmp>>d=,=br=,=br=,=br=,=h3>> Test Splits and Validation =,=img>>UT2-PD2-3.jpg>>100%>>175 =,=br=,=a>>Descargar archivo>>Tutorial Test Splits and Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Cross Validation =,=img>>UT2-PD2-4-1.jpg>>100%>>200=,=br=,=br=,=img>>UT2-PD2-4-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>> Tutorial Cross Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Visual Model Comparison =,=img>>UT2-PD2-5-1.jpg>>100%>>125=,=br=,=br=,=img>>UT2-PD2-5-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>>Tutorial Visual Model Comparison.rmp>>d"},{"unidad":"Preparaci\xf3n de datos","unit":"Data preparation","titulo":"Pre procesamiento de datos en Wine con Python","title":"Data pre-processing in Wine with Python","descripcion":"Para este caso se aplican t\xe9cnicas de pre procesamiento como normalizaci\xf3n, estandarizaci\xf3n y split en datos de training y test. Luego se obtienen estad\xedsticas para el dataset Wine utiliando funciones de Python con SciKitLearn y la librer\xeda Pandas.","description":"For this case, pre-processing techniques such as normalization, standardization and split are applied to training and test data. Statistics for the Wine dataset are then obtained using Python functions with SciKitLearn and the Pandas library.","contenido":"p>>- Se descarg\xf3 el dataset Wine.=,=p>>- Se imprimieron las columnas de las primeras 10 filas.=,=p>>- Se convirtieron los valores num\xe9ricos de string a float.=,=p>>- Se obtuvieron los valores m\xednimos y m\xe1ximos para cada columna.=,=p>>- Se obtuvo la media y la desviaci\xf3n est\xe1ndar de los valores de cada columna.=,=p>>- Se normalizaron y se estandarizaron los valores del dataset original.=,=p>>- Se dividi\xf3 el dataset en conjuntos de entrenamiento y testing.=,=br=,=h3>>C\xf3digo=,=code>>.from csv import reader=,=br=,=code>>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy=,=br=,=code>>.=,=br=,=code>>.columns = [\'Alcohol\',\'Malic acid\',\'Ash\',\'Alcalinity of ash\',\'Magnesium\',\'Total phenols\',\'Flavanoids\',\'Nonflavanoid phenols\',\'Proanthocyanins\',\'Color intensity\',\'Hue\',\'OD280/OD315 of diluted wines\',\'Proline\']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\xa0    dataset = list()=,=br=,=code>>.\xa0    with open(filename, \'r\') as file:=,=br=,=code>>.\xa0\xa0        csv_reader = reader(file)=,=br=,=code>>.\xa0\xa0        for row in csv_reader:=,=br=,=code>>.\xa0\xa0\xa0            if not row:=,=br=,=code>>.\xa0\xa0\xa0\xa0                continue=,=br=,=code>>.\xa0\xa0\xa0            dataset.append(row)=,=br=,=code>>.\xa0\xa0        return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\xa0    for row in dataset:=,=br=,=code>>.\xa0\xa0        row[column] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\xa0    minmax = list()=,=br=,=code>>.\xa0    for i in range(len(dataset[0])):=,=br=,=code>>.\xa0\xa0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\xa0\xa0        value_min = min(col_values)=,=br=,=code>>.\xa0\xa0        value_max = max(col_values)=,=br=,=code>>.\xa0\xa0        minmax.append([value_min, value_max])=,=br=,=code>>.\xa0    return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\xa0    means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\xa0    for i in range(len(dataset[0])):=,=br=,=code>>.\xa0\xa0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\xa0\xa0        means[i] = sum(col_values) / float(len(dataset))=,=br=,=code>>.\xa0    return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means):=,=br=,=code>>.\xa0    stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\xa0    for i in range(len(dataset[0])):=,=br=,=code>>.\xa0\xa0        variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\xa0\xa0        stdevs[i] = sum(variance)=,=br=,=code>>.\xa0\xa0        stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]=,=br=,=code>>.\xa0    return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\xa0    for row in dataset:=,=br=,=code>>.\xa0\xa0        for i in range(len(row)):=,=br=,=code>>.\xa0\xa0\xa0            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])=,=br=,=code>>.=,=br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\xa0    for row in dataset:=,=br=,=code>>.\xa0\xa0        for i in range(len(row)):=,=br=,=code>>.\xa0\xa0\xa0            row[i] = (row[i] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\xa0    train = list()=,=br=,=code>>.\xa0    train_size = split * len(dataset)=,=br=,=code>>.\xa0    dataset_copy = list(dataset)=,=br=,=code>>.\xa0    while len(train) < train_size:=,=br=,=code>>.\xa0\xa0        index = randrange(len(dataset_copy))=,=br=,=code>>.\xa0\xa0        train.append(dataset_copy.pop(index))=,=br=,=code>>.\xa0    return train, dataset_copy=,=br=,=code>>.=,=br=,=code>>.input_file = \\"wine.csv\\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print(\\"\\n- Primeras 10 filas:\\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\xa0    print(dataset[i])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(columns)+1):=,=br=,=code>>.\xa0    str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\\"\\n- Primeras 10 filas dataset con floats:\\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\xa0    print(strToFloatDataset[i])=,=br=,=code>>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\\"\\n- Estad\xedsticas: \\")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>>.\xa0    print(\\"*\\"+columns[i] + \\": min \\" + str(minmax[i][0]) + \\",  max \\" + str(minmax[i][1]) + \\", media \\" + str(means[i]) + \\", desviaci\xf3n est\xe1ndar \\" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.standarized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standarized_dataset, means, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\\"\\n- Primeras 10 filas dataset estandarizado:\\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\xa0    print(standarized_dataset[i])=,=br=,=code>>.print(\\"\\n- Primeras 10 filas dataset normalizado:\\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\xa0    print(normalized_dataset[i])=,=br=,=code>>.print(\\"\\n- Primeras 10 filas dataset de train:\\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\xa0    print(train[i])=,=br=,=code>>.print(\\"\\n- Primeras 10 filas dataset de train:\\")=,=br=,=code>>.\xa0for i in range(10):    =,=br=,=code>>.\xa0    print(test[i])=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD3-1.jpg>>100%>>300=,=img>>UT2-PD3-2.jpg>>100%>>300=,=img>>UT2-PD3-3.jpg>>100%>>300=,=img>>UT2-PD3-4.jpg>>100%>>600=,=img>>UT2-PD3-5.jpg>>100%>>600","content":"p>>- The Wine dataset was downloaded.=,=p>>- The columns of the first 10 rows were printed.=,=p>>- The numeric values \u200b\u200bwere converted from string to float.=,=p>> - The minimum and maximum values \u200b\u200bwere obtained for each column.=,=p>>- The mean and standard deviation of the values \u200b\u200bof each column were obtained.=,=p>>- The dataset values \u200b\u200bwere normalized and standardized original.=,=p>>- The dataset was divided into training and testing sets.=,=br=,=h3>>Code=,=code>>.from csv import reader=,=br=,=code >>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy =,=br=,=code>>.=,=br=,=code>>.columns = [\'Alcohol\',\'Malic acid\',\'Ash\',\'Alcalinity of ash\',\'Magnesium\',\'Total phenols\',\'Flavanoids\',\'Nonflavanoid phenols\',\'Proanthocyanins\',\'Color intensity\',\'Hue\',\'OD280/OD315 of diluted wines\',\'Proline\']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\xa0 dataset = list()=,=br=,=code>>.\xa0 with open(filename, \'r\') as file:=,=br=,=code>>.\xa0\xa0 csv_reader = reader(file)=,=br=,=code>>.\xa0\xa0 for row in csv_reader:=,=br=,=code>>.\xa0\xa0\xa0 if not row:=,=br=,=code>>.\xa0\xa0\xa0\xa0 continue=,=br=,=code>>.\xa0\xa0\xa0 dataset.append(row)=,=br=,=code>>.\xa0\xa0 return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\xa0 for row in dataset:=,=br=,=code>>.\xa0\xa0 row[column ] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\xa0 minmax = list()=,=br=,=code>>.\xa0 for i in range(len(dataset[0])):=,=br=,=code>>.\xa0\xa0 col_values \u200b\u200b= [row[i] for row in dataset]=,=br=,=code>>.\xa0\xa0 value_min = min(col_values)=,=br=,=code>>.\xa0\xa0 value_max = max(col_values)=,=br=,=code>>.\xa0\xa0 minmax.append([value_min, value_max])=,=br=,=code>>.\xa0 return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\xa0 means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\xa0 for i in range(len(dataset[0])):=,=br =,=code>>.\xa0\xa0 col_values \u200b\u200b= [row[i] for row in dataset]=,=br=,=code>>.\xa0\xa0 means[i] = sum(col_values) / float( len(dataset))=,=br=,=code>>.\xa0 return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means): =,=br=,=code>>.\xa0 stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\xa0 for i in range(len (dataset[0])):=,=br=,=code>>.\xa0\xa0 variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\xa0\xa0 stdevs[i] = sum(variance)=,=br=,=code>>.\xa0\xa0 stdevs = [sqrt(x/(float(len(dataset)-1 ))) for x in stdevs]=,=br=,=code>>.\xa0 return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\xa0 for row in dataset:=,=br=,=code>>.\xa0\xa0 for i in range(len(row)):=,= br=,=code>>.\xa0\xa0\xa0 row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0] )=,=br=,=code>>.=, =br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\xa0 for row in dataset:=, =br=,=code>>.\xa0\xa0 for i in range(len(row)):=,=br=,=code>>.\xa0\xa0\xa0 row[i] = (row[i ] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\xa0 train = list()=,=br=,=code>>.\xa0 train_size = split * len(dataset)=,=br=,=code>>.\xa0 dataset_copy = list( dataset)=,=br=,=code>>.\xa0 while len(train) < train_size:=,=br=,=code>>.\xa0\xa0 index = randrange(len(dataset_copy))=,= br=,=code>>.\xa0\xa0 train.append(dataset_copy.pop(index))=,=br=,=code>>.\xa0 return train, dataset_copy=,=br=,=code>> .=,=br=,=code>>.input_file = \\"wine.csv\\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print (\\"\\n- First 10 rows:\\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\xa0 print(dataset[i ])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(co lumns)+1):=,=br=,=code>>.\xa0 str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\\"\\n- First 10 dataset rows with floats: \\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\xa0 print(strToFloatDataset[i])=,=br=,=code >>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\\"\\n- Statistics: \\")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>> .\xa0 print(\\"*\\"+columns[i] + \\": min \\" + str(minmax[i][0]) + \\", max \\" + str(minmax[i][1] ) + \\", mean \\" + str(means[i]) + \\", standard deviation \\" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy( strToFloatDataset)=,=br=,=code>>.standardized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>> .normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standardized_dataset, m eans, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\\"\\n- First 10 rows standardized dataset:\\") =,=br=,=code>>.for i in range(10):=,=br=,=code>>.\xa0 print(standardized_dataset[i])=,=br=,=code>>. print(\\"\\n- First 10 rows normalized dataset:\\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\xa0 print( normalized_dataset[i])=,=br=,=code>>.print(\\"\\n- First 10 rows train dataset:\\")=,=br=,=code>>.for i in range(10 ):=,=br=,=code>>.\xa0 print(train[i])=,=br=,=code>>.print(\\"\\n- First 10 rows train dataset:\\") =,=br=,=code>>.\xa0for i in range(10): =,=br=,=code>>.\xa0 print(test[i])=,=br=,=br=,=h3>>Results=,=img>>UT2-PD3-1.jpg>>100%>>300=,=img>>UT2-PD3-2.jpg>>100%>>300=,=img>>UT2-PD3-3.jpg>>100%>>300=,=img>>UT2-PD3-4.jpg>>100%>>600=,=img>>UT2-PD3-5.jpg>>100%>>600"},{"unidad":"Preparaci\xf3n de datos","unit":"Data preparation","titulo":"Data pre processing y c\xe1lculo de probabilidades de sucesos sobre el dataset Titanic con Python","title":"Data pre-processing and calculation of probabilities of events on the Titanic dataset with Python","descripcion":"Sobre el dataset Titanic se realiz\xf3 un tratamiento de missing values con reemplazo, remoci\xf3n, sustituci\xf3n por categor\xeda \xfanica y selecci\xf3n de atributos. Posteriormente se realizaron consultas probabil\xedsticas sobre los datos.","description":"On the Titanic dataset, a missing value treatment was carried out with replacement, removal, substitution by single category and selection of attributes. Subsequently, probabilistic queries were made on the data.","contenido":"p>> - Se importaron librer\xedas y paquetes.=,=p>> - Se carg\xf3 el dataset y se los mostr\xf3.=,=p>> - Se gestionaron los datos faltantes.=,=p>> - Se graficaron los datos seg\xfan Age vs Survived y Fare vs Survived.=,=code>>.import pandas as pd=,=br=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.=,=br=,=code>>.input_file = \\"titanic.csv\\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.print(\\"-----DATASET------\\")=,=br=,=code>>.print(dataset.to_string())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\\"-----MISSSING VALUES------\\")=,=br=,=code>>.print(\\"*Antes de data pre-procesing\\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.#Se reemplazan los missing values de edad por la media=,=br=,=code>>.dataset[\\"age\\"].fillna(dataset[\\"age\\"].mean(), inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan las columnas body y cabin por altos porcentajes =,=br=,=code>>.# de missing values siendo estos 90.7% y 77% respectivamente=,=br=,=code>>.dataset.drop(labels = [\\"body\\", \\"cabin\\"], axis = 1, inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan la fila sin valor para fare y las dos filas=,=br=,=code>>.# Sin valor para embarked=,=br=,=code>>.dataset.dropna(subset=[\'fare\'], how=\'all\', inplace=True)=,=br=,=code>>.dataset.dropna(subset=[\'embarked\'], how=\'all\', inplace=True)=,=br=,=code>>.=,=br=,=code>>.#Se asigna una categor\xeda \xfanica para los missing values de boay y home.dest=,=br=,=code>>.dataset[\\"boat\\"].fillna(\'U\', inplace = True)=,=br=,=code>>.dataset[\\"home.dest\\"].fillna(\'U\', inplace = True)=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\\"*Despues de data pre-procesing\\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\\"-----GRAFICAS------\\")=,=br=,=code>>.print(\\"- x = Age, y = Survived\\")=,=br=,=code>>.print(\\"- x = Fare, y = Survived\\")=,=br=,=code>>.colors = (\\"red\\", \\"blue\\")=,=br=,=code>>.plt.scatter(dataset[\'age\'], dataset[\'survived\'], s=10, c=dataset[\'survived\'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.plt.scatter(dataset[\'fare\'], dataset[\'survived\'], s=10, c=dataset[\'survived\'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-1.jpg>>30%>>100=,=br=,=img>>UT2-PD4-2.jpg>>50%>>300=,=img>>UT2-PD4-3.jpg>>50%>>300=,=br=,=br=,=p>>- Se calcul\xf3 la probabilidad de que una persona sobreviva dados su sexo y clase de pasajero para luego aplicarlo a una serie de valores concretos.=,=p>> - Se calcul\xf3 la probabilidad de que un ni\xf1o de 10 a\xf1os o menos de 3ra clase sobreviva.=,=code>>.import pandas as pd=,=br=,=code>>.=,=br=,=code>>.input_file = \\"titanic.csv\\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.def probSgivenGandC(G, C):=,=br=,=code>>.\xa0\xa0\xa0\xa0    size = len(dataset)=,=br=,=code>>.\xa0\xa0\xa0\xa0    sizeGC = len(dataset[(dataset[\'sex\'] == G) & (dataset[\'pclass\'] == C)])=,=br=,=code>>.\xa0\xa0\xa0\xa0    sizeSGC = len(dataset[(dataset[\'sex\'] == G) & (dataset[\'pclass\'] == C) & (dataset[\'survived\'] == 1)])=,=br=,=code>>.\xa0\xa0\xa0\xa0    probGC = sizeGC/size=,=br=,=code>>.\xa0\xa0\xa0\xa0    probSGC = sizeSGC/size=,=br=,=code>>.\xa0\xa0\xa0\xa0    return probSGC/probGC=,=br=,=code>>.print(\\"\\n-Probabilidades:\\")=,=br=,=code>>.print(\\"*female, class 1: \\" + str(probSgivenGandC(\\"female\\",1)))=,=br=,=code>>.print(\\"*female, class 2: \\" + str(probSgivenGandC(\\"female\\",2)))=,=br=,=code>>.print(\\"*female, class 3: \\" + str(probSgivenGandC(\\"female\\",3)))=,=br=,=code>>.print(\\"*male, class 1: \\" + str(probSgivenGandC(\\"male\\",1)))=,=br=,=code>>.print(\\"*male, class 2: \\" + str(probSgivenGandC(\\"male\\",2)))=,=br=,=code>>.print(\\"*male, class 3: \\" + str(probSgivenGandC(\\"male\\",3)))=,=br=,=code>>.=,=br=,=code>>.def probSgivenMaxAandC(A, C):=,=br=,=code>>.\xa0\xa0\xa0\xa0    size = len(dataset)=,=br=,=code>>.\xa0\xa0\xa0\xa0    sizeMaxAC = len(dataset[(dataset[\'age\'] <= A) & (dataset[\'pclass\'] == C)])=,=br=,=code>>.\xa0\xa0\xa0\xa0    sizeSMaxAC = len(dataset[(dataset[\'age\'] <= A) & (dataset[\'pclass\'] == C) & (dataset[\'survived\'] == 1)])=,=br=,=code>>.\xa0\xa0\xa0\xa0    probMaxAC = sizeMaxAC/size=,=br=,=code>>.\xa0\xa0\xa0\xa0    probSMaxAC = sizeSMaxAC/size=,=br=,=code>>.\xa0\xa0\xa0\xa0    return probSMaxAC/probMaxAC=,=br=,=code>>.print(\\"*Less than 3yo, class 3: \\" + str(probSgivenMaxAandC(10,3)))=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-4.jpg>>60%>>200","content":"p>> - Libraries and packages were imported.=,=p>> - The dataset was loaded and displayed.=,=p>> - Missing data was managed.=,=p>> - Data was plotted according to Age vs Survived and Fare vs Survived.=,=code>>.import pandas as pd=,=br=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt =,=br=,=code>>.=,=br=,=code>>.input_file = \\"titanic.csv\\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.print(\\"-----DATASET------\\")=,=br=,=code>>.print(dataset.to_string())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>. print(\\"-----MISSSING VALUES------\\")=,=br=,=code>>.print(\\"*Before data pre-processing\\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.#Missing age values \u200b\u200bare replaced by mean=,=br=,=code>>.dataset[\\"age\\"].fillna(dataset[\\"age\\"].mean(), inplace = True)=,=br=,=code>>.=,=br=,=code>>.#The body and cabin columns are eliminated due to high percentages =,=br=,=code>>.# de missing va lues being these 90.7% and 77% respectively=,=br=,=code>>.dataset.drop(labels = [\\"body\\", \\"cabin\\"], axis = 1, inplace = True)=,=br=,=code>>.=,=br=,=code>>.#The row with no value for fare and the two rows are deleted=,=br=,=code>>.# With no value for embarked= ,=br=,=code>>.dataset.dropna(subset=[\'fare\'], how=\'all\', inplace=True)=,=br=,=code>>.dataset.dropna(subset=[ \'embarked\'], how=\'all\', inplace=True)=,=br=,=code>>.=,=br=,=code>>.#A unique category is assigned for the missing values \u200b\u200bof boay and home.dest=,=br=,=code>>.dataset[\\"boat\\"].fillna(\'U\', inplace = True)=,=br=,=code>>.dataset[\\"home. dest\\"].fillna(\'U\', inplace = True)=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>> .print(\\"*After data pre-processing\\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>. =,=br=,=code>>.print()=,=br=,=code>>.print(\\"-----GRAPHICS------\\")=,=br=,=code>>.print(\\"- x = Age, y = Survived\\")=,=br=,=code>>.print(\\"- x = Fare, y = Survived\\")=,=br =,=code>>.colors = (\\"red\\", \\"blue\\")=,=br=,=code>>.plt.scatter(dataset[\'age\'], dataset[\'survived\'], s=10, c=dataset[\'survived\'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.plt.scatter(dataset[\'fare\'], dataset[\'survived\'], s=10, c=dataset [\'survived\'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=br=,=h3>>Results=,=img>>UT2-PD4-1.jpg>>30%>>100=,=br=,=img>>UT2-PD4-2.jpg>>50%>>300=,=img>>UT2-PD4-3.jpg>>50%>>300=,=br=,=br=,=p>>- The probability of a person surviving given their gender and passenger class was calculated and then applied to a series of concrete values.=,=p>> - Calculated the probability of a 3rd class child aged 10 or younger surviving.=,=code>>.import pandas as pd=,=br=,=code>>. =,=br=,=code>>.input_file = \\"titanic.csv\\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.def probSgivenGandC(G, C):=,=br=,=code>>.\xa0\xa0\xa0\xa0 size = len(dataset)=,=br=,=code>>.\xa0\xa0\xa0\xa0 sizeGC = len(dataset[(dataset[\'sex\'] == G) & (dataset[\'pclass\'] == C)])=,=br=,=code>>.\xa0\xa0\xa0\xa0 sizeSGC = len(dataset[(dataset[\'sex\'] == G) & (dataset[\'pclass\'] == C) & (dataset[\'survived\'] == 1)])=,=br=,=code>>.\xa0\xa0\xa0\xa0 probGC = sizeGC/size=,=br=,=code>>.\xa0\xa0\xa0\xa0 probSGC = sizeSGC/size=,=br=,=code>>.\xa0\xa0\xa0\xa0 return probSGC/probGC=,=br=,=code>>.print(\\"\\n-Probabilities:\\")=,=br=,=code>>.print (\\"*female, class 1: \\" + str(probSgivenGandC(\\"female\\",1)))=,=br=,=code>>.print(\\"*female, class 2: \\" + str(probSgivenGandC(\\"female\\",2)))=,=br=,=code>>.print(\\"*female, class 3: \\" + str(probSgivenGandC(\\"female\\",3) ))=,=br=,=code>>.print(\\"*male, class 1: \\" + str(probSgivenGandC(\\"male\\",1)))=,=br=,=code>> .print(\\"*male, class 2: \\" + str(probSgivenGandC(\\"male\\",2)))=,=br=,=code>>.print(\\"*male, class 3: \\" + str(probSgivenGandC(\\"male\\",3)))=,=br=,=code>>.=,=br=,=code>>.def probSgivenMaxAandC(A, C):=,=br =,=code>>.\xa0\xa0\xa0\xa0 size = len(dataset)=,=br=,=code>>.\xa0\xa0\xa0\xa0 sizeMaxAC = len(dataset[(dataset[\'age\'] <= A) & (dataset[\'pclass\'] == C)])=,=br=,=code>>.\xa0\xa0\xa0\xa0 sizeSMaxAC = len(dataset[(dataset[\'age\'] <= A) & (dataset[\'pclass\'] == C) & (dataset[\'survived\'] == 1)])=,=br=,=code>>.\xa0\xa0\xa0\xa0 probMaxAC = sizeMaxAC/size=,=br=,=code>>.\xa0\xa0\xa0\xa0 probSMaxAC = sizeSMaxAC/size=,=br=,=code>>.\xa0\xa0\xa0\xa0 return probSMaxAC/ probMaxAC=,=br=,=code>>.print(\\"*Less than 3yo, class 3: \\" + str(probSgivenMaxAandC(10,3)))=,=br=,=br=,=h3>>Results=,=img>>UT2-PD4-4.jpg>>60%>>200"},{"unidad":"Preparaci\xf3n de datos","unit":"Data preparation","titulo":"Selecci\xf3n de atributos para dataset Sonar","title":"Selection of attributes for Sonar dataset","descripcion":"Ejercicio con selecci\xf3n de atributos en RapidMiner sobre el dataset title.","description":"Exercise with attribute selection in RapidMiner on the title dataset.","contenido":"p>>En la herramienta RapidMiner usaremos el dataset title y compararemos los resultados de performance al aplicar un simple algoritmo de Naive Bayes como l\xednea base, Forward Selection con Naive Bayes y Backward Elimination con Naive Bayes empleando cross validation con 5 folds, muestreo estratificado y la misma seed para los tres casos.=,=h4>>Modelo=,=img>>aa4.jpg>>80%>>600=,=br=,=a>>Descargar modelo>>UT4-TA9.rmp>>d=,=br=,=br=,=h4>>Gr\xe1ficas=,=p>>Se observa una tendencia en las frecuencias capturadas por el title. Vemos una amplitud y varianza clara en ambas clases. El problema que se observa a primera vista, es que las frecuencias se solapan demasiado, haciendo muy dif\xedcil su diferenciaci\xf3n.=,=img>>UT4-PD9-1.jpg>>80%>>400=,=br=,=br=,=img>>UT4-PD9-2.jpg>>80%>>400=,=br=,=br=,=h4>>Resultados=,=br=,=h6>>L\xednea base Naive Bayes=,=img>>UT4-PD9-3.jpg>>80%>>130=,=br=,=br=,=h6>>Forward Selection=,=img>>UT4-PD9-4.jpg>>80%>>130=,=p>>Mediante este algoritmo de feature selection se redujeron los atributos utilizados y la precisi\xf3n de predicciones para la clase mina mejor\xf3 sustancialmente, esto quiere decir que usando solo los atributos relevantes, que representan una mayor variaci\xf3n en la informaci\xf3n la precisi\xf3n aument\xf3. Los atributos elegidos fueron los 12, 15, 17 y 18.=,=h6>>Backward Elimination=,=img>>UT4-PD9-5.jpg>>80%>>130=,=p>>Este algoritmo de feature selection elimin\xf3 8 atributos y con ello se mejor\xf3 la performance respecto a la l\xednea base al igual que en el caso anterior.","content":"p>>In the RapidMiner tool we will use the title dataset and compare the performance results by applying a simple Naive Bayes algorithm as a baseline, Forward Selection with Naive Bayes and Backward Elimination with Naive Bayes using cross validation with 5 folds, stratified sampling and the same seed for all three cases.=,=h4>>Model=,=img>>aa4.jpg>>80%>>600=,=br=,=a>>Download model>>UT4-TA9.rmp>>d=,=br=,=br=,=h4>>Graphs=,=p>>A trend is observed in the frequencies captured by the title. We see a clear amplitude and variance in both classes. The problem that can be seen at first sight is that the frequencies overlap too much, making their differentiation very difficult.=,=img>>UT4-PD9-1.jpg>>80%>>400=,=br=,= br=,=img>>UT4-PD9-2.jpg>>80%>>400=,=br=,=br=,=h4>>Results=,=br=,=h6>>Naive Bayes Baseline =,=img>>UT4-PD9-3.jpg>>80%>>130=,=br=,=br=,=h6>>Forward Selection=,=img>>UT4-PD9-4.jpg>>80%>>130=,=p>>Through this feature selection algorithm, the attributes used were reduced and the accuracy of predictions for the mine class improved substantially, this means that using only the relevant attributes, which represent a greater variation in the information the precision increased. The chosen attributes were 12, 15, 17 and 18.=,=h6>>Backward Elimination=,=img>>UT4-PD9-5.jpg>>80%>>130=,=p>>This algorithm of feature selection removed 8 attributes and thereby improved performance compared to the baseline as in the previous case."},{"unidad":"Algoritmos lineales","unit":"Linear algorithms","titulo":"Regresi\xf3n lineal simple en hoja de c\xe1lculo","title":"Simple Linear Regression in spreadsheet","descripcion":"Utilizando una planilla electr\xf3nica se gener\xf3 un modelo de regresi\xf3n linear simple calculando los coeficientes en el primer ejercicio y por descenso de gradiente en el segundo ejercicio. Para ambos casos se usaron los modelos para hacer predicciones.","description":"Using an electronic spreadsheet, a simple linear regression model was generated by calculating the coefficients in the first exercise and by gradient descent in the second exercise. For both cases the models were used to make predictions.","contenido":"p>>La regresi\xf3n lineal es el algoritmo de machine learning mejor entendido consistiendo en una representaci\xf3n lineal entre las entradas x y la salida y para calcular predicciones en casos supervisados de regresi\xf3n.=,=p>>Cuando hay una sola variable de entrada se llama regresi\xf3n lineal simple teniendo una forma de tipo y = B0 + B1 x X y cuando hay m\xe1s lleva el nombre de regresi\xf3n lineal m\xfaltiple siendo por ejemplo y = B0 + B1 x X1 + B2 x X2 para el caso de dos predictores.=,=p>>El entrenamiento de modelos se basa en hallar los coeficientes que mejor aproximen una relaci\xf3n lineal entre las entradas y la salida y suele consistir en utilizar ecuaciones estad\xedsticas para el caso simple y m\xednimos cuadrodos ordinarios (ordinary least squares) o descenso de gradiente si hay m\xfaltiples predictores. El descenso de gradiente optimiza los coeficientes de la relaci\xf3n lineal iterando sobre los datos de entrenamiento para minimizar el error del modelo e ir aproxim\xe1ndo los par\xe1metros cada vez mejor seg\xfan un par\xe1metro de aprendizaje alfa. Los coeficientes comienzan valiendo 0 en la primer iteraci\xf3n.=,=p>>Para realizar predicciones con un modelo de regresi\xf3n lineal basta con resolver la ecuaci\xf3n con los datos de entrada utilizando los coeficientes hallados en el entrenamiento.=,=p>>Es importante mencionar que se requieren muchos pasos para preparar los datos en el caso de la regresi\xf3n lineal convertir todas las variables nominales a num\xe9ricas en caso de tenerlas, valorar si existe una relaci\xf3n lineal entre las entradas y la salida, remover las variables de entrada correlacionadas entre s\xed, observar si hay distribuciones gaussianas entre las variables y considerar utilizar transformaciones logar\xedtmicas o box cox y reescalar las entradas por estandarizaci\xf3n o normalizaci\xf3n. =,=p>>Para mostrar estos conceptos de forma practica comenzaremos hallando los coeficientes estad\xedsticamente para el caso de una regresi\xf3n lineal simple con los siguientes valores de entrada.=,=img>>aa8.jpg>>100%>>500=,=br=,=br=,=p>>- Se utilizar\xe1n las f\xf3rmulas:=,=img>>UT3-PD1-1.jpg>>70%>>100=,=br=,=br=,=img>>UT3-PD1-2.jpg>>70%>>50=,=br=,=br=,=p>>Los valores obtenidos fueron B0 = 1.466667 y B1 = 0.342857.=,=p>>Por otra parte, se realiz\xf3 un m\xe9todo alternativo con la siguiente ecuaci\xf3n para calcular B1 y B0 a partir de este al igual que antes llegando a los mismos valores.=,=img>>UT3-PD1-4.jpg>>70%>250 =,=p>> =,=p>> -Se aplic\xf3 el modelo a los valores de entrenamiento.=,=img>>aa9.jpg>>25%>>200=,=br=,=br=,=p>>-Se estim\xf3 el error de predicci\xf3n RMSE llegando a un valor de 1.101 con esta ecuaci\xf3n:=,=img>>UT3-PD1-3.jpg>>30%>>100=,=br=,=br=,=p>> -Se a\xf1adi\xf3 una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo hallado a los valores de la anterior. A partir de estas se hizo un gr\xe1fico para mostrar la recta de ajuste.=,=img>>aa10.jpg>>75%>>600=,=br=,=br=,=p>>Ahora nos enfocaremos en mostrar como se realiza descenso de gradiente para un ejemplo practico tambi\xe9n de regresi\xf3n lineal simple usando los mismos datos para X e Y que en el caso anterior.=,=p>>-Se estimaron los coeficientes del modelo simple de regresi\xf3n lineal Y = B0 + B1 x X realizando 24 iteraciones. En un principio los coeficientes B0 y B1 comienzan con 0 y luego para cada \xe9poca estos se modifican seg\xfan los datos de la \xe9poca anterior de esta manera:=,=p>> B0 = B0 de la \xe9poca anterior - alfa * error de predicci\xf3n \xe9poca anterior=,=p>>B1 = B1 de la \xe9poca anterior - alfa * error de predicci\xf3n \xe9poca anterior=,=p>>Usando alfa = 0.01 tras todas las iteraciones se lleg\xf6 a los valores B0 = 0.21781 y B1 = 0.6382.=,=p>>-Se aplic\xf3 el modelo hallado a los valores de entrada obteniendo las siguientes cifras de predicci\xf3n.=,=img>>aa11.jpg>>25%>>200.=,=p>>-Calculando el error medio cuadr\xe1tico RMSE empleando la misma f\xf3rmula que en el caso anterior se lleg\xf3 a RMSE = 1.230204 =,=p>>-Se realiz\xf3 el gr\xe1fico error de predicci\xf3n contra iteraciones.=,=img>>aa12.jpg>>60%>>400=,=br=,=br=,=p>>- Nuevamente se a\xf1adi\xf3 una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo a las entradas.=,=img>>aa13.jpg>>75%>>600=,=br=,=br=,=p>>Comparando los dos m\xe9todos para llegar al modelo de regresi\xf3n lineal simple se observa que en el primero se obtuvo un menor error RMSE y que la segunda tiene una pendiente mayor al anterior.=,=p>>La hoja de trabajo donde se realizaron los dos ejercicios explicados anteriormente y en la que se encuentran los resultados correspondientes puede descargarla en el siguiente enlace =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD1.xlsx>>d","content":"p>>Linear regression is the best understood machine learning algorithm consisting of a linear representation between the inputs x and the output y to compute predictions in supervised regression cases.=,=p>>When there is only one input variable it is called simple linear regression having a form of type y = B0 + B1 x X and when there are more it is called multiple linear regression being for example y = B0 + B1 x X1 + B2 x X2 for the case of two predictors.=,= p>>Model training is based on finding the coefficients that best approximate a linear relationship between the inputs and the output and usually consists of using statistical equations for the simple case and ordinary least squares or gradient descent if there are multiple predictors. Gradient descent optimizes the coefficients of the linear relationship by iterating over the training data to minimize the model error and approximate the parameters better and better according to a learning parameter alpha. The coefficients start with a value of 0 in the first iteration.=,=p>>To make predictions with a linear regression model, it is enough to solve the equation with the input data using the coefficients found in the training.=,=p>>It is It is important to mention that many steps are required to prepare the data in the case of linear regression, convert all the nominal variables to numeric ones if they exist, assess if there is a linear relationship between the inputs and the output, remove the input variables correlated between yes, observe if there are Gaussian distributions between the variables and consider using logarithmic or box cox transformations and rescaling the inputs by standardization or normalization. =,=p>>To show these concepts in a practical way we will start by finding the coefficients statistically for the case of a simple linear regression with the following input values.=,=img>>aa8.jpg>>100%>>500=,=br=,=br=,=p>>- The formulas will be used:=,=img>>UT3-PD1-1.jpg>>70%>>100=,=br=,=br=,= img>>UT3-PD1-2.jpg>>70%>>50=,=br=,=br=,=p>>The values \u200b\u200bobtained were B0 = 1.466667 and B1 = 0.342857.=,=p>>By On the other hand, an alternative method was carried out with the following equation to calculate B1 and B0 from this as before, arriving at the same values.=,=img>>UT3-PD1-4.jpg>>70%>250 =,=p>> =,=p>> -The model was applied to the training values.=,=img>>aa9.jpg>>25%>>200=,=br=,=br=,= p>>-The RMSE prediction error was estimated reaching a value of 1.101 with this equation:=,=img>>UT3-PD1-3.jpg>>30%>>100=,=br=,=br=,=p>> -A column was added with values \u200b\u200bof x between 0 and 8 with step 0.1 and another with the result of applying the model found to the values \u200b\u200bof the previous one. From these, a graph was made to show the fit line.=,=img>>aa10.jpg>>75%>>600=,=br=,=br=,=p>>Now we will focus on showing how gradient descent is performed for a practical example also of simple linear regression using the same data for X and Y as in the previous case.=,=p>>-The coefficients of the simple linear regression model Y = B0 + were estimated B1 x X performing 24 iterations. Initially the coefficients B0 and B1 start with 0 and then for each epoch these are modified according to the data of the previous epoch in this way:=,=p>> B0 = B0 of the previous epoch - alpha * epoch prediction error previous=,=p>>B1 = B1 of the previous epoch - alpha * prediction error previous epoch=,=p>>Using alpha = 0.01 after all the iterations, the values \u200b\u200bB0 = 0.21781 and B1 = 0.6382 were reached.=,=p>>-The model found was applied to the input values, obtaining the following prediction figures.=,=img>>aa11.jpg>>25%>>200.=,=p>>-Calculating the error mean square RMSE using the same formula as in the previous case, RMSE = 1.230204 was reached =,=p>>-The prediction error graph was made against iterations.=,=img>>aa12.jpg>>60%>> 400=,=br=,=br=,=p>>- Again, a column with values \u200b\u200bof x between 0 and 8 was added with step 0.1 and another with the result of applying the model to the inputs.=,=img>>aa13.jpg>>75%>>600=,=br=,=br=,=p>>Comparing the two methods to arrive to the simple linear regression model, it is observed that in the first one a lower RMSE error was obtained and that the second has a greater slope than the previous one.=,=p>>The worksheet where the two exercises explained above were carried out and in the corresponding results can be downloaded at the following link =,=a>>Download worksheet>>Linear algorithms PD1.xlsx>>d"},{"unidad":"Algoritmos lineales","unit":"Linear algorithms","titulo":"Regresi\xf3n log\xedstica en planilla electr\xf3nica","title":"Logistic Regression in spreadsheet","descripcion":"En una planilla electr\xf3nica se visualizar\xe1 una funci\xf3n log\xedstica y se genarar\xe1n modelos de regresi\xf3n log\xedstica con descenso de gradiente estoc\xe1stico.","description":"A logistic function will be visualized in an electronic spreadsheet and logistic regression models will be generated with stochastic gradient descent.","contenido":"p>>La reges\xed\xf3n log\xedstica es un algoritmo que resuelve problemas supervisados de clasificaci\xf3n binaria en base a la funci\xf3n log\xedstica la cual tiene la siguiente forma y gr\xe1fica.=,=img>>aaa1.jpg>>250>>100=,=br=,=br=,=img>>aaa2.jpg>>75%>>375.=,=p>>En el caso de la regresi\xf3n lineal se utilza la siguiente funci\xf3n de tipo log\xedstica..=,=img>>a1.jpg>>250>>100=,=br=,=br=,=p>>A partir de esta se entrena hallando los valores de los coeficientes con descenso de gradiente estoc\xe1stico y tras ello se realizan predicciones reemplazando los valoes en la funci\xf3n obtenida. Siempre los resultados obtenidos estar\xe1n en el rango (0,1) por lo que definiendo un cierto valor en ese rango se determinan dos subrangos en los que a partir del resultado obtenido de la funci\xf3n evaluada con ciertos datos de entrada entrada se los clasifica como de una clase u otra.=,=p>>Para aplicar regresi\xf3n log\xedstica es conveniente remover outliers, tener distribuciones gaussianas para lo que puede contribuir realizar transformar logar\xedtmicas o box cox y quitar entradas correlacionadas.=,=p>>En esta ocasi\xf6n veremos un ejercicio pr\xe1ctico en el que se generar\xe1 un modelo de regresi\xf3n log\xedstica con el que realizar predicciones a partir del siguiente conjunto de datos.=,=img>>a2.jpg>>30%>>400=,=br=,=br=,=p>>Se estimaron los coeficientes del modelo de regresi\xf3n log\xedstica Y = B0+B1xX1+B2xX2 con el algoritmo de descenso de gradiente estoc\xe1stico utilizando un coeficiente alpha = 0.3 y repitiendo el proceso para 10 \xe9pocas lo que equivale a hacer 10 iteraciones sobre cada ejemplo del dataset.=,=p>>Para ver c\xf3mo van mejoran las predicciones al aumentar las \xe9pocas se realizaron las gr\xe1ficas de exactitud vs \xe9pocas y RMSE VS \xe9pocas gener\xe1ndose las siguientes visualizaciones.=,=img>>a3.jpg>>100%>>300=,=br=,=br=,=p>>La hoja de trabajo donde se realizaron los procedimientos anteriores puede ser descargada en el siguiente enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD2.xlsx>>d","content":"p>>Logistic regestion is an algorithm that solves supervised binary classification problems based on to the logistic function which has the following form and graph.=,=img>>aaa1.jpg>>250>>100=,=br=,=br=,=img>>aaa2.jpg>>75%>>375.=,=p>>In the case of linear regression, the following logistic type function is used..=,=img>>a1.jpg>>250>>100=,=br=,=br=,=p>>From this, it is trained by finding the values \u200b\u200bof the coefficients with stochastic gradient descent and after that, predictions are made by replacing the values \u200b\u200bin the function obtained. The results obtained will always be in the range (0,1), so by defining a certain value in that range, two subranges are determined in which, based on the result obtained from the function evaluated with certain input data, they are classified as of one class or another.=,=p>>To apply logistic regression it is convenient to remove outliers, to have Gaussian distributions for which it can contribute to carry out logarithmic or box cox transforms and remove correlated entries.=,=p>>This time we will see a practical exercise in which a logistic regression model will be generated with which to make predictions from the following data set.=,=img>>a2.jpg>>30%>>400=,=br=,=br=,=p>>The coefficients of the logistic regression model Y = B0+B1xX1+B2xX2 were estimated with the stochastic gradient descent algorithm using an alpha coefficient = 0.3 and repeating the process for 10 epochs, which is equivalent to doing 10 iterations on each example of the dataset.=,=p>>To see how the predictions improve as the epochs increase, the graphs of accuracy vs. epochs and RMSE vs. epochs were made, generating the following visualizations.=,=img>>a3.jpg>>100%>>300=,=br=,=br=,=p>>The worksheet where the previous procedures were carried out can be downloaded at the following link: =,=a>>Download worksheet>>Linear algorithms PD2.xlsx>>d"},{"unidad":"Algoritmos lineales","unit":"Linear algorithms","titulo":"LDA en hoja de c\xe1lculo","title":"LDA in spreadsheet","descripcion":"Empleando una planilla electr\xf3nica se generar\xe1 un modelo de an\xe1lisis de discriminante lineal LDA y con \xe9l se har\xe1 la predicci\xf3n de clase para un conjunto de datos con un atributo y una salida con dos clases.","description":"Using an electronic spreadsheet, an LDA linear discriminant analysis model will be generated and with it the class prediction will be made for a data set with one attribute and an output with two classes.","contenido":"p>>El algoritmo de Linear Discriminant analysis se utiliza para problemas de clasificaci\xf3n sin tener la restricci\xf3n de ser solo para variables nominales como ocurria en Logistic Regression. Este modelo casume que los datos tienen distribuci\xf3n gaussiana y que los atributos tienen la misma varianza.=,=p>>Asumiendo los puntos anteriores LDA estima la media y la varianza para cada clase utilizando las siguientes ecuaci\xf3nes.=,=img>>a4.jpg>>30%>>90=,=img>>a5.jpg>>40%>>90=,=p>>LDA hace predicciones estimando las probabilidades de que un input pertenezca a una clase utilizando el discriminante de las diferentes clases cuya teor\xeda proviene del teorema de Bayes y su f\xf3rmula para la clase k se encuentra a continuaci\xf3n.=,=img>>a6.jpg>>60%>>100=,=p>>Siendo P(k) el ratio de instancias con esa clase en el dataset.=,=p>>Finalmente, para un ejemplo se le designa la clase cuyo discriminante de un valor m\xe1s alto.=,=p>>A partir de la teor\xeda anterior se procede a realizar un ejemplo practico que se muestra contiguamente.=,=p>>- Se insertaron los valores de un dataset con variables num\xe9ricas X para la entrada e Y para la salida. =,=p>> - Se graficaron los datos separando seg\xfan las clases de salida.=,=img>>a7.jpg>>60%>>400=,=br=,=br=,=p>> - Se calcularon, siendo n el n\xfamero de clases, las medias para cada clase k y la varianza de x con las ecuaciones que se mostraron anteriormente.=,=p>> - Se calcul\xf3 la predicci\xf3n de clases usando: =,=img>>UT3-PD3-4.jpg>>100%>200 =,=p>> - Se utiliz\xf3 la ecuaci\xf3n para calcular los discriminantes de x para ambas clases Y = 0 e Y = 1 y finalmente se realizaron las predicciones de clase para los datos comparando los discriminantes. La exactitud alcanzada por el modelo generado fue calculada dando un valor de 0.65.=,=p>>La hoja de trabajo donde se hizo la pr\xe1ctica puede descargarse en este enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD3.xlsx>>d","content":"p>>The Linear Discriminant analysis algorithm is used for classification problems without the restriction of being only for nominal variables as was the case in Logistic Regression. This model assumes that the data have a Gaussian distribution and that the attributes have the same variance.=,=p>>Assuming the previous points, LDA estimates the mean and variance for each class using the following equations.=,=img>>a4.jpg>>30%>>90=,=img>>a5.jpg>>40%>>90=,=p>>LDA makes predictions by estimating the probabilities that an input belongs to a class using the discriminant of the different classes whose theory comes from Bayes theorem and its formula for class k is found below.=,=img>>a6.jpg>>60%>>100=,=p>>where P(k) is the ratio of instances with that class in the dataset.=,=p>>Finally, for an example, the class whose discriminant value is higher is designated.=,=p>>Based on the previous theory, a practical example shown next.=,=p>>- The values \u200b\u200bof a dataset with numeric variables X for input and Y for output were inserted. =,=p>> - The data was plotted separating according to the output classes.=,=img>>a7.jpg>>60%>>400=,=br=,=br=,=p>> - It calculated, where n is the number of classes, the means for each class k and the variance of x with the equations shown above.=,=p>> - Class prediction was calculated using: =,=img>>UT3-PD3-4.jpg>>100%>200 =,=p>> - The equation was used to calculate the discriminants of x for both classes Y = 0 and Y = 1 and finally class predictions were made for the data comparing the discriminants. The accuracy achieved by the generated model was calculated giving a value of 0.65.=,=p>>The worksheet where the practice was done can be downloaded at this link: =,=a>>Download worksheet>>Linear algorithms PD3.xlsx>>d"},{"unidad":"Algoritmos lineales","unit":"Linear algorithms","titulo":"Regresi\xf3n lineal con descenso de gradiente","title":"Comparaci\xf3n de LDA con Python y RapidMiner para el dataset Sports","descripcion":"En este ejercicio se usa una planilla electr\xf3nica para minimizar una funci\xf3n siguiendo los gradientes de la funci\xf3n de costo y luego armar un modelo de regresi\xf3n lineal.","description":"Se realizan ejercicios con modelos de LDA para en Python y RapidMiner y se comparan los resultados obtenidos.","contenido":"p>>- Se represent\xf3 en la planilla un dataset con una variable de entrada X y una de salida Y.=,=p>> - Se graficaron los datos. =,=img>>UT3-PD4-1.jpg>>100%>400=,=p>> - Se realiz\xf3 el procedimiento de descenso de gradiente con alpha=0.01 en 24 iteraciones hallando valores para los coeficientes B0 y B1 que en una regresi\xf3n lineal siguen la relaci\xf3n Y = B1xX + B0, la predicci\xf3n para dicho modelo y el error de predicci\xf3n. =,=p>> - Se grafic\xf3 el error de predicci\xf3n vs iteraciones. =,=img>>UT3-PD4-2.jpg>>100%>400=,=p>> - Se calcul\xf3 el error medio cuadr\xe1tico RMSE. =,=p>> - Se generaron nuevos valores de entrada para X entre 0 y 8 con un paso 0.1 y se predijo su valor de Y a partir de un modelo de regresi\xf3n lineal usando los coeficientes hallados al finalizar el procedimento mencionado anteriormente. El resultado fue graficado. =,=img>>UT3-PD4-3.jpg>>100%>400 =,=p>> - Se analizaron los datos de entrada desde la \xf3ptica de los requerimientos para aplicar un m\xe9todo de regresi\xf3n lineal.=,=br=,=h3>>Resultados=,=p>> La planilla de trabajo donde se realizaron los pasos descriptos anteriormente se encuentra accesible para la descarga con el enlace que se encuentra a continuaci\xf3n:=,=a>>Descargar plantilla>>Algoritmos lineales PD4.xlsx>>d","content":"p>>- A dataset with an input variable X and an output variable Y was represented on the spreadsheet.=,=p>> - The data was graphed. =,=img>>UT3-PD4-1.jpg>>100%>400=,=p>> - The gradient descent procedure was performed with alpha=0.01 in 24 iterations, finding values \u200b\u200bfor the coefficients B0 and B1 that in a linear regression they follow the relation Y = B1xX + B0, the prediction for said model and the prediction error. =,=p>> - The prediction error vs iterations was plotted. =,=img>>UT3-PD4-2.jpg>>100%>400=,=p>> - RMSE mean square error was calculated. =,=p>> - New input values \u200b\u200bfor X between 0 and 8 were generated with a step of 0.1 and its Y value was predicted from a linear regression model using the coefficients found at the end of the procedure mentioned above. The result was graphed. =,=img>>UT3-PD4-3.jpg>>100%>400 =,=p>> - The input data was analyzed from the perspective of the requirements to apply a linear regression method.=,=br =,=h3>>Results=,=p>> The worksheet where the steps described above were carried out is accessible for download with the link below:=,=a>>Download template>>Algorithms linear PD4.xlsx>>d"},{"unidad":"Algoritmos lineales","unit":"Linear algorithms","titulo":"Comparaci\xf3n de LDA con Python y RapidMiner para el dataset Sports","descripcion":"Utilizaremos An\xe1lisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificaci\xf3n a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.","contenido":"h2>>Ejercicio 1=,=p>>- Se descarg\xf3 un dataset de prueba Sample.csv.=,=p>>- Se ley\xf3 el archivo CSV y se graficaron los datos utilizando pyplot de matplotlib.=,=p>>- Se dividi\xf3 el conjunto de datos en una parte de entrenamiento y otra de prueba.=,=p>>- Se cre\xf3 y entren\xf3 un modelo LDA.=,=p>>- Se predijeron las clases para los datos de prueba y se compararon los resultados.=,=p>>- Se imprimi\xf3 el reporte de clasificaci\xf3n y la matriz de confusi\xf3n=,=code>>.=,=br=,=code>>.input_file = \\"sample.csv\\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br=,=code>>.=,=br=,=code>>.colors = (\\"orange\\", \\"blue\\")=,=br=,=code>>.plt.scatter(df[\'x\'], df[\'y\'], s=300, c=df[\'label\'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.=,=br=,=code>>.X = df[[\'x\', \'y\']].values=,=br=,=code>>.y = df[\'label\'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state=0, shuffle=True)   =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\\"Predicted vs Expected\\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br=,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br=,=h3>>Resultados=,=img>>UT3-PD5-1.jpg>>100%>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br=,=br=,=h2>>Ejercicio 2=,=p>>- Se descarg\xf3 el dataset sports_Training.csv.=,=p>>- Se eliminaron las filas con atributo CapacidadDecision menores de 3 y mayores a 100. =,=p>>- Se transformaron los atributos string a n\xfameros. =,=p>> - Se utiliz\xf3 el modelo para clasificar los datos del archivo sports_Scoring.csv.=,=p>>- Se realiz\xf3 el procedimiento equivalente en la herramienta RapidMiner.=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = [\'Edad\',\'Fuerza\',\'Velocidad\',\'Lesiones\',\'Vision\',\'Resistencia\',\'Agilidad\',\'CapacidadDecision\']=,=br=,=code>>.=,=br=,=code>>.input_file = \\"sports_Training.csv\\"=,=br=,=code>>.input_file2 = \\"sports_Scoring.csv\\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training[\'CapacidadDecision\'] >= 3) & (data_training[\'CapacidadDecision\'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring[\'CapacidadDecision\'] >= 3) & (data_scoring[\'CapacidadDecision\'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training[\'DeportePrimario\'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({\'Prediccion\': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter(\'Prediccion.xlsx\', engine=\'xlsxwriter\')=,=br=,=code>>.df.to_excel(writer, sheet_name=\'Sheet1\')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Esquema proceso RapidMiner=,=img>>UT3-PD5-3.jpg>>100%>>300=,=br=,=a>>Descargar proceso>>Algoritmos Lineales PD5.rmp>>d=,=br=,=br=,=h3>>Comparaci\xf3n de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicci\xf3n iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar plantilla comparativa>>Comparaci\xf3n Rapid Miner Sci Kit Learn.xlsx>>d","content":"h2>>Exercise 1=,=p>>- Downloaded a test dataset Sample.csv.=,=p>>- Read the CSV file and plotted the data using pyplot from matplotlib .=,=p>>- The data set was divided into a training part and a test part.=,=p>>- An LDA model was created and trained.=,=p>>- The classes were predicted for the test data and the results were compared.=,=p>>- The classification report and the confusion matrix were printed=,=code>>.=,=br=,=code>>.input_file = \\"sample.csv\\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br =,=code e>>.=,=br=,=code>>.colors = (\\"orange\\", \\"blue\\")=,=br=,=code>>.plt.scatter(df[\'x\' ], df[\'y\'], s=300, c=df[\'label\'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()= ,=br=,=code>>.=,=br=,=code>>.X = df[[\'x\', \'y\']].values=,=br=,=code>>.y = df[\'label\'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state= 0, shuffle=True) =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit( train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\\"Predicted vs Expected\\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br =,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br= ,=h3>>Results=,=img>>UT3-PD5-1.jpg>>100%>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br= ,=br=,=h2>>Exercise 2=,=p>>- The sports_Training.csv dataset was downloaded.=,=p>>- S The rows with the CapacityDecision attribute less than 3 and greater than 100 were eliminated. =,=p>>- The string attributes were transformed into numbers. =,=p>> - The model was used to classify the data in the file sports_Scoring.csv.=,=p>>- The equivalent procedure was carried out in the RapidMiner tool.=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=, =br=,=code>>.labels = [\'Age\',\'Strength\',\'Speed\',\'Injuries\',\'Vision\',\'Endurance\',\'Agility\',\'DecisionAbility\']=,=br=, =code>>.=,=br=,=code>>.input_file = \\"sports_Training.csv\\"=,=br=,=code>>.input_file2 = \\"sports_Scoring.csv\\"=,=br= ,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code> >.=,=br=,=code>>.data_training = data_training[(data_training[\'DecisionCapacity\'] >= 3) & (data_training[\'DecisionCapacity\'] <= 100)]=,=br=,=code> >.data_scoring = data_scoring[(data_scoring[\'DecisionCapacity\'] >= 3) & (data_scoring[\'DecisionCapacity\'] <= 100)]= ,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels] )=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training[\'PrimarySport\'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring [labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({\'Prediction\': y_pred})= ,=br=,=code>>.writer = pd.ExcelWriter(\'Prediction.xlsx\', engine=\'xlsxwriter\')=,=br=,=code>>.df.to_excel(writer, sheet_name=\'Sheet1\' )=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>RapidMiner process scheme=,=img>>UT3-PD5-3.jpg>>100 %>>300=,=br=,=a>>Download process>>Linear Algorithms PD5.rmp>>d=,=br=,=br=,=h3>>Comparison of results between Scikitlearn and RapidMiner=,= p>>98.81% of values \u200b\u200bwere obtained of prediction the same for the models made with Sckitlearn and RapidMiner.=,=a>>Download comparative template>>Comparison Rapid Miner Sci Kit Learn.xlsx>>d"},{"unidad":"Algoritmos lineales","unit":"Linear algorithms","titulo":"Titanic","title":"Titanic dataset","descripcion":"Se realizar\xe1 un an\xe1lisis del problema y de los datos del dataset Titanic para a partir de este crear un modelo de regresi\xf3n log\xedstica con la herramienta RapidMiner y otro equivalente con SciKitLearn, medir sus performances y compararlas.","description":"In this case, a data preparation study is done for the Titanic dataset and equivalent logistic regression models are created for RapidMiner and SciKitLearn.","contenido":"h2>>Problema=,=p>>A partir de datos respecto al hundimiento de Titanic se pretende predecir si una persona con determinadas caracter\xedsticas sobrevivi\xf3 o no por lo cual es un problema supervisado de clasificaci\xf3n binaria. Para este se propone una soluci\xf3n con un algoritmo lineal de regresi\xf3n log\xedstica utilizando CrossValidation para el testeo de su performance.=,=h2>>An\xe1lisis de datos=,=p>>El dataset cuenta con 13 atributos de entrada m\xe1s la variable objetivo \\"survived\\" para los cuales se cuenta con 1310 observaciones.=,=h4>>Atributos=,=p>>- pclass: Clase del pasajero. Tipo integer.=,=p>>- name. Nombre del pasajero. Tipo string.=,=p>>- sex. Sexo del viajante. Tipo string.=,=p>>- age. Edad de la persona. Tipo real. Hay 263 datos faltantes para esta variable.=,=p>>- sibsp: N\xfamero de hermanos a bordo. Tipo entero.=,=p>>- parch: N\xfamero de padres a bordo. Tipo entero.=,=p>>- ticket: C\xf3digo del ticket del pasajero. Tipo string.=,=p>>- fare: Tarifa pagada. Tipo real. Hay un dato faltante para esta entrada.=,=p>>- cabin. Tipo string. Cabina del viajero. Tipo string. Tiene 1014 missing values. =,=p>>- embarked: Puerto en el que embarc\xf3. Tipo string. Faltan dos de estos datos.=,=p>>- boat: C\xf3digo del bote salvavida que us\xf3 la persona. Tipo string. Cuenta con 823 missing values.=,=p>>- body: N\xfamero de cuerpo si falleci\xf3. Tipo integer. Cuenta con 1188 missing values. =,=p>>- home.dest: Lugar de destino. Tipo string. Tiene 564 missing values.=,=p>>- survived: Si sobrevivi\xf3 o no. Tipo integer.=,=br=,=h3>>Preparaci\xf3n de datos=,=br=,=h5>>Missing Values=,=p>>Entre las entradas hay variables con alto porcentaje de datos faltantes por lo que no se cont\xf3 con ellas siendo estas body con un 90.7% y cabin con un 77.4%, para la variable age se reemplazaron los valores faltantes por el promedio, se eliminaron las filas con missing values para fare y embarked y se le asign\xf3 una categor\xeda \xfanica para los missing values de boat y home.dest. Luego de todo este proceso se eliminaron todos los missing values.=,=h5>>Outliers=,=p>>El algoritmo de regresi\xf3n log\xedstica es altamente influenciable a los outliers y por lo tanto tras observar los histogramas de los datos para los distintos par\xe1metros, que se encuentran a continuaci\xf3n, se eliminaron aquellos con una alguna/s de las siguientes condiciones age>=70, sibsp>=6, parch>=6 o fare>=350.=,=img>>caso-2-2.jpg>>100%>>150=,=img>>caso-2-3.jpg>>100%>>150=,=br=,=img>>caso-2-4.jpg>>100%>>150=,=img>>caso-2-5.jpg>>100%>>150=,=h5>>Correlaci\xf3n=,=p>>Observando la matriz de correlaci\xf3n se observ\xf3 que hay una correlaci\xf3n grande entre home.dest y pclass con 0.887 y otra bastante significativa con un 0.612 entre fare y pclass de forma inversamente proporcional. Por lo tanto se decidi\xf3 quitar pclass.=,=img>>caso-2-6.jpg>>100%>>200=,=br=,=br=,=h5>>Selecci\xf3n de atributos=,=p>>Se descartaron las variables name y ticket por ser ids.=,=h3>>Modelo=,=p>>Se realiz\xf3 el procedimiento de data pre-processing descripto anteriormente y luego se utiliz\xf3 un algoritmo de regresi\xf3n log\xedstica con CrossValidation de 10 folds y muestreo estratificado para el testeo.=,=h5>>Dise\xf1o de RapidMiner=,=img>>caso-2-7.jpg>>100%>>400=,=p>>A continuaci\xf3n se mostrar\xe1 la configuraci\xf3n bloque a bloque del esquema de RapidMiner y se presentar\xe1 a la vez el c\xf3digo del modelo id\xe9ntico creado en SciKitLearn.=,=h5>>Retrieve Titanic=,=img>>caso-2-rm-1.jpg>>100%>>75.=,=br=,=code>>.input_file = \\"titanic.csv\\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=br=,=h5>>Select Attributes=,=img>>caso-2-rm-2.jpg>>100%>>400.=,=br=,=code>>.dataset.drop(labels = [\\"body\\", \\"cabin\\",\\"name\\",\\"pclass\\",\\"ticket\\"], axis = 1, inplace = True)=,=br=,=br=,=h5>>Primer replace Missing Values=,=img>>caso-2-rm-3.jpg>>100%>>175=,=br=,=code>>.dataset[\\"age\\"].fillna(dataset[\\"age\\"].mean(), inplace = True)=,=br=,=br=,=h5>>Segundo replace Missing Values=,=img>>caso-2-rm-4.jpg>>100%>>400=,=br=,=code>>.dataset[\\"boat\\"].fillna(\'0\', inplace = True)=,=br=,=code>>.dataset[\\"home.dest\\"].fillna(\'0\', inplace = True)=,=br=,=br=,=h5>>Filter Examples=,=img>>caso-2-rm-5.jpg>>100%>>400.=,=br=,=code>>.dataset = dataset[dataset[\'sibsp\'] <= 5]=,=br=,=code>>.dataset = dataset[dataset[\'parch\'] <= 5]=,=br=,=code>>.dataset = dataset[dataset[\'age\'] <= 70]=,=br=,=code>>.dataset = dataset[dataset[\'fare\'] <= 350]=,=br=,=code>>.dataset.dropna(subset=[\'fare\'], how=\'all\', inplace=True)=,=br=,=code>>.dataset.dropna(subset=[\'embarked\'], how=\'all\', inplace=True)=,=br=,=br=,=h5>>Nominal to Numerical=,=img>>caso-2-rm-6.jpg>>100%>>400..=,=br=,=code>>.le = LabelEncoder()=,=br=,=code>>.le.fit(dataset[\\"sex\\"])=,=br=,=code>>.encoded_sex_training = le.transform(dataset[\\"sex\\"])=,=br=,=code>>.dataset[\\"sex\\"] = encoded_sex_training=,=br=,=code>>.le.fit(dataset[\\"embarked\\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\\"embarked\\"])=,=br=,=code>>.dataset[\\"embarked\\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\\"home.dest\\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\\"home.dest\\"])=,=br=,=code>>.dataset[\\"home.dest\\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\\"boat\\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\\"boat\\"])=,=br=,=code>>.dataset[\\"boat\\"] = encoded_embarked_training=,=br=,=br=,=h5>>Cross Validation=,=img>>caso-2-rm-7.jpg>>100%>>150.=,=br=,=code>>.X = dataset.drop(labels=[\\"survived\\"], axis=1)=,=br=,=code>>.y = dataset[\\"survived\\"]=,=br=,=code>>.X, y = make_classification(n_samples=1283, n_features=10)=,=br=,=code>>.cv = StratifiedKFold(n_splits=10, random_state=None, shuffle=True)=,=br=,=code>>.model = LogisticRegression()=,=br=,=code>>.scores = cross_val_score(model, X, y, scoring=\'accuracy\', cv=cv, n_jobs=-1)=,=br=,=code>>.print(\'Accuracy: %.3f (%.3f)\' % (mean(scores), std(scores)))=,=br=,=br=,=h4>>Resultados=,=p>>Se obtuvo un muy alto nivel de presici\xf3n en RapidMiner con un 96.88% +/- 1.1%. =,=img>>caso-2-9.jpg>>100%>>200=,=br=,=br=,=img>>caso-2-10.jpg>>30%>>25=,=p>> En cuanto al modelo de SciKitLearn se obtuvo una presici\xf3n de 94.5% +/- 1.2%. De esta forma se puede destacar que se obtuvieron presiciones similares para de los modelos id\xe9nticos de regresi\xf3n log\xedstica aplicados al dataset Titanic realizados en las dos herramientas.=,=br=,=a>>Descargar modelo RapidMiner>>Titanic Logistic Regression.rmp>>d=,=br=,=a>>Descargar c\xf3digo SciKitLearn>>Titanic Logistic Regression.py>>d","content":"h2>>Problem=,=p>>Based on data regarding the sinking of the Titanic, the aim is to predict whether a person with certain characteristics survived or not, which is why it is a supervised binary classification problem. For this, a solution is proposed with a linear logistic regression algorithm using CrossValidation to test its performance.=,=h2>>Data analysis=,=p>>The dataset has 13 input attributes plus the target variable \\"survived\\" for which there are 1310 observations.=,=h4>>Attributes=,=p>>- pclass: Passenger class. Type integer.=,=p>>- name. Name of the passenger. Type string.=,=p>>-sex. Sex of the traveler. Type string.=,=p>>-age. Age of the person. royal guy. There are 263 missing data for this variable.=,=p>>- sibsp: Number of brothers on board. Integer type.=,=p>>- patch: Number of parents on board. Integer type.=,=p>>- ticket: Passenger ticket code. Type string.=,=p>>- fare: Rate paid. royal guy. There is a missing data for this entry.=,=p>>- cabin. String type. Traveler\'s cabin. String type. It has 1014 missing values. =,=p>>- embarked: Port where you embarked. String type. Two of these data are missing.=,=p>>- boat: Code of the lifeboat used by the person. String type. It has 823 missing values.=,=p>>- body: Body number if deceased. Integer type. It has 1188 missing values. =,=p>>- home.dest: Place of destination. String type. It has 564 missing values.=,=p>>- survived: If it survived or not. Type integer.=,=br=,=h3>>Data preparation=,=br=,=h5>>Missing Values=,=p>>Among the inputs there are variables with a high percentage of missing data, so it is not it had them being these body with 90.7% and cabin with 77.4%, for the age variable the missing values \u200b\u200bwere replaced by the average, the rows with missing values \u200b\u200bfor fare embarked were eliminated and a unique category was assigned for the missing values \u200b\u200bof boat and home.dest. After all this process, all the missing values \u200b\u200bwere eliminated.=,=h5>>Outliers=,=p>>The logistic regression algorithm is highly influenced by the outliers and therefore, after observing the histograms of the data for the different parameters, found below, those with one/s of the following conditions age>=70, sibsp>=6, parch>=6 or fare>=350.=,=img>>caso-2-2.jpg>>100%>>150=,=img>>caso-2-3.jpg>>100%>>150=,=br=,=img>>caso-2-4.jpg>>100%>>150=,=img>>caso-2-5.jpg>>100%>>150=,=h5>>Correlation=,=p>>Observing the correlation matrix, it was observed that there is a large correlation between home.dest and pclass with 0.887 and another quite significant one with 0.612 between fare and pclass in an inversely proportional way. Therefore it was decided to remove pclass.=,=img>>caso-2-6.jpg>>100%>>200=,=br=,=br=,=h5>>Selection attributes=,=p>>The name and ticket variables were discarded because they were ids.=,=h3>>Model=,=p>>The data pre-processing procedure described above was performed and then a logistic regression algorithm with 10-fold CrossValidation was used and stratified sampling for testing.=,=h5>>RapidMiner design=,=img>>caso-2-7.jpg>>100%>>400=,=p>>The block configuration will be shown below RapidMiner schematic block and the identical model code created in SciKitLearn will be presented at the same time.=,=h5>>Retrieve Titanic=,=img>>caso-2-rm-1.jpg>>100%>>75=,=br=,=code>>.input_file = \\"titanic.csv\\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=br=,=h5>>Select Attributes=,=img>>caso-2-rm-2.jpg>>100%>>400.=,=br=,=code>>.dataset.drop(labels = [\\"body\\",\\"cabin\\",\\"name\\",\\"pclass\\",\\"ticket\\"], axis = 1, inplace = True)=,=br=,=br=,=h5>>First replace Missing Values=,=img>>caso-2-rm-3.jpg>>100%>>175=,=br=,=code>>.dataset[\\"age\\"].fillna(dataset[\\"age \\"].mean(), inplace = True)=,=br=,=br=,=h5>>Second replace Missing Values=,=img>>caso-2-rm-4.jpg>>100%>>400=,=br=,=code>>.dataset[\\"boat\\"].fillna(\'0\', inplace = True)=,=br=,=code>>.dataset[\\"home.dest \\"].fillna(\'0\', inplace = True)=,=br=,=br=,=h5>>Filter Examples=,=img>>caso-2-rm-5.jpg>>100%>>400.=,=br=,=code>>.dataset = dataset[dataset[\'sibsp\'] <= 5]=,=br=,=code>>.dataset = dataset[dataset[\'parch\'] < = 5]=,=br=,=code>>.dataset = dataset[dataset[\'age\'] <= 70]=,=br=,=code>>.dataset = dataset[dataset[\'fare\'] < = 350]=,=br=,=code>>.dataset.dropna(subset=[\'fare\'], how=\'all\', inplace=True)=,=br=,=code>>.dataset.dropna (subset=[\'embarked\'], how=\'all\', inplace=True)=,=br=,=br=,=h5>>Nominal to Numerical=,=img>>caso-2-rm-6. jpg>>100%>>400..=,=br=,=code>>.le = LabelEncoder()=,=br=,=code>>.le.fit(dataset[\\"sex\\"]) =,=br=,=code>>.encoded_sex_training = le.transform(dataset[\\"sex\\"])=,=br=,=code>>.dataset[\\"sex \\"] = encoded_sex_training=,=br=,=code>>.le.fit(dataset[\\"embarked\\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\\" embarked\\"])=,=br=,=code>>.dataset[\\"embarked\\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\\"home.dest\\" ])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\\"home.dest\\"])=,=br=,=code>>.dataset[\\"home.dest\\" ] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\\"boat\\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\\"boat\\"])=,=br=,=code>>.dataset[\\"boat\\"] = encoded_embarked_training=,=br=,=br=,=h5>>Cross Validation=,=img>>caso-2- rm-7.jpg>>100%>>150.=,=br=,=code>>.X = dataset.drop(labels=[\\"survived\\"], axis=1)=,=br=,=code>>.y = dataset[\\"survived\\"]=,=br=,=code>>.X, y = make_classification(n_samples=1283, n_features=10)=,=br=,=code>> .cv = StratifiedKFold(n_splits=10, random_state=None, shuffle=True)=,=br=,=code>>.model = LogisticRegression()=,=br=,=code>>.scores = cross_val_score(model, x, y, score g=\'accuracy\', cv=cv, n_jobs=-1)=,=br=,=code>>.print(\'Accuracy: %.3f (%.3f)\' % (mean(scores), std(scores )))=,=br=,=br=,=h4>>Results=,=p>>A very high level of accuracy was obtained in RapidMiner with 96.88% +/- 1.1%. =,=img>>caso-2-9.jpg>>100%>>200=,=br=,=br=,=img>>caso-2-10.jpg>>30%>>25=,=p>> Regarding the SciKitLearn model, an accuracy of 94.5% +/- 1.2% was obtained. In this way, it can be noted that similar precisions were obtained for the identical logistic regression models applied to the Titanic dataset carried out in the two tools.=,=br=,=a>>Download RapidMiner model>>Titanic Logistic Regression.rmp>>d=,=br=,=a>>Download SciKitLearn code>>Titanic Logistic Regression.py>>d"},{"unidad":"Algoritmos lineales","unit":"Linear algorithms","titulo":"Deportes","title":"Sports","descripcion":"Utilizaremos An\xe1lisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificaci\xf3n a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.","description":"We will use Linear Discriminant Analysis (LDA) with Python\'s Sckit-learn and RapidMiner to train and classify from the Sports dataset to finally compare the predictions made by both models.","contenido":"h3>>Problema=,=p>>Seg\xfan caracter\xedsticas personales se desea predecir cual es el mejor deporte para una persona por lo cual este es un problema supervisado de clasificaci\xf3n. Para este se plantear\xe1 un modelado con Linear Descriminant Analysis a continuaci\xf3n.=,=h3>>An\xe1lisis de datos=,=p>>El dataset tiene 8 columnas de entrada num\xe9ricas y la variable objetivo categ\xf3rica \\"DeportePrimario\\" conteniendo 493 filas.=,=h4>>Atributos=,=p>>- Edad=,=p>>- Fuerza=,=p>>- Velocidad =,=p>>- Lesiones=,=p>>- Vision =,=p>>- Resistencia =,=p>>- Agilidad =,=p>>- CapacidadDecision =,=p>>- DeportePrimario=,=br=,=h3>>Preparaci\xf3n de datos=,=br=,=h5>>Missing Values=,=p>>En el dataset no hay datos faltantes.=,=h5>>Outliers=,=p>>Para LDA es importante quitar los outliers y tras observar los histogramas de los atributos se decidi\xf3 quitar los datos con CapacidadDecision<3 o CapacidadDecision>100. La distribuci\xf3n para esta variable es la siguiente.=,=img>>caso-3-1.jpg>>100%>>200=,=h4>>Est\xe1ndarizaci\xf3n=,=p>>El algoritmo a utilizar asume que las variables de entrada tienen la misma varianza por lo que es buena idea estandarizar los datos para que tengan media 0 y varianza 1.=,=h3>>Modelo=,=p>>El procedimiento descripto anteriormente y el modelo LDA fueron realizados de forma equivalente en RapidMiner y SciKitLearn. Se cont\xf3 con un dataset de entrenamiento y uno de prueba para el cual realizar predicciones.=,=h4>>Dise\xf1o de RapidMiner=,=img>>UT3-PD5-3.jpg>>100%>>300=,=br=,=br=,=h4>>C\xf3digo SciKitLearn=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = [\'Edad\',\'Fuerza\',\'Velocidad\',\'Lesiones\',\'Vision\',\'Resistencia\',\'Agilidad\',\'CapacidadDecision\']=,=br=,=code>>.=,=br=,=code>>.input_file = \\"sports_Training.csv\\"=,=br=,=code>>.input_file2 = \\"sports_Scoring.csv\\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training[\'CapacidadDecision\'] >= 3) & (data_training[\'CapacidadDecision\'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring[\'CapacidadDecision\'] >= 3) & (data_scoring[\'CapacidadDecision\'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training[\'DeportePrimario\'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({\'Prediccion\': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter(\'Prediccion.xlsx\', engine=\'xlsxwriter\')=,=br=,=code>>.df.to_excel(writer, sheet_name=\'Sheet1\')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Comparaci\xf3n de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicci\xf3n iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar proceso>>Sports LDA.rmp>>d=,=br=,=a>>Descargar c\xf3digo SciKitLearn>>Sports LDA.py>>d=,=br=,=a>>Descargar plantilla comparativa>>Comparaci\xf3n Rapid Miner Sci Kit Learn.xlsx>>d","content":"h3>>Problem=,=p>>According to personal characteristics, it is desired to predict which is the best sport for a person, for which this is a supervised classification problem. For this, a modeling with Linear Discriminant Analysis will be proposed below.=,=h3>>Data analysis=,=p>>The dataset has 8 numeric input columns and the categorical target variable \\"DeportePrimario\\" containing 493 rows. =,=h4>>Attributes=,=p>>- Age=,=p>>- Strength=,=p>>- Speed \u200b\u200b=,=p>>- Injuries=,=p>>- Vision =,= p>>- Endurance =,=p>>- Agility =,=p>>- DecisionCapacity =,=p>>- PrimarySport=,=br=,=h3>>Data preparation=,=br=,=h5 >>Missing Values=,=p>>There are no missing data in the dataset.=,=h5>>Outliers=,=p>>For LDA it is important to remove the outliers and after observing the histograms of the attributes it was decided to remove the data with CapacityDecision<3 or CapacityDecision>100. The distribution for this variable is as follows.=,=img>>caso-3-1.jpg>>100%>>200=,=h4>>Standardization=,=p>>The algorithm to use assumes that the variables input have the same variance so it is a good idea to standardize the data so that they have mean 0 and variance 1.=,=h3>>Model=,=p>>The procedure described above and the LDA model were performed equivalently in RapidMiner and SciKitLearn. We had a training dataset and a test dataset for which to make predictions.=,=h4>>RapidMiner Design=,=img>>UT3-PD5-3.jpg>>100%>>300=,=br =,=br=,=h4>>SciKitLearn code=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>> .from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = [\'Age\',\'Strength\',\'Speed\',\'Injuries\',\'Vision \',\'Endurance\',\'Agility\',\'CapacityDecision\']=,=br=,=code>>.=,=br=,=code>>.input_file = \\"sports_Training.csv\\"=,=br=,=code>>.input_file2 = \\"sports_Scoring.csv\\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training[\'CapacityDecision\'] >= 3) & ( data_training[\'DecisionCapacity\'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring[\'DecisionCapacity\'] >= 3) & (data_scoring[\'DecisionCapacity\'] ision\'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br =,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training[\'PrimarySport\'].values=,=br=,=code>>.=,= br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame( {\'Prediction\': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter(\'Prediction.xlsx\', engine=\'xlsxwriter\')=,=br=,=code>>.df. to_excel(writer, sheet_name=\'Sheet1\')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Comparison of results between Scikitlearn and RapidMiner=,= p>>98.81% equal prediction values \u200b\u200bwere obtained for the models made with Sckitlearn and RapidMiner.=,=a>>Download process>>Sports LDA.rmp >>d=,=br=,=a>>Download SciKitLearn Code>>Sports LDA.py>>d=,=br=,=a>>Download Comparison Template>>Rapid Miner Sci Kit Learn Comparison.xlsx>> d"},{"unidad":"Algoritmos no lineales","unit":"Non linear algorithms","titulo":"CART para clasificaci\xf3n binaria simple","title":"CART for Simple Binary Classification","descripcion":"Emplearemos un modelo CART para un problema de clasificaci\xf3n binaria simple con dos variables de entrada X1 y X2 y una variable de salida Y.","description":"We will use a CART model for a simple binary classification problem with two input variables X1 and X2 and one output variable Y.","contenido":"p>>Los \xe1rboles de decisi\xf3n son un importante algoritmo de predicci\xf3n de machine learning para predicci\xf3n supervisada de tanto para clasificaci\xf3n como para regresi\xf3n. Esta estructura es la misma de algoritmos y estructuras de datos, \xe1rboles binarios, pero en este caso cada nodo representa una sola variable de entrada x y un split point para esa variable. Los nodos hojas se diferencian en que contienen un valor de salida el cual es utilizado para hacer la predicci\xf3n. Tras tener entrenado el \xe1rbol es muy sencillo hacer predicc\xedones, solo basta con recorrer el \xe1rbol con los datos de entrada a etiquetar.=,=p>>Para entrenar los \xe1rboles de decisi\xf3n recursivamente se divide el espacio de la entrada de forma binaria utilizando un algoritmo voraz el cual consiste en probar diferentes splits utilizando una funcion de costos y quedarse con el que lo minimice.=,=p>>En los problemas de regression la funcion de costos a minimizar es la suma de error cuadr\xe1tico.=,=img>>pds0.jpg>>35%>>75=,=br=,=br=,=p>>En los problemas de clasificaci\xf3n se utiliza el \xedndice de Gini que indica que tan puros son los nodos hoja.=,=img>>aa1.jpg>>35%>>75=,=br=,=br=,=p>>El criterio de freno del alguritmo recursivos m\xe1s com\xfan es utilizar un m\xednimo de instancias asignadas a cada nodo hoja, cuando al realizar un splir se encuentra con un n\xfamero menor a ese no lo hace y ese nodo se designa hoja. Cuantos m\xe1s nodos por hoja m\xe1s gen\xe9rico es el \xe1rbol y al tener menos es m\xe1s espec\xedfico lo que puede tender al overfitting.=,=p>>A continuaci\xf3n se realizar\xe1 un ejercicio en una hoja de c\xe1lculo que muestra como realizar una partici\xf3n en base al \xedndice de Gini para armar un \xe1rbol de decisi\xf3n sencillo que luego ser\xe1 utilizado para realizar predicciones.=,=p>>- Se ingres\xf3 el dataset de entrada y se graficaron los datos diferenciando las clases Y = 0 e Y = 1. =,=img>>UT4-PD1-1.jpg>>100%>>200=,=p>> - Se utiliz\xf3 la variable X1 para armar el modelo CART con un valor para el punto de divisi\xf3n de X1 = 2.771244718 calculando el \xedndice Gini del modelo producido. =,=p>> - Se repiti\xf3 el proceso anterior con un punto de divisi\xf3n de X1 = 6.642287351 y se obtuvo el valor del coeficiente Gini del CART generado adem\xe1s de una descripci\xf3n del \xe1rbol de decisi\xf3n generado.=,=img>>aa2.jpg>>100%>>400=,=p>> Se utiliz\xf3 el CART producido para hacer predicciones de un dataset de test y se calcul\xf3 la exactitud.=,=img>>aa3.jpg>>50%>>300=,=br=,=br=,=p>> Las planillas de trabajo en las que se hicieron los pasos explicados est\xe1n accesibles para descargar con los siguientes enlaces:=,=a>>Descargar plantilla entrenamiento>>Algoritmos no lineales PD1-1.xlsx>>d=,=br=,=a>>Descargar plantilla predicciones>>Algoritmos no lineales PD1-2.xlsx>>d","content":"p>>Decision trees are an important machine learning prediction algorithm for supervised prediction for both classification and regression. This structure is the same as algorithms and data structures, binary trees, but in this case each node represents a single input variable x and a split point for that variable. Leaf nodes differ in that they contain an output value which is used to make the prediction. After having trained the tree, it is very easy to make predictions, it is enough to traverse the tree with the input data to label.=,=p>>To train the decision trees recursively, the input space is divided binary using a greedy algorithm which consists of trying different splits using a cost function and staying with the one that minimizes it.=,=p>>In regression problems the cost function to be minimized is the sum of quadratic error.=,=img >>pds0.jpg>>35%>>75=,=br=,=br=,=p>>In classification problems, the Gini index is used, which indicates how pure the leaf nodes are.=,= img>>aa1.jpg>>35%>>75=,=br=,=br=,=p>>The most common recursive algorithm brake criterion is to use a minimum number of instances assigned to each leaf node, when at performing a splir encounters a number less than that does not and that node is designated a leaf. The more nodes per leaf, the more generic the tree is and having fewer is more specific, which can tend to overfitting.=,=p>>Next, an exercise will be carried out in a spreadsheet that shows how to perform a partition based on the Gini index to build a simple decision tree that will later be used to make predictions.=,=p>>- The input dataset was entered and the data was plotted, differentiating the classes Y = 0 and Y = 1. =,= img>>UT4-PD1-1.jpg>>100%>>200=,=p>> - The variable X1 was used to build the CART model with a value for the division point of X1 = 2.771244718 calculating the Gini index of the model produced. =,=p>> - The previous process was repeated with a division point of X1 = 6.642287351 and the value of the Gini coefficient of the generated CART was obtained, as well as a description of the generated decision tree.=,=img>>aa2.jpg>>100%>>400=,=p>> The CART produced was used to make predictions of a test dataset and the accuracy was calculated.=,=img>>aa3.jpg>>50%>>300=,=br=,=br=,=p>> The worksheets in which the explained steps were carried out are accessible for download with the following links:=,=a>>Download training template>>Nonlinear algorithms PD1- 1.xlsx>>d=,=br=,=a>>Download Predictions Template>>Nonlinear Algorithms PD1-2.xlsx>>d"},{"unidad":"Algoritmos no lineales","unit":"Non linear algorithms","titulo":"\xc1rboles de decisi\xf3n en KNIME","title":"Decision Trees in KNIME","descripcion":"Estudiaremos la construcci\xf3n de un modelo de \xe1rbol de decisi\xf3n de regresi\xf3n simple en la herramienta KNIME.","description":"We will study the construction of a simple regression decision tree model in the KNIME tool.","contenido":"img>>UT4-PD2-1.jpg>>100%>500=,=br=,=br=,=h3>>An\xe1lisis=,=br=,=h5>>Descripci\xf3n=,=p>> Este workflow utiliza el dataset Iris dividi\xe9ndolo con muestreo estrat\xedficado en una parte de entrenamiento de un 80% y otra de prueba de un 20% para el caso de un modelo de \xe1rbol de regresi\xf3n simple del cual se obtiene la performance y se la visualiza a trav\xe9s de una es gr\xe1fica. En este modelo a partir del ancho y el largo del s\xe9palo, el largo de los p\xe9talos y la clase de una flor entre Iris-setosa, Iris-versicolor y Iris virg\xednica predice el ancho de sus p\xe9talos. =,=h5>> Componentes =,=h6>>File reader=,=p>>El operador equivalente a FileReader en RapidMiner es Retrieve. Estos se diferencian en que el primero deja editar caracter\xedsticas del dataset luego de ser incluido en el proyecto como los tipos para las variables y en RM esto se configura solo al importar el dataset. En el dataset sepal length, sepal width, petal length y pethal width son doubles y class es una string.=,=img>>UT4-PD2-2.jpg>>50%>>650=,=br=,=br=,=h6>>Partitioning=,=p>> El operador partitioning ofrece elegir el tama\xf1o de la partici\xf3n y la forma en la que se quiere separar determinando por ejemplo si se eligen los primeros valores o si se hace muestreo estart\xedficado, da la opci\xf3n para usar una seed como en RM, permite alternativas acerca de las Flow Variables que funcionan para a hacer variar ciertas configuraciones en el nodo de forma din\xe1mica con cada ejecuci\xf3n y deja a elecci\xf3n pol\xedticas sobre el uso de la memoria. =,=p>> El operador equivalente a Patitioning en RapidMiner es Split. La principal diferencia es que mientras que en el componente de KNIME se hace una partici\xf3n por cada unidad del componente, en RM se permite realizar varias a la vez.=,=img>>UT4-PD2-3.jpg>>50%>>50=,=br=,=br=,=h6>>Simple Regression Tree Learner.=,=p>> El proceso utiliza un algoritmo base de \xe1rbol de regresi\xf3n simple siguiendo el algoritmo descripto en \u201cClassification and Regression Tress\u201d (Breiman et al, 1984) con algunas simplifaciones como no pruning, no necesariamente \xe1rboles binarios, tratar de encontrar la mejor direcci\xf3n para los missing values, etc. En estos \xe1rboles de regesi\xf3n el valor de predicci\xf3n el valor para cada nodo hoja es la media de los registros dentro de ella y la predicci\xf3n mejora cuanto menor sea la varianza de los valores dentro de una hoja. Por lo tanto, para armarlo en cada nodo se hacen splits que minimicen la suma de errores cuadr\xe1ticos de los hijos. El operador Simple Regression Tree Learner soporta predictores de tipo num\xe9rico y categ\xf3rico aunque solo soporta target columns de tipo num\xe9rico.=,=p>> Los par\xe1metros que se pueden configurar del algoritmo son determinar el uso o no splits binarios para los atributos nominales, la forma en la que se manejan los missing values siendo XGBoost un algoritmo que calcula la mejor direcci\xf3n para los valores faltantes y Surrogate que calcula para cada Split otros alternativos que mejoran la aproximaci\xf3n, el l\xedmite para la profundidad del \xe1rbol, el m\xednimo de valores que puede tener un nodo para que el Split se intente y el m\xednimo de registros que un nodo hijo puede tener.=,=img>>UT4-PD2-4.jpg>>50%>>800=,=br=,=br=,=h6>>Simple Regression Tree Predictor=,=p>> Este operador recibe por un lado el modelo entrenado y por otro los datos de test, c\xf3mo salida tiene las predicciones realizadas. Permite modificar manualmente la columna de predicci\xf3n, utlizar Flow Variables y decidir sobre pol\xedticas de memoria. =,=img>>UT4-PD2-5.jpg>>50%>>300=,=br=,=br=,=h6>>Line Plot=,=p>> Este componente muestra una gr\xe1fica que compara los valores de predicci\xf3n con la salida conocida para esos inputs del dataset de entrenamiento.=,=p>> Los par\xe1metros que se pueden editar son el n\xfamero de filas a mostrar, el l\xedmite del valor nominal a partir del cual una columna sea ignorada y la opci\xf3n de colocar Flow Variables.=,=img>>UT4-PD2-6.jpg>>50%>>300=,=br=,=br=,=img>>UT4-PD2-7.jpg>>70%>>300=,=br=,=br=,=h6>>Numeric Scorer=,=p>> El operador funciona realizando los c\xe1lculos para los valores de coeficiente de terminaci\xf3n, media de error absoluto, error cuadr\xe1tico medio y desviaci\xf3n media con signo de la predicci\xf3n realizada.=,=img>>UT4-PD2-8.jpg>>50%>>400=,=br=,=br=,=img>>UT4-PD2-9.jpg>>30%>>125","content":"img>>UT4-PD2-1.jpg>>100%>500=,=br=,=br=,=h3>>Analysis=,=br=,=h5>>Description=,=p>> This workflow uses the Iris dataset dividing it with stratified sampling into a training part of 80% and a test part of 20% for the case of a simple regression tree model from which the performance is obtained and it is displayed through a graph. In this model from the width and length of the sepal, the length of the petals and the class of a flower between Iris-setosa, Iris-versicolor and Iris virginica predicts the width of its petals. =,=h5>> Components =,=h6>>File reader=,=p>>The operator equivalent to FileReader in RapidMiner is Retrieve. These differ in that the first one allows dataset characteristics to be edited after being included in the project, such as the types for the variables, and in RM this is configured only when importing the dataset. In the dataset sepal length, sepal width, petal length and pethal width are doubles and class is a string.=,=img>>UT4-PD2-2.jpg>>50%>>650=,=br=,=br =,=h6>>Partitioning=,=p>> The partitioning operator offers to choose the size of the partition and the way in which it is wanted to be separated, determining for example if the first values \u200b\u200bare chosen or if statified sampling is done, it gives the option to use a seed as in RM, allows alternatives about the Flow Variables that work to vary certain configurations in the node dynamically with each execution and leaves to choose policies on the use of memory. =,=p>> The equivalent operator to Patitioning in RapidMiner is Split. The main difference is that while in the KNIME component a partition is made for each unit of the component, in RM several are allowed at the same time.=,=img>>UT4-PD2-3.jpg>>50%>>400=,=br=,=br=,=h6>>Simple Regression Tree Learner.=,=p>> The process uses a simple regression tree base algorithm following the algorithm described in \u201cClassification and Regression Tress\u201d ( Breiman et al, 1984) with some simplifications like no pruning, not necessarily binary trees, trying to find the best direction for missing values, etc. In these regression trees, the prediction value for each leaf node is the mean of the records within it, and the prediction improves the smaller the variance of the values \u200b\u200bwithin a leaf. Therefore, to assemble it in each node, splits are made that minimize the sum of squared errors of the children. The Simple Regression Tree Learner operator supports numeric and categorical type predictors, although it only supports numeric type target columns.=,=p>> The parameters that can be configured for the algorithm are to determine whether or not to use binary splits for nominal attributes, the way in which missing values \u200b\u200bare handled, XGBoost being an algorithm that calculates the best address for missing values \u200b\u200band Surrogate that calculates for each Split other alternatives that improve the approximation, the limit for the depth of the tree, the minimum of values \u200b\u200bthat can have a node for the split to try and the minimum number of records a child node can have.=,=img>>UT4-PD2-4.jpg>>50%>>800=,=br=,=br=,=h6>>Simple Regression Tree Predictor=,=p>> This operator receives the trained model on the one hand and the test data on the other, as output it has the predictions made. It allows manually modifying the prediction column, using Flow Variables and deciding on memory policies. =,=img>>UT4-PD2-5.jpg>>50%>>300=,=br=,=br=,=h6>>Line Plot=,=p>> This component displays a plot comparing the prediction values \u200b\u200bwith the known output for those training dataset inputs.=,=p>> The parameters that can be edited are the number of rows to display, the nominal value limit from which a column is ignored, and the option to place Flow Variables.=,=img>>UT4-PD2-6.jpg>>50%>>300=,=br=,=br=,=img>>UT4-PD2-7.jpg>>70%>>300=,=br=,=br=,=h6>>Numeric Scorer=,=p>> The operator works by performing calculations for the values \u200b\u200bof completion coefficient, mean absolute error, mean square error, and deviation signed mean of the prediction made.=,=img>>UT4-PD2-8.jpg>>50%>>400=,=br=,=br=,=img>>UT4-PD2-9.jpg>>30%>>125"},{"unidad":"Algoritmos no lineales","unit":"Non linear algorithms","titulo":"Comparaci\xf3n de Decision Trees en herramientas de ML","title":"Comparison of Decision Trees in ML Tools","descripcion":"Comparaci\xf3n de \xe1rboles de decisi\xf3n entre RapidMiner y Weka aplicados al dataset Iris. Adem\xe1s se analiza el componente mencionado en Azure ML Studio, KNIME y Python Sci kit learn.","description":"Comparison of decision trees between RapidMiner and Weka applied to the Iris dataset. In addition, the mentioned component is analyzed in Azure ML Studio, KNIME and Python Sci kit learn.","contenido":"p>>Se utiliza el dataset Iris y las herramientas RapidMiner y Weka para hacer una comparaci\xf3n de la performance de sus modelos de \xe1rboles de decisi\xf3n midi\xe9ndola con un Split de datos de un 70% y un 30% para entrenamiento y test respectivamente en ambas.=,=h4>>RapidMiner=,=p>>Tipo de problema. Clasificaci\xf3n y regresi\xf3n.=,=p>>Algoritmo base. C4.5=,=p>>Caracter\xedsticas requeridas de atributos y label. Las variables de entrada pueden ser num\xe9ricas o nominales. Se exige una variable objetivo nominal para clasificaci\xf3n y num\xe9rica para de regresi\xf3n. =,=h6>>Par\xe1metros=,=p>>- Criterion: gain_ratio. Selecciona el criterio que se usar\xe1 para seleccionar los atributos sobre los que hacer los splits.=,=p>>- Maximal Depth: 10. La m\xe1xima profundidad del \xe1rbol.=,=p>>- Apply pruning: Activado. Si se aplica pruning o no.=,=p>>- Confidence: 0.1. Nivel de confianza usado para el error pesimista del c\xe1lculo de pruning.=,=p>>- Apply prepruning: Activado. Si se aplica prepruning o no=,=p>>- Minimal gain: 0.01. La ganancia de un nodo se calcula antes del Split. Se hace el Split si la ganancia es mayor al minimal gain.=,=p>>- Minimal leaf size: 2. Tama\xf1o m\xednimo de observaciones por hoja.=,=p>>- Minimal size for split: 4. El tama\xf1o de un nodo es el n\xfamero de ejemplos en \xe9l. Solo se hace el Split para obtener nodos con un tama\xf1o mayor al minimal size for Split.=,=p>>- Number of alternatives for pruning: 3. Numero de nodos alternativos testeados para un Split cuando el prepruning previene un Split.=,=br=,=h4>>Weka=,=p>>Tipo de problema. Clasificaci\xf3n y regresi\xf3n.=,=p>>Algoritmo base. C4.5.=,=p>>Caracter\xedsticas requeridas de atributos y label. Las variables de entrada pueden ser num\xe9ricas o nominales. Se exige una variable objetivo nominal para clasificaci\xf3n y num\xe9rica para de regresi\xf3n.=,=h6>>Par\xe1metros=,=p>>- batchSize: 100. Numero de instancias a procesar si la predicci\xf3n batch est\xe1 siendo realiza.=,=p>>- Debug: False. Si es verdadero el clasificador podr\xeda poner info adicional como salida en consola.=,=p>>- doNotCheckCapabilities: False. Si es verdadero las capacidades del clasificador no son checkeadas antes de la compilaci\xf3n=,=p>>- initialCount: 0.0. Valor inicial del contador de la clase.=,=p>>- MaxDepth: 10. M\xe1xima profundidad del \xe1rbol, con -1 no hay restricci\xf3n.=,=p>>- MinNum: 2.0. M\xednimo peso total para las instancias de una hoja.=,=p>>- minVariancePrep: 0.001. M\xednima proporci\xf3n de la varianza de todos los datos que necesita estar en un nodo para que se haga un Split.=,=p>>- noPruning. False. Si se realiza pruning.=,=p>>- numDecimalPlaces. 2. N\xfamero de posiciones decimales para usar en la salida del modelo.=,=p>>- numFolds: 3. Determina el tama\xf1o de los datos usados para pruning.=,=p>>- Seed. 1. La semilla usada para la aleatoriedad de los datos.=,=p>>- spreadIntialCount. False. Distribuir el recuento inicial en todos los valores en lugar de utilizar el recuento por valor.=,=br=,=h4>>Resultados=,=p>>Se obtuvo una performance de un 91.11% en el caso de RapidMiner y un 96% en Weka utilizando los par\xe1metros que se muestran en la secci\xf3n anterior para cada uno.=,=h6>>RapidMiner=,=img>>UT4-PD5-1.jpg>>100%>>200=,=br=,=br=,=h6>>Weka=,=img>>UT4-PD5-2.jpg>>70%>>800=,=br=,=br=,=h4>>Otras herramientas=,=br=,=h5>>Azure Machine Learning Studio=,=p>>Tipo de problema. Clasificaci\xf3n y regresi\xf3n.=,=p>>Algoritmo base. Se utiliza una versi\xf3n mejorada de los \xe1rboles de decisi\xf3n con una gradient boosting machine.=,=p>>Caracter\xedsticas requeridas de atributos y label. Las variables de entrada pueden ser num\xe9ricas o nominales. Se exige una variable objetivo nominal para clasificaci\xf3n y num\xe9rica para de regresi\xf3n.=,=h6>>Par\xe1metros=,=p>>- Maximum number of leaves per tree. N\xfamero m\xe1ximo de hojas del \xe1rbol=,=p>>- Minimum number of samples per leaf node. M\xednimo n\xfamero de ejemplos en un nodo hoja.=,=p>>- Learning rate. Ritmo de aprendizaje.=,=p>>- Total num trees constructed. N\xfamero de \xe1rboles construidos al entrenar el algoritmo.=,=p>>- Random number seed. Semilla de entrenamiento.=,=p>>- Allow unknown categorical levels. Seleccionado crea un nuevo nivel para cada atributo categ\xf3rico.=,=br=,=h5>>KNIME=,=p>>Tipo de problema. Clasificaci\xf3n.=,=p>>Algoritmo base. C4.5.=,=p>>Caracter\xedsticas requeridas de atributos y label. Las variables de entrada pueden solo ser num\xe9ricas o nominales. La variable objetivo solo puede ser nominal.=,=h6>>Par\xe1metros=,=p>>- Class column. Selecciona la variable objetivo.=,=p>>- Quality measure. Para seleccionar la medida de calidad para la cual se calcularan los Splits. Las opciones son Gini index y Gain Ratio.=,=p>>- Pruning method. Pruning reduce el tama\xf1o del \xe1rbol y evita el overfitting.=,=p>>- Reduced error pruning. Si se checkea se usa un simple m\xe9todo de pruning.=,=p>>- Min number records per node. M\xednimo numero de registros requeridos por nodo.=,=p>>- Number records to store for view. Selecciona el n\xfamero de registros guardados en un tree para la view.=,=p>>-  Average Split point. Al checkearla el valor para Split con atributos num\xe9ricos se determina seg\xfan la media de los valores que separan a las dos particiones.=,=p>>- Number threads. Permite multiprocesamiento.=,=p>>- Skip nominal columns without domain information. Seleccionada las columnas nominales que no informaci\xf3n de valores de dominio se saltean.=,=p>>- Force root split column. Seleccionada, el primer split es calculado en la columna elegida sin evaluar ninguna otra para posibles splits.=,=p>>- Binary nominal splits. Al seleccionarla a los atributos nominales se les hacen splits binarios.=,=p>>- Max nominal. N\xfamero m\xe1ximo de valores nominales.=,=p>>- Filter invalid attribute values in child nodes. Habilitando esta opci\xf3n se hace un post procesamiento del tree y se filtran checkeos inv\xe1lidos.=,=p>>- No true child strategy: Opciones para cuando el valor de los atributos de un nodo es desconocido.=,=p>>- Missing value strategy. Opciones para los valores faltantes.=,=br=,=h5>>Python Sci kit learn=,=p>>Tipo de problema. Clasificaci\xf3n y regresi\xf3n=,=p>>Algoritmo base. CART=,=p>>Caracter\xedsticas requeridas de atributos y label. Las variables de entrada pueden solo ser num\xe9ricas. La variable objetivo puede ser nominal o num\xe9rica.=,=h6>>Par\xe1metros=,=p>>- Criterio. Funci\xf3n para medir la calidad del Split. Puede ser Gini o entropy=,=p>>- Splitter. Estrategia utilizada para elegir el Split en cada nodo. Las opciones son mejor o random.=,=p>>- Max Depth. M\xe1xima profundidad del \xe1rbol.=,=p>>- Min samples. M\xednimo de ejemplos requeridos para hacer un Split generando un nodo interno.=,=p>>- Min samples leaf. M\xednimo de ejemplos requeridos para hacer un Split generando un nodo hoja.=,=p>>- Min weight fraction. Fracci\xf3n de peso m\xednimo del total de peso requerido en un nodo hoja.=,=p>>- Max features. M\xe1ximo n\xfamero de variables de entrada considerado para hacer el mejor Split.=,=p>>- Random_state. Controla la aleatoriedad del estimador.=,=p>>- Max_leaf_node. M\xe1ximo n\xfamero de nodos hoja.=,=p>>- Min_impurity_decrease: Se le har\xe1 un Split a un nodo si el Split da un decenso de la impureza mayor o igual a este valor.=,=p>>- Class_weight: Pesos asociados a las clases.=,=p>>- Ccp_alpha: Par\xe1metro de complehidad usado para el pruning de m\xednimo costo-complejidad.","content":"p>>The Iris dataset and the RapidMiner and Weka tools are used to compare the performance of their decision tree models, measuring it with a data split of 70% and 30% for training and testing, respectively, in both. =,=h4>>RapidMiner=,=p>>Type of problem. Classification and regression.=,=p>>Base algorithm. C4.5=,=p>>Required characteristics of attributes and label. Input variables can be numeric or nominal. A nominal objective variable is required for classification and numerical for regression. =,=h6>>Parameters=,=p>>- Criterion: gain_ratio. Select the criteria that will be used to select the attributes on which to make the splits.=,=p>>- Maximal Depth: 10. The maximum depth of the tree.=,=p>>- Apply pruning: Activated. If pruning is applied or not.=,=p>>- Confidence: 0.1. Confidence level used for the pessimistic error of the pruning calculation.=,=p>>- Apply prepruning: Activated. If prepruning is applied or not=,=p>>- Minimal gain: 0.01. The gain of a node is calculated before the split. Split is done if the gain is greater than minimal gain.=,=p>>- Minimal leaf size: 2. Minimum size of observations per leaf.=,=p>>- Minimal size for split: 4. The size of a node is the number of instances in it. The Split is only done to obtain nodes with a size greater than the minimal size for Split.=,=p>>- Number of alternatives for pruning: 3. Number of alternative nodes tested for a Split when prepruning prevents a Split.=,=br=,=h4>>Weka=,=p>>Type of problem. Classification and regression.=,=p>>Base algorithm. C4.5.=,=p>>Required characteristics of attributes and label. Input variables can be numeric or nominal. A nominal objective variable is required for classification and numerical for regression.=,=h6>>Parameters=,=p>>- batchSize: 100. Number of instances to process if the batch prediction is being performed.=,=p>>-Debug:False. If true the classifier could put additional info as console output.=,=p>>- doNotCheckCapabilities: False. If true classifier capabilities are not checked before compilation=,=p>>- initialCount: 0.0. Initial value of the class counter.=,=p>>- MaxDepth: 10. Maximum depth of the tree, with -1 there is no restriction.=,=p>>- MinNum: 2.0. Minimum total weight for the instances of a sheet.=,=p>>- minVariancePrep: 0.001. Minimum proportion of the variance of all the data that needs to be in a node for a Split to be done.=,=p>>- noPruning. False. If pruning is done.=,=p>>- numDecimalPlaces. 2. Number of decimal places to use in the model output.=,=p>>- numFolds: 3. Determines the size of the data used for pruning.=,=p>>- Seed. 1. The seed used for randomization of the data.=,=p>>- spreadIntialCount. False. Distribute the initial count over all values \u200b\u200binstead of using count per value.=,=br=,=h4>>Results=,=p>>A performance of 91.11% was obtained in the case of RapidMiner and 96 % in Weka using the parameters shown in the previous section for each.=,=h6>>RapidMiner=,=img>>UT4-PD5-1.jpg>>100%>>200=,=br=,=br=,=h6>>Weka=,=img>>UT4-PD5-2.jpg>>70%>>800=,=br=,=br=,=h4>>Other tools=,=br=,=h5>>Azure Machine Learning Studio=,=p>>Issue type. Classification and regression.=,=p>>Base algorithm. An improved version of decision trees with a gradient boosting machine is used.=,=p>>Required attributes and label features. Input variables can be numeric or nominal. A nominal objective variable is required for classification and numerical for regression.=,=h6>>Parameters=,=p>>- Maximum number of leaves per tree. Maximum number of tree leaves=,=p>>- Minimum number of samples per leaf node. Minimum number of examples in a leaf node.=,=p>>- Learning rate. Learning rate.=,=p>>- Total num trees constructed. Number of trees built when training the algorithm.=,=p>>- Random number seed. Training seed.=,=p>>- Allow unknown categorical levels. Selected creates a new level for each categorical attribute.=,=br=,=h5>>KNIME=,=p>>Issue type. Classification.=,=p>>Base algorithm. C4.5.=,=p>>Required characteristics of attributes and label. Input variables can only be numeric or nominal. The target variable can only be nominal.=,=h6>>Parameters=,=p>>- Class column. Select the target variable.=,=p>>- Quality measure. To select the quality measure for which Splits will be calculated. The options are Gini index and Gain Ratio.=,=p>>- Pruning method. Pruning reduces the size of the tree and avoids overfitting.=,=p>>- Reduced pruning error. If checked, a simple pruning method is used.=,=p>>- Min number records per node. Minimum number of records required per node.=,=p>>- Number records to store for view. Select the number of records stored in a tree for the view.=,=p>>- Average Split point. When checking it, the value for Split with numeric attributes is determined according to the average of the values \u200b\u200bthat separate the two partitions.=,=p>>- Number threads. Allows multiprocessing.=,=p>>- Skip nominal columns without domain information. Selected nominal columns that no domain value information is skipped.=,=p>>- Force root split column. When checked, the first split is calculated on the chosen column without evaluating any other for possible splits.=,=p>>- Binary nominal splits. When selecting it, the nominal attributes are split binary.=,=p>>- Max nominal. Maximum number of nominal values.=,=p>>- Filter invalid attribute values \u200b\u200bin child nodes. Enabling this option makes a post processing of the tree and invalid checks are filtered.=,=p>>- No true child strategy: Options for when the value of the attributes of a node is unknown.=,=p>>- Missing value strategy. Options for missing values.=,=br=,=h5>>Python Sci kit learn=,=p>>Type of problem. Classification and regression=,=p>>Base algorithm. CART=,=p>>Required characteristics of attributes and label. Input variables can only be numeric. The target variable can be nominal or numerical.=,=h6>>Parameters=,=p>>- Criterion. Function to measure the quality of the Split. It can be Gini or entropy=,=p>>- Splitter. Strategy used to choose the Split at each node. Options are best or random.=,=p>>- Max Depth. Maximum depth of the tree.=,=p>>- Min samples. Minimum of examples required to make a Split generating an internal node.=,=p>>- Min samples leaf. Minimum of examples required to make a Split generating a leaf node.=,=p>>- Min weight fraction. Minimum weight fraction of the total weight required in a leaf node.=,=p>>- Max features. Maximum number of input variables considered to make the best Split.=,=p>>- Random_state. Controls the randomness of the estimator.=,=p>>- Max_leaf_node. Maximum number of leaf nodes.=,=p>>- Min_impurity_decrease: A node will be split if the split gives a decrease in impurity greater than or equal to this value.=,=p>>- Class_weight: Associated weights to classes.=,=p>>- Ccp_alpha: Complexity parameter used for minimum cost-complexity pruning."},{"unidad":"Algoritmos no lineales","unit":"Non linear algorithms","titulo":"SVM lineal con descenso de sub gradiente","title":"Linear SVM with Sub-Gradient Descent","descripcion":"En una hoja de c\xe1lculo analizaremos el algoritmo de Support Vector Machines lineal con descenso de sub gradiente y lo utilizaremos para realizar predicciones.","description":"In a spreadsheet, we will discuss the Sub-Gradient Descent Linear Support Vector Machines algorithm and use it to make predictions.","contenido":"p>>Support Vector machines es uno de los algoritmos de machine learning m\xe1s populares. Este modelo se basa en la definici\xf3n de hiperplanos n dimensionales, siendo n el n\xfamero de variables de entrada, para dividir los puntos ejemplos en dos clases aceptando solo variables de predicci\xf3n binomiales de forma tal de que se maximice el margen de la separaci\xf3n entre clases. De todas maneras se puede permitir dejar que algunos puntos violen la l\xednea de separaci\xf3n a partir de un coeficiente C se determina la magnitud de este fen\xf3meno.=,=p>>La divisi\xf3n de las clases en SVM depende de los llamados kernels. Estos determinan el tipo de hiperplano con el que se dividir\xe1 el espacio n dimensinal. Algunos ejemplos son lineal, polinomial, radial, etc.=,=p>>Proximamente estudiaremos un caso de SVM dividiendo un espacio bidimensional al tener solamente las variables de entrada x1 y x2, utilizando un kernel lineal formando as\xed nada m\xe1s ni nada menos que una recta que se deber\xe1 definir.  =,=p>>- Se graficaron los datos en dos series para Y=1 y para Y=-1.=,=img>>UT4-PD3-1.jpg>>70%>>250=,=br=,=br=,=p>>- Para hallar los coeficientes B1 y B2 del modelo SVM lineal B0 + B1 x X1 + B2 x X2 = 0 se utiliz\xf3 el m\xe9todo del descenso de sub-gradiente. El coeficiente B0 fue descartado por lo cual la recta resultante pasa por el origen.=,=p>>Para aplicar este algoritmo se comienza con los coeficientes B1 y B2 en 0 y posteriormente se calcula un primer valor de salida con la f\xf3rmula dispuesta a continuaci\xf3n.=,=img>>UT4-PD3-2.jpg>>50%>>50=,=p>>Si el valor de salida es mayor a 1 el patr\xf3n de entrenamiento no es un vector de soporte y por lo tanto se aplica la siguiente funci\xf3n para b1 y b2=,=img>>UT4-PD3-3.jpg>>20%>>75=,=p>>En caso contrario se aplica la siguiente f\xf3rmula sobre los valores de los vectores utilizando en este caso lambda=0.45 y donde t es la iteraci\xf3n actual=,=img>>UT4-PD3-4.jpg>>70%>>75=,=p>>- Este procedimiento fue realizado por 16 \xe9pocas en las que para cada una se itera sobre todo el dataset.=,=p>>- Se realiz\xf3 una gr\xe1fica de la exactitud en funci\xf3n de las \xe9pocas obteniendo el siguiente resultado.=,=img>>UT4-PD3-5.jpg>>70%>>350=,=br=,=br=,=p>>- Tras todas las iteraciones los coeficientes obtenidos fueron B1=0.55 Y B2=-0.72 obteniendo el plano 0.55xX1 -0.72xX2 = 0. Este se utiliz\xf3 para calcular las predicciones con los datos de entrenamiento y se obtuvo un 100% de exactitud.=,=img>>aa7.jpg>>50%>>200=,=br=,=br=,=a>>Descargar planilla de trabajo>>Algoritmos no lineales PD3.xlsx>>d","content":"p>>Support Vector machines is one of the most popular machine learning algorithms. This model is based on the definition of n-dimensional hyperplanes, where n is the number of input variables, to divide the example points into two classes accepting only binomial prediction variables in such a way as to maximize the margin of separation between classes. In any case, it is permissible to allow some points to violate the line of separation. Starting from a coefficient C, the magnitude of this phenomenon is determined.=,=p>>The division of classes in SVM depends on the so-called kernels. These determine the type of hyperplane with which the n-dimensional space will be divided. Some examples are linear, polynomial, radial, etc.=,=p>>Soon we will study a case of SVM dividing a two-dimensional space by having only the input variables x1 and x2, using a linear kernel thus forming nothing more and nothing less than a line to be defined. =,=p>>- The data was plotted in two series for Y=1 and for Y=-1.=,=img>>UT4-PD3-1.jpg>>70%>>250=,=br=,=br=,=p>>- To find the coefficients B1 and B2 of the linear SVM model B0 + B1 x X1 + B2 x X2 = 0, the sub-gradient descent method was used. Coefficient B0 was discarded, so the resulting straight line passes through the origin.=,=p>>To apply this algorithm, start with coefficients B1 and B2 at 0 and then calculate a first output value with the formula provided below continuation.=,=img>>UT4-PD3-2.jpg>>50%>>50=,=p>>If the output value is greater than 1 the training pattern is not a support vector and therefore Therefore, the following function is applied to b1 and b2=,=img>>UT4-PD3-3.jpg>>20%>>75=,=p>>Otherwise, the following formula is applied to the values \u200b\u200bof the vectors using in this case lambda=0.45 and where t is the current iteration=,=img>>UT4-PD3-4.jpg>>70%>>75=,=p>>- This procedure was performed for 16 epochs in the which for each one is iterated over the entire dataset.=,=p>>- A graph of the accuracy was made as a function of the epochs, obtaining the following result.=,=img>>UT4-PD3-5.jpg>>70%>>350=,=br=,=br=,=p>>- After all the iterations, the coefficients obtained were B1=0.55 and B2 =-0.72 obtaining the plane 0.55xX1 -0.72xX2 = 0. This was used to calculate the predictions with the training data and 100% accuracy was obtained.=,=img>>aa7.jpg>>50%>> 200=,=br=,=br=,=a>>Download worksheet>>Nonlinear algorithms PD3.xlsx>>d"},{"unidad":"Algoritmos no lineales","unit":"Non linear algorithms","titulo":"SVM no lineal en RapidMiner","title":"Nonlinear SVM in RapidMiner","descripcion":"En este ejercicio se analizar\xe1 el componente SVM de RapidMiner y se lo utilizar\xe1 para resolver un problema no separable linealmente.","description":"In this exercise you will explore the SVM component of RapidMiner and use it to solve a linearly non-separable problem.","contenido":"p>>El operador SVM utiliza la implementaci\xf3n de Java de support vector machine mySVM de Stefan Rueping pudiendo ser usado para regresi\xf3n y clasificaci\xf3n. Este fue empleado para resolver un problema no separable linealmente primero con los par\xe1metros por defecto y luego se lo resolvi\xf3 modificando el kernel a polinomial.=,=p>> A continuaci\xf3n se describen y muestran los valores de los par\xe1metros utilizados del operador y luego los resultados de performance obtenidos para ambos casos. =,=h5>>Par\xe1metros=,=p>>- Kernel type: Tipo de funci\xf3n kernel a utilizar en el algoritmo. Valor inicial dot, final polynomial.=,=p>>- Kernel cache: Fija el tama\xf1o de cache para las evaluaciones kernel. Valor: inicial 200, final 200.=,=p>>- C: Es una constante de complejidad que fija la tolerancia a clasificaci\xf3n err\xf3nea, cuando m\xe1s alta m\xe1s suaves son los l\xedmites y cuanto baja estos son m\xe1s duros. Si es demasiado alta puede dar overfitting y si es muy baja puede dar over generalization. Valor: inicial 0.0, final 0.0.=,=p>>- Convergence \xe9psilon: Especifica la precisi\xf3n de las KKT conditions. Valor: inicial 0.01, final 0.01.=,=p>>- Max iterations: N\xfamero m\xe1ximo de iteraciones. Valor: inicial 100000, final 100000.=,=p>>- Scale: Si est\xe1 activado los valores se escalan. Valor: inicial activado, final activado.=,=p>>- Lpos: Factor para constante de complejidad del SVM caso positivos. Valor: inicial 1.0, final 1.0.=,=p>>- Lneg: Factor para constante de complejidad del SVM caso negativos. Valor: inicial 1.0, final 1.0.=,=p>>- \xc9psilon: Constante de insensibilidad. Valor: inicial 0.0, final 0.0.=,=p>>- \xc9psilon plus: Par\xe1metro parte de la funci\xf3n de p\xe9rdida. Valor: inicial 0.0, final 0.0.=,=p>>- \xc9psilon minus: Par\xe1metro parte de la funci\xf3n de p\xe9rdida. Valor: inicial 0.0, final 0.0.=,=p>>- Balance cost: Adapta Cpos y Cneg al tama\xf1o relative de las clases. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss pos: Usa p\xe9rdida cuadr\xe1tica para desviaci\xf3n positiva. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss neg: Usa p\xe9rdida cuadr\xe1tica para desviaci\xf3n negativa. Valor: inicial desactivado, final desactivado.=,=br=,=h5>>Resultados=,=br=,=h6>>Caso inicial=,=img>>UT4-PD4-1.jpg>>100%>>150=,=br=,=br=,=h6>>Caso final=,=img>>UT4-PD4-2.jpg>>100%>>150=,=br=,=br=,=a>>Descargar proceso de RapidMiner>>UT4-TA7.rmp>>d","content":"p>>The SVM operator uses the Java implementation of support vector machine mySVM by Stefan Rueping being able to be used for regression and classification. This was used to solve a linearly non-separable problem first with the default parameters and then it was solved by modifying the kernel to polynomial.=,=p>> The values \u200b\u200bof the used parameters of the operator are described and shown below and then the performance results obtained for both cases. =,=h5>>Parameters=,=p>>- Kernel type: Type of kernel function to use in the algorithm. Initial value dot, final polynomial.=,=p>>- Kernel cache: Sets the size of the cache for kernel evaluations. Value: initial 200, final 200.=,=p>>- C: It is a complexity constant that fixes the tolerance to erroneous classification, the higher it is, the softer the limits are and the lower it is, they are harder. If it is too high it can give overfitting and if it is too low it can give over generalization. Value: initial 0.0, final 0.0.=,=p>>- Convergence epsilon: Specifies the precision of the KKT conditions. Value: initial 0.01, final 0.01.=,=p>>- Max iterations: Maximum number of iterations. Value: initial 100000, final 100000.=,=p>>- Scale: If activated, the values \u200b\u200bare scaled. Value: initial activated, final activated.=,=p>>- Lpos: Factor for the complexity constant of the SVM in positive cases. Value: initial 1.0, final 1.0.=,=p>>- Lneg: Factor for the complexity constant of the SVM in negative cases. Value: initial 1.0, final 1.0.=,=p>>- Epsilon: Insensitivity constant. Value: initial 0.0, final 0.0.=,=p>>- Epsilon plus: Parameter part of the loss function. Value: initial 0.0, final 0.0.=,=p>>- Epsilon minus: Parameter part of the loss function. Value: initial 0.0, final 0.0.=,=p>>- Balance cost: Adapts Cpos and Cneg to the relative size of the classes. Value: start off, end off.=,=p>>- Quadratic loss pos: Use quadratic loss for positive offset. Value: start off, end off.=,=p>>- Quadratic loss neg: Use quadratic loss for negative offset. Value: start off, end off.=,=br=,=h5>>Results=,=br=,=h6>>Initial Case=,=img>>UT4-PD4-1.jpg>>100%>> 150=,=br=,=br=,=h6>>Final case=,=img>>UT4-PD4-2.jpg>>100%>>150=,=br=,=br=,=a>>Download RapidMiner process>>UT4-TA7.rmp>>d"},{"unidad":"Algoritmos no lineales","unit":"Non linear algorithms","titulo":"Naive Bayes en planilla electr\xf3nica","title":"Naive Bayes in spreadsheet","descripcion":"Implementaci\xf3n de un modelo de Naive Bayes en una planilla electr\xf3nica.","description":"Implementation of a Naive Bayes model in an electronic spreadsheet.","contenido":"p>>El modelo Naive Bayes para clasificaci\xf3n en problemas binarios o multiclase se caracteriza por asumir que los atributos son independientes entre s\xed (lo cual rara vez acontece) y en basarse en el teorema de Bayes.=,=h6>>Teorema de Bayes=,=p>>Este es utilizado para calcular probabilidad de una hip\xf3tesis dado un suceso de la siguiente manera.=,=img>>UT4-PD6-1.jpg>>40%>>100=,=p>>Siendo=,=p>>- P(h|d) la probabilidad de la hip\xf3tesis h dado el susceso d (conocida como la probabilidad a posteriori).=,=p>>- P(d|h) la probabilidad del suceso d dado que la hip\xf3tesis h sea cierta.=,=p>>- P(h) la probabilidad de que la hip\xf3tesis h sea cierta (conocida como la probabilidad a priori).=,=p>>- P(d) la probabilidad de que suceda el suceso d m\xe1s all\xe1 de que se cumpla h o no.=,=p>>En Naive Bayesian se calcula las probabilidades a priori para cada clase, las probabilidades para cada combinaci\xf3n entre un valor de un atributo y una clase, se emplea el Teorema de Bayes para obtener la probabilidad a posteriori y finalmente se clasifica seg\xfan la clase que produjo el valor m\xe1s elevado.=,=p>>Ahora mostraremos el comportamiento del modelo Naive Bates emple\xe1ndolo para el dataset Jugar Tenis prediciendo si se juega o no al tenis en condiciones metereol\xf3gicas particulares definidas por los atributos de los datos.=,=h5>>Dataset=,=img>>UT4-PD6-2.jpg>>70%>>300=,=br=,=br=,=p>>Se calcularon las siguientes probabilidades para generar el modelo tal y como se describi\xf3 anteriormente pero para este caso particular.=,=img>>UT4-PD6-3.jpg>>80%>>450=,=br=,=br=,=p>>Posteriormente se utiliz\xf3 el modelo generado para realizar las predicciones que se encuentran a continuaci\xf3n.=,=img>>UT4-PD6-4.jpg>>90%>>100","content":"p>>The Naive Bayes model for classification in binary or multiclass problems is characterized by assuming that the attributes are independent of each other (which rarely happens) and based on in Bayes theorem.=,=h6>>Bayes theorem=,=p>>This is used to calculate probability of a hypothesis given an event as follows.=,=img>>UT4-PD6-1.jpg>>40%>>100=,=br=,=br=,=p>>Being=,=p>>- P(h|d) is the probability of hypothesis h given event d (known as the posterior probability).=,= p>>- P(d|h) the probability of event d given that hypothesis h is true.=,=p>>- P(h) the probability that hypothesis h is true (known as the prior probability ).=,=p>>- P(d) the probability of event d happening beyond s e meets ho no.=,=p>>In Naive Bayesian, the prior probabilities for each class are calculated, the probabilities for each combination between a value of an attribute and a class, Bayes\' Theorem is used to obtain the probability a posteriori and finally it is classified according to the class that produced the highest value.=,=p>>Now we will show the behavior of the Naive Bates model using it for the Play Tennis dataset predicting whether or not tennis is played in particular weather conditions defined by the data attributes.=,=h5>>Dataset=,=img>>UT4-PD6-2.jpg>>70%>>300=,=br=,=br=,=p>>The following were calculated probabilities to generate the model as described above but for this particular case.=,=img>>UT4-PD6-3.jpg>>80%>>450=,=br=,=br=,=p>>Later, the generated model was used to make the following predictions.=,=img>>UT4-PD6-4.jpg>>90%>>100"},{"unidad":"Algoritmos no lineales","unit":"Non linear algorithms","titulo":"KNN en hoja de c\xe1lculo y RapidMiner","title":"KNN in Sheet calculation and RapidMiner","descripcion":"Empleo de KNN para realizar predicciones en una hoja de c\xe1lculo y RapidMiner.","description":"Using KNN to make predictions in a spreadsheet and RapidMiner.","contenido":"p>>El algoritmo para clasificaci\xf3n y regresi\xf3n K-Nearest Neighbors se diferencia de los dem\xe1s en que no es un modelo aprendido sino que se realizan predicciones a partir del dataset de entrenamiento directamente. El procedimiento consiste en que para cada punto a predecir se busca entre las instancias m\xe1s similares en base a medidas como distancia euclideana, Hamming, Manhattan, Jaccard, Minkowski, etc.=,=p>>Para utilizar KNN para predecir basta con elegir un valor de k y una medida de distancia que ser\xe1n utilizados para hallar los k puntos m\xe1s cercanos de entre los que est\xe1n en los datos de entrenamiento respecto al ejemplo que se quiere predecir. A partir de all\xed para clasificaci\xf3n se compara las clases de estos k puntos y la m\xe1s repetida ser\xe1 la designada como predicci\xf3n y para regresi\xf3n se asigna el promedian de la salida de los k valores m\xe1s cercanos. Cabe destacacr que es importante normalizar si las escalas de los datos se encuentran en diferentes magnitudes para que esto no afecte la calidad de las predicciones de nuestro modelo.=,=p>>A continiuaci\xf3n aplicaremos el algoritmo KNN para un sencillo ejemplo de clasificaci\xf3n con distintos valores de k y distancia euclideana en una hoja de c\xe1lculo.=,=p>>- Graficamos de los datos en dos series para Y=0 y para Y=1=,=img>>UT4-PD8-1.jpg>>75%>>200=,=br=,=br=,=p>>- Agregamos el punto con (x1, x2)= (8.093607318, 3.365731514) para clasificarlo usando diferentes valores de k.=,=img>>aa5.jpg>>100%>>150=,=p>>K=3: Puntos m\xe1s cercanos: (x1, x2) = (7.423436942, 4.696522875), (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Predicci\xf3n Y = 1.=,=p>>K=2: Puntos m\xe1s cercanos: (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Predicci\xf3n Y = 1.=,=p>>K=1: Puntos m\xe1s cercanos: (x1, x2) = (7.792783481, 3.424088941). Predicci\xf3n Y = 1.=,=p>>- A\xf1adimos el punto con (x1, x2)= (5, 2.5) para clasificarlo usando diferentes valores de k.=,=img>>aa6.jpg>>100%>>150=,=p>>K=3: Puntos m\xe1s cercanos: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (3.110073483, 1.781539638), (x1, x2) = (5.745051997, 3.533989803). Predicci\xf3n Y = 0.=,=p>>K=2: Puntos m\xe1s cercanos: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (5.745051997, 3.533989803). Predicci\xf3n Y = N/A.=,=p>>K=1: Puntos m\xe1s cercanos: (x1, x2) = (5.745051997, 3.533989803). Predicci\xf3n Y = 1.=,=a>>Descargar planilla electr\xf3nica>>KVecinosMasCercanos.xlsx>>d=,=br=,=br=,=p>>Ahora estudiaremos al algoritmo KNN en RapidMiner para el dataset de clasificaci\xf3n Iris observando c\xf3mo es el operador y cu\xe1les son sus par\xe1metros en esta herramienta.=,=h6>>Modelo=,=img>>UT4-PD8-2.jpg>>100%>>300=,=br=,=br=,=h6>>Gr\xe1fica=,=img>>UT4-PD8-3.jpg>>100%>>450=,=br=,=br=,=h6>>Consideraciones=,=p>>Se puede observar que los datos correspondientes a la clase iris-setosa se encuentran bastante separados de los de las otras dos. En el caso de estos \xfaltimos iris-versicolor e iris-virginica, si bien tienen un poco de solapamiento tambi\xe9n se pueden ver \xe1reas diferenciables.=,=h6>>Preparaci\xf3n de datos=,=p>>Es conveniente estandarizar los datos con un range transformation entre 0 y 1.=,=h6>>Operador KNN en RapidMiner=,=p>>Voto ponderado: Si se activa este par\xe1metro los valores de distancia entre los ejemplos se tienen en cuenta para la predicci\xf3n. Es \xfatil para ponderar las contribuciones de los vecinos de forma tal que los vecinos m\xe1s cercanos contribuyan m\xe1s que los m\xe1s lejanos.=,=p>>Tipos de medici\xf3n: Este par\xe1metro se utiliza para seleccionar el tipo de medida que se utilizar\xe1 para encontrar los vecinos m\xe1s cercanos. Las opciones son las siguientes.=,=p>>- MixedMeasures: Se utiliza para calcular distancias en el caso de atributos tanto nominales como num\xe9ricos.=,=p>>- NominalMeasure: Se usa para el caso de solo atributos nominales.=,=p>>- NumericalMeasure: Se usa para el caso de solo atributos num\xe9ricos.=,=p>>- BregmannDivergences: Se selecciona para emplear diveregencias de Bregmann como tipos de medidas de cercan\xeda.=,=p>>Funciones de medici\xf3n:=,=p>>- EuclideanDistance. Dist = Sqrt ( Sum_(j=1) [y(1,j)-y(2,j)]^2).=,=p>>- CanberraDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)| / (|y(1,j)|+|y(2,j)|).=,=p>>- ChebychevDistance. Dist = max_(j=1) (|y(1,j)-y(2,j)|).=,=p>>- CosineSimilarity. Mide el coseno del \xe1ngulo entre los vectores de atributos de los dos ejemplos.=,=p>>- DiceSimilarity. La DiceSimilarity para atributos num\xe9ricos se calcula como 2 * Y1Y2 / (Y1 + Y2). Y1Y2 = Suma sobre el producto de valores = Suma_ (j = 1) y (1, j) * y (2, j). Y1 = Suma de los valores del primer Ejemplo = Suma_ (j = 1) y (1, j) Y2 = Suma de los valores del segundo Ejemplo = Suma_ (j = 1) y (2, j).=,=p>>- DynamicTimeWarpingDistance. Se calcula la distancia en una ruta de deformaci\xf3n \xf3ptima desde el vector atributo del primer ejemplo al segundo ejemplo.=,=p>>- InnerProductSimilarity. Dist = -Similarity = -Sum_(j=1) y(1,j)*y(2,j).=,=p>>- JaccardSimilarity. Dist = Y1Y2/(Y1+Y2-Y1Y2).=,=p>>- KernerlEuclideanDistance. La distancia se calcula mediante la distancia euclidiana de los dos Ejemplos, en un espacio transformado. La transformaci\xf3n est\xe1 definida por el kernel y los par\xe1metros correspondientes elegidos.=,=p>>- ManhattanDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)|.=,=p>>- MaxProductSimilarity. Dist = -Similarity = -max_(j=1) (y(1,j)*y(2,j)).=,=p>>- OverlapSimilarity. Dist = minY1Y2 / min (Y1, Y2).=,=h6>>Resultados=,=p>>K=3, euclidean distance:=,=img>>UT4-PD8-4.jpg>>100%>>150=,=p>>K=2, euclidean distance:=,=img>>UT4-PD8-5.jpg>>100%>>150=,=p>>K=3, manhattan distance:=,=img>>UT4-PD8-7.jpg>>100%>>150=,=p>>K=2, manhattan distance:=,=img>>UT4-PD8-6.jpg>>100%>>150=,=br=,=br=,=h6>>Conclusiones=,=p>>Con distancia euclideana, al variar el valor de k se obtuvieron diferentes valores de performance, pero esto no sucedi\xf3 en los ejemplos realizados para la distancia manhattan.=,=a>>Descargar proceso de RapidMiner>>UT4-TA8.rmp>>d","content":"p>>The algorithm for classification and regression K-Nearest Neighbors differs from the others in that it is not a learned model, rather predictions are made from the training dataset directly. The procedure consists of searching for each point to be predicted among the most similar instances based on measurements such as Euclidean distance, Hamming, Manhattan, Jaccard, Minkowski, etc.=,=p>>To use KNN to predict, simply choose a value of k and a measure of distance that will be used to find the k closest points among those in the training data with respect to the example to be predicted. From there, for classification, the classes of these k points are compared and the most repeated will be designated as prediction, and for regression the average of the output of the k closest values \u200b\u200bis assigned. It should be noted that it is important to normalize if the data scales are at different magnitudes so that this does not affect the quality of our model\'s predictions.=,=p>>Next we will apply the KNN algorithm for a simple classification example with different values \u200b\u200bof k and Euclidean distance in a spreadsheet.=,=p>>- We plot the data in two series for Y=0 and for Y=1=,=img>>UT4-PD8-1.jpg>> 75%>>200=,=br=,=br=,=p>>- We add the point with (x1, x2)= (8.093607318, 3.365731514) to classify it using different values \u200b\u200bof k.=,=img>>aa5.jpg>>100%>>150=,=p>>K=3: Closest points: (x1, x2) = (7.423436942, 4.696522875), (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Prediction Y = 1.=,=p>>K=2: Closest points: (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Prediction Y = 1.=,=p>>K=1: Closest points: (x1, x2) = (7.792783481, 3.424088941). Prediction Y = 1.=,=p>>- We add the point with (x1, x2)= (5, 2.5) to classify it using different values \u200b\u200bof k.=,=img>>aa6.jpg>>100%>> 150=,=p>>K=3: Closest points: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (3.110073483, 1.781539638), (x1, x2) = (5.745051997, 3.533989803) . Prediction Y = 0.=,=p>>K=2: Closest points: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (5.745051997, 3.533989803). Prediction Y = N/A.=,=p>>K=1: Closest points: (x1, x2) = (5.745051997, 3.533989803). Prediction Y = 1.=,=a>>Download spreadsheet>>KNearestNeighbors.xlsx>>d=,=br=,=br=,=p>>Now we will study the KNN algorithm in RapidMiner for the Iris classification dataset observing how is the operator and what are its parameters in this tool.=,=h6>>Model=,=img>>UT4-PD8-2.jpg>>100%>>300=,=br=,=br=,=h6>>Graph=,=img>>UT4-PD8-3.jpg>>100%>>450=,=br=,=br=,=h6>>Considerations=,=p>>It can be seen that the data corresponding to the iris-setosa class are quite separated from those of the other two. In the case of the latter iris-versicolor and iris-virginica, although they have a little overlap, distinguishable areas can also be seen.=,=h6>>Data preparation=,=p>>It is convenient to standardize the data with a range transformation between 0 and 1.=,=h6>>KNN operator in RapidMiner=,=p>>Weighted vote: If this parameter is activated, the distance values \u200b\u200bbetween the examples are taken into account for the prediction. It is useful for weighting the neighbor contributions so that the nearest neighbors contribute more than the farthest.=,=p>>Measure Types: This parameter is used to select the type of measure to use to find the neighbors. closest neighbors. The options are as follows.=,=p>>- MixedMeasures: Used to calculate distances in the case of both nominal and numeric attributes.=,=p>>- NominalMeasure: Used in the case of only nominal attributes.=,=p>>- NumericalMeasure: Used for the case of only numeric attributes.=,=p>>- BregmannDivergences: Selected to use Bregmann divergences as closeness measure types.=,=p>>Measurement functions :=,=p>>- EuclideanDistance. Dist = Sqrt ( Sum_(j=1) [y(1,j)-y(2,j)]^2).=,=p>>- CanberraDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)| / (|y(1,j)|+|y(2,j)|).=,=p>>- ChebychevDistance. Dist = max_(j=1) (|y(1,j)-y(2,j)|).=,=p>>- CosineSimilarity. Measures the cosine of the angle between the attribute vectors of the two examples.=,=p>>- DiceSimilarity. The DiceSimilarity for numeric attributes is calculated as 2 * Y1Y2 / (Y1 + Y2). Y1Y2 = Sum over the product of values \u200b\u200b= Sum_ (j = 1) and (1, j) * and (2, j). Y1 = Sum of the values \u200b\u200bof the first Example = Sum_ (j = 1) and (1, j) Y2 = Sum of the values \u200b\u200bof the second Example = Sum_ (j = 1) and (2, j).=,=p>>-DynamicTimeWarpingDistance. Calculate the distance in an optimal deformation path from the attribute vector of the first example to the second example.=,=p>>- InnerProductSimilarity. Dist = -Similarity = -Sum_(j=1) y(1,j)*y(2,j).=,=p>>- JaccardSimilarity. Dist = Y1Y2/(Y1+Y2-Y1Y2).=,=p>>- KernerlEuclideanDistance. The distance is calculated by the Euclidean distance of the two Examples, in a transformed space. The transformation is defined by the kernel and the corresponding parameters chosen.=,=p>>- ManhattanDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)|.=,=p>>- MaxProductSimilarity. Dist = -Similarity = -max_(j=1) (y(1,j)*y(2,j)).=,=p>>- OverlapSimilarity. Dist = minY1Y2 / min (Y1, Y2).=,=h6>>Results=,=p>>K=3, euclidean distance:=,=img>>UT4-PD8-4.jpg>>100%>> 150=,=p>>K=2, euclidean distance:=,=img>>UT4-PD8-5.jpg>>100%>>150=,=p>>K=3, manhattan distance:=,= img>>UT4-PD8-7.jpg>>100%>>150=,=p>>K=2, manhattan distance:=,=img>>UT4-PD8-6.jpg>>100%>>150 =,=br=,=br=,=h6>>Conclusions=,=p>>With Euclidean distance, by varying the value of k, different performance values \u200b\u200bwere obtained, but this did not happen in the examples carried out for the Manhattan distance .=,=a>>Download RapidMiner Process>>UT4-TA8.rmp>>d"},{"unidad":"No supervisado","unit":"Unsupervised","titulo":"DBSCAN en Wine","title":"DBSCAN in Wine","descripcion":"Aplicamos el algoritmo de clustering DBSCAN para encontrar agrupaciones en el dataet Wine","description":"We apply the DBSCAN clustering algorithm to find clusters in the Wine dataset.","contenido":"p>>El algoritmo no supervisado de clustering DBSCAN se basa en las densidades de ejemplos para definir agrupaciones. Se fundamenta en descubrir \xe1reas de alta densidad de datos rodeadas de otras de baja densidad seg\xfan los par\xe1metros de radio \xe9psilon (\u03b5) y el m\xednimo de puntos que puede haber en cl\xfaster. A partir de estos valores claifica a los puntos en 3 tipos, puntos de n\xfancleo los cuales est\xe1n en una regi\xf3n de alta densidad de al menos un punto, los de borde que est\xe1n a un a distancie \u03b5 de alg\xfan punto y los de ruido son aquellos que caen fuera de las dos clases anteriores. De esta manera, el usuario puede modificar ambos par\xe1metros seg\xfan su conveniencia pero no definir\xe1 la cantidad de clusteres que el modelo hallar\xe1 como sucede con kmeans.=,=p>>Para ver un caso de empleo de DBSCAN nos ayudaremos de RapidMiner para aplicarlo a los atributos petal-width y petal height del dataset Iris. Para estos casos se mantendr\xe1 constante el m\xednimo de puntos con 5 y se ir\xe1 modificando el valor de \u03b5 en distintas iteraciones para luego analizar como var\xedan los resultados obtenidos.=,=h6>>Proceso RapidMiner=,=img>>dbscan-m.jpg>>100%>>250=,=a>>Descargar proceso>>ut5 ta1 ej2.rmp>>d=,=br=,=br=,=h6>>Con \u03b5=1.0=,=img>>dbscan-0.jpg>>70%>>550=,=br=,=br=,=h6>>Con \u03b5=0.3=,=img>>dbscan-1.jpg>>70%>>550=,=br=,=br=,=h6>>Con \u03b5=0.2=,=img>>dbscan-2.jpg>>70%>>550=,=br=,=br=,=p>>Se observa como el cluster 0 corresponde a puntos de ruido. Al ir disminuyendo el radio cada vez aparecen m\xe1s puntos de ruido y nuevos clusteres, con \u03b5=1.0 y \u03b5=0.3 hay solo 2 pero a partir de \u03b5=0.2 se suma uno nuevo y si se contin\xfaa realizando este descenso en el par\xe1metro cada vez aparecer\xe1n m\xe1s.","content":"p>>The unsupervised DBSCAN clustering algorithm relies on sample densities to define clusters. It is based on discovering areas of high data density surrounded by others of low density according to the parameters of radius epsilon (\u03b5) and the minimum number of points that can be in a cluster. Based on these values, it classifies the points into 3 types: core points, which are in a high-density region of at least one point, edge points, which are at a distance \u03b5 from some point, and noise points are those that they fall outside of the above two classes. In this way, the user can modify both parameters according to his convenience but he will not define the number of clusters that the model will find as it happens with kmeans.=,=p>>To see a case of using DBSCAN we will use RapidMiner to apply it to the petal-width and petal-height attributes of the Iris dataset. For these cases, the minimum number of points will be kept constant with 5 and the value of \u03b5 will be modified in different iterations to later analyze how the results obtained vary.=,=h6>>RapidMiner process=,=img>>dbscan-m.jpg>>80%>>250=,=br=,=a>>Download process>>ut5 ta1 ej2.rmp>>d=,=br=,=br=,=h6>>With \u03b5=1.0=,=img>>dbscan-0.jpg>>70%>>550=,=br=,=br=,=h6>>With \u03b5=0.3=,=img>>dbscan-1.jpg>>70%>>550=, =br=,=br=,=h6>>With \u03b5=0.2=,=img>>dbscan-2.jpg>>70%>>550=,=br=,=br=,=p>>It is observed as cluster 0 corresponds to noise points. As the radius decreases, more and more noise points and new clusters appear, with \u03b5=1.0 and \u03b5=0.3 there are only 2 but from \u03b5=0.2 a new one is added and if this decrease in the parameter is continued each time more will appear."},{"unidad":"No supervisado","unit":"Unsupervised","titulo":"PCA para dataset Cereals","title":"PCA for Cereals dataset","descripcion":"Estudio del algoritmo PCA utilizando los datos de Cereals","description":"Study of the PCA algorithm using Cereals data","contenido":"p>>Principal Component Analyisis (PCA) es un algoritmo que reduce los atributos en algunos atributos principales que contienen la mayor variabilidad transformando variables existentes en componentes principales o nuevas variables que no tienen correlaci\xf3n entre s\xed, de forma acumulada explican la mayor cantidad de varianza entre los datos y son revertibles a las variables originales por factores de peso. Otras de sus caracter\xedsticas son que solamente se puede aplicar para variables num\xe9ricas y que se pierde el significado de las variables del problema con los componentes principales.=,=p>>A continuaci\xf3n realizaremos un ejemplo en RapidMiner sobre el dataset cereals que cuenta con 77 ejemplos cuyas variables son las siguientes.=,=p>>- Name (categ\xf3rica): Nombre del cereal=,=p>>- mfr (categ\xf3rica): Manufacturero del cereal (A = American Home Food Products; G = General Mills; K = Kelloggs; N = Nabisco; P = Post; Q = Quaker Oats; R = Ralston Purina)=,=p>>- type (categ\xf3rico): fr\xedo o caliente.=,=p>>- protein (num\xe9rico): prote\xednas del cereal.=,=p>>- fat (num\xe9rico): grasas del cereal.=,=p>>- sodium (num\xe9rico): miligramos de sodio.=,=p>>- fiber (num\xe9rico): gramos de fibra diet\xe9tica.=,=p>>- carbo (num\xe9rico): gramos de carbohidratos complejos.=,=p>>- sugars (num\xe9rico): gramos de az\xfacar.=,=p>>- potass (num\xe9rico): miligramos de potasio.=,=p>>- vitamines (num\xe9rico): viraminas y minerales indicando el porcentaje t\xedpico de las recomendaciones del FDA.=,=p>>- shelf (num\xe9rico): Estante (1,2,3,4).=,=p>>- Weight (num\xe9rico): Peso en onzas por porci\xf3n.=,=p>>- Cups (num\xe9rico): Cantidad de tazas por porci\xf3n.=,=p>>- Rating (num\xe9rico): Puntaje para los cereales.=,=a>>Descargar dataset>>Cereals.xls>>d=,=br=,=br=,=p>>Se import\xf3 el modelo y para poder aplicar el algoritmo se tuvieron que quitar los atributos categ\xf3ricos Name, mfr y type y reemplazar los 4 valores faltantes por el promedio del atributo correspondiente debido a que PCA ni variables categ\xf3ricas ni missing values.=,=p>>Luego del pre procesamiento anterior se coloc\xf3 el operador PCA con 0.95 de variance threshold que refiere a la cantidad de varianza que buscar\xe1 explicar por las variables que seleccione en los componentes principales generados y keep reduction en dimensionality reduction por lo que los atributos con varianza mayor al umbral se eliminan del dataset.=,=h6>>Esquema RapidMiner =,=img>>pca-0.jpg>>80%>>200=,=br=,=a>>Descargar modelo>>pca.rmp>>d=,=br=,=br=,=p>>Los resultados obtenidos son los siguientes.=,=img>>pca-1.jpg>>80%>>400=,=br=,=br=,=img>>pca-2.jpg>>80%>>400=,=br=,=br=,=p>>Observando la primer imagen se puede concluir que se supera a la varianza acumulada establecida de en variance threshold de 0.95 contando con tan solo con los primeros componentes al llegar 0.966. En la segunda representaci\xf3n se ve el peso que tiene cada atributo en cada componente lo cual nos permite observar que rating, calories, potasium y vitamins son los atributos que m\xe1s aportan en los 3 primeros componentes principales y por lo tanto los m\xe1s importantes.","content":"p>>Principal Component Analysis (PCA) is an algorithm that reduces the attributes into some principal attributes that contain the greatest variability by transforming existing variables into principal components or new variables that have no correlation with each other, cumulatively they explain the greatest amount of variance between the data and are revertible to the original variables by weight factors. Other characteristics are that it can only be applied to numerical variables and that the meaning of the variables of the problem with the main components is lost.=,=p>>Next we will carry out an example in RapidMiner on the cereals dataset that has 77 examples whose variables are as follows.=,=p>>- Name (categorical): Name of the cereal=,=p>>- mfr (categorical): Manufacturer of the cereal (A = American Home Food Products; G = General Mills; K = Kelloggs; N = Nabisco; P = Post; Q = Quaker Oats; R = Ralston Purina)=,=p>>- type (categorical): hot or cold.=,=p>>- protein (numeric): cereal proteins.=,=p>>- fat (numeric): cereal fats.=,=p>>- sodium (numeric): milligrams of sodium.=,=p>>- fiber (numeric): grams of dietary fiber.=,=p>>- carbo (numeric): grams of complex carbohydrates.=,=p>>- sugars (numeric): grams of sugar.=,=p>>- potass (numeric): milligrams of potassium.=,=p>>- vitamins (numeric): viramines and minerals indicating the typical percentage of FDA recommendations.=,=p>>- shelf (numeric): Shelf (1,2,3,4).=,=p>>- Weight (numeric): Weight in ounces per serving.=,=p>>- Cups (numeric): Number of cups per serving.=,=p>>- Rating (numeric): Score for cereals.=,=a>>Download dataset>>Cereals.xls>>d =,=br=,=br=,=p>>The model was imported and in order to apply the algorithm the Name, mfr and type categorical attributes had to be removed and the 4 missing values \u200b\u200breplaced by the average of the corresponding attribute due to that PCA neither categorical variables nor missing values.=,=p>>After the previous pre-processing, the PCA operator was placed with a variance threshold of 0.95, which refers to the amount of variance that it will seek to explain by the variables that it selects in the main components generated and keep reduction in dimensionality reduction so that attributes with variance greater than the threshold are removed from the dataset.=,=h6>>RapidMiner Scheme =,=img>>pca-0.jpg>>80%>>200=,= br=,=a>>Download r model>>pca.rmp>>d=,=br=,=br=,=p>>The results obtained are the following.=,=img>>pca-1.jpg>>80%>>400=,=br=,=br=,=img>>pca-2.jpg>>80%>>400=,=br=,=br=,=p>>Observing the first image, it can be concluded that it exceeds the cumulative variance established in the variance threshold of 0.95, counting only with the first components when reaching 0.966. In the second representation, the weight that each attribute has in each component can be seen, which allows us to observe that rating, calories, potassium and vitamins are the attributes that contribute the most in the first 3 main components and therefore the most important."},{"unidad":"Caso","unit":"Case","titulo":"Ensambles","title":"Ensembles","descripcion":"","description":"","contenido":"p>>Los algoritmos de ensamble se caracterizan por generar m\xfaltiples modelos independientes de machine learning y combinar sus predicciones para obtener mejores performances.=,=h6>>Bagging=,=p>>Es un m\xe9todo de ensamble que se basa en crear muchas sub muestras tomadas con reemplazo, entrenar distintos modelos en ellos de alta varianza (t\xedpicamente CARTs) y dado un nuevo dataset calcular el promedio para cada predicci\xf3n en el caso de problemas de regresi\xf3n o mediante voto para clasificaci\xf3n. Para el caso de los \xe1rboles de decisi\xf3n, al usar bagging nos preocupamos menos porque uno en particular haga overfitting de los datos de forma tal de que se puede permitir que los \xe1rboles indivualmente crezcan profundamente y sin poda. =,=h6>>Random Forest=,=p>>Es uno de los algoritmo m\xe1s potentes de machine learing. Se basa en \xe1rboles de decisi\xf3n que utiliza bagging pero modificando a los \xe1rboles de decisi\xf3n en que en lugar de buscar los split points de forma \xf3ptima se limita este procedimiento a aprender de una muestra aleatoria de los predictores con el fin de que los \xe1rboles generados sean m\xe1s independientes entre s\xed.=,=h6>>Boosting=,=p>>Es otra t\xe9cnica de ensamble que busca generar un modelo fuerte a partir de varios clasificadores d\xe9biles. Esto se hace a partir de crear un modelo a partir de los datos de entrenamiento, luego creando un segundo modelo que mejore los errores del primer modelo y as\xed hasta que la predicci\xf3n es perfecta (pudiendo causar overfitting) o hasta que un m\xe1ximo de modelos son a\xf1adidos.=,=h6>>Ada Boost=,=p>>Es un algoritmo basado en boosting que es muy \xfatil para clasificaci\xf3n binaria y el modelo d\xe9bil que utiliza t\xedpicamente son \xe1rboles de decisi\xf3n de un nivel. Estos se entrenan poniendo pesos a cada ejemplo del dataset de entrenamiento, a\xf1adiendo peso si los modelos d\xe9biles clasifican equivocadamente para darle m\xe1s importancia en la siguiente iteraci\xf3n a esa instancia, y calculando una ponderaci\xf3n para el modelo d\xe9bil que luego que luego se utiliza en cada predicci\xf3n que este modelo haga. Lo cual se repite hasta un criterio de parada o hasta que no se pueda seguir mejorando la performance. Las predicciones se realizan calculando el promedio de las predicciones ponderadas de los modelos d\xe9biles clasificando seg\xfan se obtiene un valor positivo o negativo.=,=p>>A continuaci\xf3n aplicaremos Random Forest y Ada Boost con \xe1rboles de decisi\xf3n aplicado al sencillo dataset de clasificaci\xf3n binaria Banking en RapidMiner. Mediremos sus preformances y compararemos sus curvas ROC.=,=h5>>Dataset=,=p>>El conjunto de datos banking marketing cuenta con 17 atributos correspondientes a caracter\xedsticas de clientes para predecir si un cliente invierte en un dep\xf3sito a plazo fijo seg\xfan una campa\xf1a de marketing telef\xf3nico. Entre los datos hay 4521 ejemplos y no se encuentran missing values.=,=p>>Los atributos son las siguientes:=,=p>>- age (num\xe9rico): La edad del cliente.=,=p>>- job (categ\xf3rico): tipo de trabajo entre admin, unknown, unemployed, management, housemaid, entrepreneur, student, blue-collar, self-employed, retired, technician, services.=,=p>>- marital (categ\xf3rico): Estado civil entre married, divorced (divorciado y viudo se condieran dentro de esta clase) y single.=,=p>>- education (categ\xf3rico): Nivel educativo entre unknown, secondary, primary y tertiary.=,=p>>- default (categ\xf3rico): Si tiene el cr\xe9dito en default entre yes y no.=,=p>>- balance (num\xe9rico): promedio de balance anual en euros.=,=p>>- loan (categ\xf3rico): si el cliente tiene pr\xe9stamos entre yes y no.=,=p>>- contact (categorico): Medio de contacto entre unknown, telephone y cellular.=,=p>>- day (num\xe9rico): D\xeda del \xfaltimo contacto.=,=p>>- month (categ\xf3rico): Mes del \xfaltimo contacto.=,=p>>- duration (num\xe9rico): Duraci\xf3n del \xfaltimo contacto en segundos.=,=p>>- campaign (num\xe9rico): n\xfamero de conactos realizados con el cliente esta campa\xf1a.=,=p>>- pdays (num\xe9rico): n\xfamero de d\xedas desde que el cliente fue llamado desde una campa\xf1a previa en la que -1 significa que nunca fue contactado.=,=p>>- previous (num\xe9rico): n\xfamero de contactos realizados antes de esta campa\xf1a con el cliente.=,=p>>- poutcome (categ\xf3rico): Resultado de la \xfaltima campa\xf1a de marketing entre unknown, other, failure y success.=,=p>>- y (categ\xf3rico): El cliente se sucribi\xf3 al plazo fijo entre yes y no.=,=h5>>Operador=,=h6>>Descripciones=,=img>>1.jpg>>75%>>500=,=br=,=br=,=img>>2.jpg>>75%>>500=,=br=,=br=,=h6>>Par\xe1metros=,=p>>-Random Forest: Cuenta con number of trees que determina la cantidad de \xe1rboles generados, voting strategy utilizado para elegir la estrategia de predicci\xf3n entre los \xe1rboles del modelo y las configuraciones propias del componente de \xe1rbol de decisi\xf3n para seleccionar cualidades de aquellos que lo componen.=,=p>>-AdaBoost: Iterations utilizado para fijar el n\xfamero m\xe1ximo de iteraciones del algoritmo.=,=h5>>Modelo=,=img>>3.jpg>>75%>>600=,=br=,=br=,=h5>>Resultados=,=p>>Performance Random Forest.=,=img>>4.jpg>>90%>>175=,=br=,=br=,=p>>Performance AdaBoost=,=img>>5.jpg>>90%>>175=,=br=,=br=,=p>>Comparaci\xf3n entre curvas ROC=,=img>>6.jpg>>90%>>600=br=,=br=,=br=,=h4>>Conclusiones=,=p>>Observamos que para este caso particular tanto los valores de accuracy, precisi\xf3n de clases y recall de categor\xedas dieron practicamente iguales con leves ventajas para Random Forest aunque ambos algoritmos dieron muy buenos resultados en todos exceptuando la precisi\xf3n y sobre todo el recall para la clase yes fallando los dos en lo mismo.=,=p>>En donde si se pueden ver m\xe1s diferencias es en la comparaci\xf3n de las curvas ROC. En esta se puede observar que la curva de AdaBoost tiene un mucho mayor crecimiento de verdaderos positivos respecto a falsos positivos que Random Forest con lo cual resulta ser un mejor modelo.","content":"p>>Ensemble learning algorithms are characterized by generating multiple independent machine learning models and combining their predictions to obtain better performances.=,=h6>>Bagging=,=p>>It is an assembly method that is based on creating many sub-samples taken with replacement, train different models on them with high variance (typically CARTs) and given a new dataset calculate the average for each prediction in the case of regression problems or by voting for classification. For the case of decision trees, by using bagging we worry less about one particular overfitting the data in such a way that the individual trees can be allowed to grow deeply and without pruning. =,=h6>>Random Forest=,=p>>It is one of the most powerful machine learning algorithms. It is based on decision trees that use bagging but modifying the decision trees in that instead of searching for the split points optimally, this procedure is limited to learning from a random sample of the predictors so that the trees generated are more independent from each other.=,=h6>>Boosting=,=p>>This is another assembly technique that seeks to generate a strong model from several weak classifiers. This is done by creating a model from the training data, then creating a second model that improves the errors of the first model and so on until the prediction is perfect (which may cause overfitting) or until a maximum of models are added.=,=h6>>Ada Boost=,=p>>It is a boosting based algorithm that is very useful for binary classification and the weak model it typically uses is one level decision trees. These are trained by putting weights on each example in the training dataset, adding weight if weak models misclassify to give that instance more weight in the next iteration, and calculating a weight for the weak model that is then used in each prediction. that this model does. Which is repeated until a stop criterion or until it is not possible to continue improving the performance. The predictions are made by calculating the average of the weighted predictions of the weak models, classifying according to a positive or negative value.=,=p>>Next we will apply Random Forest and Ada Boost with decision trees applied to the simple binary classification dataset Banking in RapidMiner. We will measure their preformances and compare their ROC curves.=,=h5>>Dataset=,=p>>The banking marketing data set has 17 attributes corresponding to customer characteristics to predict whether a customer invests in a fixed-term deposit according to a telephone marketing campaign. Among the data there are 4521 examples and there are no missing values.=,=p>>The attributes are the following:=,=p>>- age (numeric): The age of the client.=,=p>>- job (categorical): type of job between admin, unknown, unemployed, management, housemaid, entrepreneur, student, blue-collar, self-employed, retired, technician, services.=,=p>>- marital (categorical): Marital status between married, divorced (divorced and widowed are considered within this class) and single.=,=p>>- education (categorical): Educational level between unknown, secondary, primary and tertiary.=,=p>>- default ( categorical): If the loan is in default between yes and no.=,=p>>- balance (numeric): average annual balance in euros.=,=p>>- loan (categorical): if the client has loans Between yes and no.=,=p>>- contact (categorical): Means of contact between unknown, telephone and cellular.=,=p>>- day (numeric): Day of last contact.=,=p>> - month (categorical): Month of the last contact.=,=p>>- duration (numeric): Duration of the last c oncontact in seconds.=,=p>>- campaign (numeric): number of contacts made with the client this campaign.=,=p>>- pdays (numeric): number of days since the client was called from a campaign previous in which -1 means that he was never contacted.=,=p>>- previous (numeric): number of contacts made before this campaign with the client.=,=p>>- poutcome (categorical): Result of the last marketing campaign between unknown, other, failure and success.=,=p>>- y (categorical): The client subscribed to the fixed term between yes and no.=,=h5>>Operador=,=h6>>Descriptions=,=img>>1.jpg>>75%>>500=,=br=,=br=,=img>>2.jpg>>75%>>500=,=br=,=br =,=h6>>Parameters=,=p>>-Random Forest: It has number of trees that determines the number of generated trees, voting strategy used to choose the prediction strategy between the model trees and the component\'s own configurations decision tree to select qualities of those that compose it.=,=p>>-AdaBoost: Iteration s used to set the maximum number of iterations of the algorithm.=,=h5>>Model=,=img>>3.jpg>>75%>>600=,=br=,=br=,=h5>>Results =,=p>>Performance Random Forest.=,=img>>4.jpg>>90%>>175=,=br=,=br=,=p>>Performance AdaBoost=,=img>>5.jpg>>90%>>175=,=br=,=br=,=p>>Comparison between ROC curves=,=img>>6.jpg>>90%>>600=br=,=br=,=br=,=h4>>Conclusions=,=p>>We observe that for this particular case both the accuracy, class precision and category recall values \u200b\u200bwere practically the same with slight advantages for Random Forest although both algorithms gave very good results in all except the precision and especially the recall for the yes class, both failing in the same thing.=,=p>>Where if you can see more differences is in the comparison of the ROC curves. In this it can be seen that the AdaBoost curve has a much higher growth of true positives compared to false positives than Random Forest, which turns out to be a better model."},{"unidad":"Caso","unit":"Case","titulo":"Enfermedad card\xedaca","title":"Heart disease","descripcion":"","description":"","contenido":"h2>>Contexto=,=p>>Las enfermedades cardiovasculares son la principal causa de muerte en el mundo seg\xfan la Organizaci\xf3n Mundial de la Salud (OMS) llev\xe1ndose un estimado de 17.9 millones de vidas cada a\xf1o lo cual supone un 32% de todas las muertes anuales alrededor del planeta. En el Uruguay seg\xfan datos oficiales del Ministerio de Salud P\xfablica (MSP), para cada a\xf1o del quinquenio (2015-2019)  anterior a la pandemia del SARS-CoV2, conocido coloquialmente como coronavirus, estas enfermedades motivaron entre un 25% y un 27% de la totalidad de decesos conforme a lo que pudo identificar nuestro sistema de salud.=,=p>>Entre las principales causas de las enfermedades coronarias identificadas por el Centro de Control y Prevenci\xf3n de Enfermedades de los Estados Unidos (CDC) se encuentran alta presi\xf3n arterial, colesterol elevado, fumar, diabetes, sobrepeso u obesidad, dietas poco saludables, poca actividad f\xedsica y excesivo uso consumo de alcohol.=,=p>>Por otra parte, seg\xfan datos del CDC se observa que esta enfermedad se da con mayor frecuencia para los hombres que para las mujeres con un 13.6% frente a un 8.4% de las respectivas poblaciones habiendo reportado padecimientos coronarios y que tambi\xe9n se da con m\xe1s ocurrencia al envejecer llegando hasta la franja a partir de los 65 a\xf1os en la que un 17% de esa poblaci\xf3n report\xf3 tenerla.=,=p>>La mayor sugerencia realizada por el sector de la salud al respecto de los padecimientos del coraz\xf3n es tratar de prevenirlos lo m\xe1ximo posible teniendo una vida saludable. De todas maneras, existen tratamientos como prescripci\xf3n de cambios de rutina, medicaci\xf3n y hasta procedimientos quir\xfargicos dependiendo de los diversos factores que est\xe9n causando la enfermedad, pero para todos ellos es fundamental tener un diagn\xf3stico acertado y a tiempo con un m\xe9dico.=,=p>>=,=br=,=h2>>Entendimiento del negocio=,=hr=,=p>>En este caso de estudio se desarrollar\xe1n todos los pasos CRISP-DM para la problem\xe1tica supervisada y de clasificaci\xf3n acerca de la predicci\xf3n de enfermedades card\xedacas con fines acad\xe9micos, sin que exista inicialmente la finalidad de poner en producci\xf3n el resultado conseguido. A continuaci\xf3n se pasar\xe1 por las etapas de entendimiento de datos, preparaci\xf3n, modelado y evaluaci\xf3n para la realizaci\xf3n de predicciones acerca de si un paciente cuenta o no con una enfermedad del coraz\xf3n a partir de algunas de sus caracter\xedsticas empleando para la herramienta RapidMiner.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>Para el trabajo se cuenta con cuatro bases de datos vinculadas al diagn\xf3stico de enfermedades del coraz\xf3n y cualidades de los pacientes provenientes respectivamente de V.A Medical Center (California, Estados Unidos) autor: Robert Detrano M.D. Ph.D,  Cleveland Clinic Foundation (Ohio, Estados Unidos) autor: Robert Detrano M.D. Ph.D, University Hospital (Zurich, Suiza) autor: William Steinbrunn M.D y Hungarian Institute of Cardiology (Budapest, Hungr\xeda) autor: Andras Janosi M.D.=,=p>> Los datos fueron analizados en la herramienta RapidMiner y para ello hubo que realizarles un pre procesamiento en Python armando documentos en formato .csv con un formato con una cabecera conteniendo los nombres de los atributos y los registros de un paciente por fila a partir de cada uno de los cuatro archivos originales individualmente en los que los datos est\xe1n separados por espacios y en varios renglones para cada individuo del cual se cuenta con informaci\xf3n.=,=code>>.lines = open(\\"switzerland.data\\", \\"r\\", errors=\'ignore\')=,=code>>.writer = open(\\"cleanedSwitzerland.data\\", \'w\', encoding=\\"cp437\\", errors=\'ignore\')=,=br=,=code>>.header = (\\"id,ccf,age,sex,painloc,painexer,relrest,pncaden,cp,trestbps,htn,chol,smoke,cigs,years,\\"=,=br=,=code>>.\xa0\xa0        \\"fbs,dm,famhist,restecg,ekgmo,ekgday,ekgyr,dig,prop,nitr,pro,diuretic,proto,thaldur,\\"=,=br=,=code>>.\xa0\xa0        \\"thaltime,met,thalach,thalrest,tpeakbps,tpeakbpd,dummy,trestbpd,exang,xhypo,oldpeak,\\"=,=br=,=code>>.\xa0\xa0        \\"slope,rldv5,rldv5e,ca,restckm,exerckm,restef,restwm,exeref,exerwm,thal,thalsev,thalpul,\\"=,=br=,=code>>.\xa0\xa0        \\"earlobe,cmo,cday,cyr,num,lmt,ladprox,laddist,diag,cxmain,ramus,om1,om2,rcaprox,rcadist,\\"=,=br=,=code>>.\xa0\xa0        \\"lvx1,lvx2,lvx3,lvx4,lvf,cathef,junk\\")=,=br=,=code>>.writer.write(header+\'\\n\')=,=br=,=code>>.info = []=,=br=,=code>>.print(lines)=,=br=,=code>>.for line in lines:=,=br=,=code>>.  lineList = line[:-1].split(\\" \\")=,=br=,=code>>.  if\'name\' not in lineList:=,=br=,=code>>.\xa0    info.extend(lineList)=,=br=,=code>>.  else:=,=br=,=code>>.\xa0    info.extend(lineList[:-1])=,=br=,=code>>.\xa0    infoWithMV = [\\"?\\" if x in [\\"-9\\",\\"-9.\\",\\"-9.0\\"]  else x for x in info]=,=br=,=code>>.\xa0    infoToWrite = \\",\\".join(infoWithMV)=,=br=,=code>>.\xa0    writer.write(infoToWrite+\'\\n\')=,=br=,=code>>.\xa0    info = []=,=br=,=code>>.writer.close()=,=br=,=code>>.lines.close()=,=br=,=br=,=h6>>Descargas:=,=a>>Descripci\xf3n de datos>>cardiac description.txt>>d=,=br=,=a>>Datos>>cardiac data.zip>>d=,=br=,=br=,=p>>Los datasets cuentan con una totalidad de 920 ejemplos etiquetados para los cuales hay 76 atributos num\xe9ricos que especifican diversas caracter\xedsticas de los pacientes como su edad(age), sex (sexo), colesterol (chol), az\xfacar en sangre en reposo (fbs), etc. La variable objetivo del problema llamada num la cual representa el diagn\xf3stico de enfermedad card\xedaca. Esta \xfaltima tiene un valor de 0 si no hay presencia de enfermedad en el individuo y valores del 1 al 4 seg\xfan diferentes tipos de padecimientos del coraz\xf3n.=,=p>>A pesar de la gran cantidad de atributos 10 de ellos no fueron utilizados, tal y como se expresa en la descripci\xf3n de los datasets, siendo estos thalsev, thalpul, earlobe, lvx1, lvx2, lvx3, lvx4, lvf, cathef y junk. Por otra parte, hay un gran n\xfamero de predictores con valores faltantes los cuales vienen represntados con el valor -9. Existen 20 de ellos con m\xe1s de un 50% de missing values respecto a la totalidad de los registros.=,=br=,=h2>>Preparaci\xf3n de datos=,=hr=,=p>>Lo primero que se realiz\xf3 fue importar los datos correspondientes a las cuatro bases en la herramienta RapidMiner comprobando que todos se cargaban con los mismos tipos para cada uno de sus atributos. Posteriormente se procedi\xf3 a juntarlas utilizando el componente Append y obteniendo as\xed el conjunto con todos los datos etiquetados.=,=p>>Se declar\xf3 a los registros con valor -9 como missing values y primeramente se descartaron las columnas de id y las que ten\xedan m\xe1s de 50% de valores faltantes considerando que la mayor parte de estas concierne atributos no utilizados seg\xfan lo expresado en la descripcion de los datos y a que estas pueden generar dificultades para el aprendizaje de los modelos a crear, los restantes fueron reemplazados por el promedio de los datos correspondientes a cada predictor seg\xfan el caso.=,=p>>Otros procedimientos realizados fueron la remoci\xf3n de outliers utilizando los operadores Detect Outliers y Filter Examples para quitar los 10 outliers m\xe1s lejanos de los 10 vecinos m\xe1s cercanos seg\xfan distancia euclideana, el cambio de tipo de la variable objetivo de num\xe9rica a polinomial usando Numerical to Polynomial y la estandarizaci\xf3n de todos los predictores con el componente Normalize.=,=br=,=h2>>Modelado=,=hr=,=p>>Para los modelos a desarrollar se consideraron aquellos supervisados y de clasificaci\xf3n que soportaran variables polinomiales como Naive Bayes, \xc1rboles de Decisi\xf3n, K-Nearest Neighbors y los algoritmos de ensamble siendo estos uno de tipo bagged como Random Forest y otro de boosting como Gradient Boosted Trees.=,=p>> De los estudiados fueron descartados K-Nearest Neighbors debido a la alta dimensionalidad de los datos y \xc1rboles de Decisi\xf3n al incluir Random Forest dado que este \xfaltimo es una evolucion del anterior en ser menos propenso al overfitting al estar compuesto por muchos \xe1rboles con predictores seleccionados randomicamente con reemplazo en cada uno brindando as\xed generalizaci\xf3n al modelo.=,=p>>Por lo tanto, se eligieron los modelos de Naive Bayes como l\xednea de base, Random Forest y Gradient Boosting Trees, aplicando en todos los casos Cross Validation de 10 folds con la misma random seed local n\xfamero 1992 y midiendo las performances para cada uno.=,=h6>>Proceso en RapidMiner=,=img>>cardiac-0.jpg>>100%>>550=,=a>>Descargar proceso>>Heart Disease.rmp>>d=,=br=,=br=,=h2>>Resultados=,=hr=,=p>>Para evaluar los modelos y medir su rendimiento se estudiaron las matrices de confusi\xf3n obteniendo las siguientes tablas.=,=h6>>Naive Bayes=,=img>>cardiac-1.jpg>>100%>>250=,=br=,=br=,=h6>>Random Forest=,=img>>cardiac-2.jpg>>100%>>250=,=p>>Params: num trees = 1000, criterion = gain_ratio, min leaf size = 2, min size for split = 4 y max depth = 10.=,=h6>>Gradient Boosting Trees=,=img>>cardiac-3.jpg>>100%>>250=,=p>>Params: num trees = 50, max depth = 5, min rows = 10, min split imp. = 1.0E-5, num bins = 20, lear. rate = 0.01. =,=br=,=h2>>Conclusiones=,=hr=,=p>>Se puede observar que los que dan mejores resultados de accuracy son los de ensambles con Gradient Boosting Trees en la delantera y Random Forest muy cerca. Igualmente, si bien la performance global siempre es muy importante, para este caso de diagn\xf3stico de enfermedades podr\xeda llegarse a considerar hasta de mayor relevancia la presici\xf3n para la clase 0 que representa el pocentaje de acierto en la predicci\xf3n de ausencia de enfermedad dado que no se pueden permitir altos valores de falsos positivos en problemas tan sensibles como la salud del coraz\xf3n en la que si ocurren pueden llegar hasta ser terminales para los pacientes. En cuanto a esto se puede destacar que Naive Bayes tiene la presici\xf3n m\xe1s alta para la clase 0 aunque tambi\xe9n es el que tiene menos recall para esta identificando la mayor cantidad de personas que no tienen enfermedades card\xedacas como que si las tuvieran de entre los modelos realizados.=,=p>>Para decidir entre que modelo es mejor entre Naive Bayes y su notable predicci\xf3n para la clase 0 y Gradient Boosting Machines y su incre\xedble acurracy prediciendo mejor para todas las dem\xe1s clases es necesario conocer que tan profunda es la diferenciar entre enfermedades de coraz\xf3n tipo 4, 3, 2 y 1. Si la diferencia es grande en cuanto al tratamiento requerido y las posibilidades del paciente entonces el mejor algoritmo es Gradient Boosting Machines, pero si no es as\xed entre estos modelos yo eligir\xeda Naive Bayes para asegurarme de tener la menor cantidad de falsos negativos de la enfermedad. Lamentablemente no se dispone de esta informaci\xf3n en el dataset y en un caso real habr\xeda que consultar equipos m\xe9dicos para tomar la decisi\xf3n adecuada.","content":"h2>>Context=,=p>>Cardiovascular diseases are the main cause of death in the world according to the World Health Organization (WHO), taking an estimated 17.9 million lives each year, which represents 32% of all annual deaths around the planet. In Uruguay, according to official data from the Ministry of Public Health (MSP), for each year of the five-year period (2015-2019) prior to the SARS-CoV2 pandemic, colloquially known as coronavirus, these diseases caused between 25% and 27% of all deaths according to what our health system could identify.=,=p>>Among the main causes of coronary diseases identified by the United States Center for Disease Control and Prevention (CDC) are high blood pressure, high cholesterol, smoking, diabetes, overweight or obesity, unhealthy diets, little physical activity and excessive use of alcohol.=,=p>>On the other hand, according to data from the CDC, it is observed that this disease occurs with greater frequency for men than for women with 13.6% compared to 8.4% of the respective populations having reported coronary diseases and that also occurs with more occurrence as they age, reaching the range from of the 65 years in which 17% of that population reported having it.=,=p>>The greatest suggestion made by the health sector regarding heart diseases is to try to prevent them as much as possible by having a healthy life . In any case, there are treatments such as prescription of routine changes, medication and even surgical procedures depending on the various factors that are causing the disease, but for all of them it is essential to have an accurate and timely diagnosis with a doctor.=,=p>>=,=br=,=h2>>Understanding the business=,=hr=,=p>>In this case study, all the CRISP-DM steps will be developed for the supervised and classification problem about disease prediction for academic purposes, without initially having the purpose of putting the achieved result into production. Next, it will go through the stages of data understanding, preparation, modeling and evaluation to make predictions about whether or not a patient has heart disease based on some of its characteristics using the RapidMiner tool. =,=br=,=h2>>Data knowledge=,=hr=,=p>>For the work, there are four databases linked to the diagnosis of heart diseases and qualities of the patients, respectively from the VA Medical Center ( California, United States) author: Robert Detrano MD Ph.D, Cleveland Clinic Foundation (Ohio, United States) author: Robert Detrano MD Ph.D, University Hospital (Zurich, Switzerland) author: William Steinbrunn MD and Hungarian Institute of Cardiology ( Budapest, Hungary) author: Andras Janosi MD=,=p>> The data was analyzed in the RapidMiner tool and for this it was necessary to perform a pre-processing in Python, putting together documents in .csv format with a format with a header containing the names of the attributes and the records of a patient per row from each of the four original files individually in which the data is separated by spaces and in several rows for each individual for whom information is available.=,=code>>.lines = open(\\"switzerland.data\\", \\"r\\", errors=\'ignore\')=,=code>>.writer = open(\\"cleanedSwitzerland.data\\", \'w \', encoding=\\"cp437\\", errors=\'ignore\')=,=br=,=code>>.header = (\\"id,ccf,age,sex,painloc,painexer,relrest,pncaden,cp, trestbps,htn,chol,smoke,cigs,years,\\"=,=br=,=code>>.\xa0\xa0 \\"fbs,dm,famhist,restecg,ekgmo,ekgday,ekgyr,dig,prop,nitr ,pro,diuretic,proto,thaldur,\\"=,=br=,=code>>.\xa0\xa0 \\"thaltime,met,thalach,thalrest,tpeakbps,tpeakbpd,dummy,trestbpd,exang,xhypo,oldpeak, \\"=,=br=,=code>>.\xa0\xa0 \\"slope,rldv5,rldv5e,ca,restckm,exerckm,restef,restwm,exeref,exerwm,thal,thalsev,thalpul,\\"=,= br=,=code>>.\xa0\xa0 \\"earlobe,cmo,cday,cyr,num,lmt,ladprox,laddist,diag,cxmain,ramus,o m1,om2,rcaprox,rcadist,\\"=,=br=,=code>>.\xa0\xa0 \\"lvx1,lvx2,lvx3,lvx4,lvf,cathef,junk\\")=,=br=,= code>>.writer.write(header+\'\\n\')=,=br=,=code>>.info = []=,=br=,=code>>.print(lines)=,=br=,=code>>.for line in lines:=,=br=,=code>>. lineList = line[:-1].split(\\" \\")=,=br=,=code>>. if\'name\' not in lineList:=,=br=,=code>>.\xa0 info.extend(lineList)=,=br=,=code>>. else:=,=br=,=code>>.\xa0 info.extend(lineList[:-1])=,=br=,=code>>.\xa0 infoWithMV = [\\"?\\" if x in [\\"-9\\",\\"-9.\\",\\"-9.0\\"] else x for x in info]=,=br=,=code>>.\xa0 infoToWrite = \\",\\" .join(infoWithMV)=,=br=,=code>>.\xa0 writer.write(infoToWrite+\'\\n\')=,=br=,=code>>.\xa0 info = []=,=br=,=code>>.writer.close()=,=br=,=code>>.lines.close()=,=br=,=br=,=h6>>Downloads:=,=a>>Description data>>cardiac description.txt>>d=,=br=,=a>>Data>>cardiac data.zip>>d=,=br=,=br=,=p>>The datasets have a totality of 920 labeled examples for which there are 76 numerical attributes that specify various characteristics of the patients such as their age (age), sex (sex), cholesterol (chol), resting blood sugar (fbs), etc. The objective variable of the problem called num which represents the diagnosis of heart disease. The latter has a value of 0 if there is no disease in the individual and values \u200b\u200bfrom 1 to 4 according to different types of heart disease.=,=p>>Despite the large number of attributes, 10 of them were not used , as expressed in the description of the datasets, these being thalsev, thalpul, earlobe, lvx1, lvx2, lvx3, lvx4, lvf, cathef and junk. On the other hand, there is a large number of predictors with missing values \u200b\u200bwhich are represented by the value -9. There are 20 of them with more than 50% missing values \u200b\u200bwith respect to all the records.=,=br=,=h2>>Data preparation=,=hr=,=p>>The first thing that was done was import the data corresponding to the four databases in the RapidMiner tool, verifying that they were all loaded with the same types for each of their attributes. Subsequently, they were joined using the Append component and thus obtaining the set with all the labeled data.=,=p>>The records with value -9 were declared as missing values \u200b\u200band first the id columns and those with more than 50% of missing values \u200b\u200bconsidering that most of these concern attributes not used as expressed in the description of the data since these can generate difficulties for learning the models to be created, the rest were replaced by the average of the data corresponding to each predictor according to the case.=,=p>>Other procedures carried out were the removal of outliers using the Detect Outliers and Filter Examples operators to remove the 10 farthest outliers from the 10 closest neighbors according to Euclidean distance, the changing the type of the target variable from numeric to polynomial using Numerical to Polynomial and standardizing all predictors with the comp Normalize component.=,=br=,=h2>>Modelado=,=hr=,=p>>For the models to be developed, supervised and classification models that supported polynomial variables such as Naive Bayes, Decision Trees, K- Nearest Neighbors and the assembly algorithms, one being bagged as Random Forest and another boosting as Gradient Boosted Trees.=,=p>> Of those studied, K-Nearest Neighbors were discarded due to the high dimensionality of the data and Trees of Decision by including Random Forest since the latter is an evolution of the former in being less prone to overfitting as it is composed of many trees with randomly selected predictors with replacement in each one, thus providing generalization to the model.=,=p>>Therefore Therefore, the Naive Bayes models were chosen as a baseline, Random Forest and Gradient Boosting Trees, applying in all cases 10-fold Cross Validation with the same local random seed number 1992 and measuring the performances. it is for everyone.=,=h6>>Process in RapidMiner=,=img>>cardiac-0.jpg>>100%>>550=,=a>>Download process>>Heart Disease.rmp>>d=,=br=,=br=,=h2>>Results=,=hr=,=p>>To evaluate the models and measure their performance, the confusion matrices were studied, obtaining the following tables.=,=h6>>Naive Bayes=,=img>>cardiac-1.jpg>>100%>>250=,=br=,=br=,=h6>>Random Forest=,=img>>cardiac-2.jpg>>100% >>250=,=p>>Params: num trees = 1000, criterion = gain_ratio, min leaf size = 2, min size for split = 4 and max depth = 10.=,=h6>>Gradient Boosting Trees=,= img>>cardiac-3.jpg>>100%>>250=,=p>>Params: num trees = 50, max depth = 5, min rows = 10, min split imp. = 1.0E-5, num bins = 20, read. rate = 0.01. =,=br=,=h2>>Conclusions=,=hr=,=p>>It can be seen that the ones that give the best accuracy results are those of assemblages with Gradient Boosting Trees in the lead and Random Forest very close. Likewise, although the global performance is always very important, for this case of diagnosis of diseases, the accuracy for class 0 could be considered even more relevant, which represents the percentage of success in predicting the absence of disease since they can allow high values \u200b\u200bof false positives in problems as sensitive as heart health in which, if they occur, they can even be terminal for patients. Regarding this, it can be highlighted that Naive Bayes has the highest precision for class 0, although it is also the one with the least recall for this, identifying the largest number of people who do not have heart disease as if they had it among the models made. .=,=p>>To decide between which model is better between Naive Bayes and his remarkable prediction for class 0 and Gradient Boosting Machines and his incredible acurracy predicting better for all other classes it is necessary to know how deep the difference between heart diseases type 4, 3, 2 and 1. If the difference is great in terms of the treatment required and the possibilities of the patient, then the best algorithm is Gradient Boosting Machines, but if it is not the case between these models, I would choose Naive Bayes to make sure to have the fewest false negatives for the disease. Unfortunately, this information is not available in the dataset and in a real case, medical teams would have to be consulted to make the appropriate decision."},{"unidad":"Caso","unit":"Case","titulo":"Bienes ra\xedces en Ames Iowa","title":"Ames housing","description":"","descripcion":"","contenido":"h2>>Contexto=,=p>>La industria de bienes ra\xedces o real estate es una de las que genera m\xe1s actividad y crecimiento en el mundo. En los Estados Unidos seg\xfan datos anualizados del Bureau of Economic Analyisis (BEA) para el segundo cuarto de 2021 el sector vinculado a la vivienda es una de las industrias que genera mayor actividad econ\xf3mica en dicho pa\xeds con un PIB de 2.908 trillones de d\xf3lares. Conforme datos tambi\xe9n anualizados del FMI de octubre 2021, esta cifra supera el PIB de pa\xedses como Espa\xf1a, Italia, Brasil, Australia, Canad\xe1, Rusia, Corea del Sur y equivale a 48.38 veces el de Uruguay.=,=p>>El valor de compra de una propiedad depende de muchos factores entre los cuales los expertos destacan los siguientes.=,=p>>Localizaci\xf3n: Uno de los principales est\xe1 vinculado a la famosa frase de la industria de bienes ra\xedces \u201clocation, location, location\u201d haciendo referencia a que gran parte del valor de un inmueble depende la calidad de oportunidades y servicios que se puedan obtener viviendo en ese lugar como por ejemplo posibilidades de obtener un buen empleo, seguridad, proximidad a escuelas de calidad, etc.=,=p>>Espacio: Es un factor fundamental para determinar su valor haciendo especial \xe9nfasis en el espacio utilizable que es aqu\xe9l en el que no se cuenta el \xe1rea correspondiente a \xe1ticos, s\xf3tanos y los lugares que las personas rara vez utilizan en su d\xeda a d\xeda. Muchas personas eligen vivir en suburbios en los que pueden disponer de interiores, jardines o fondos m\xe1s amplios que vivir en el centro de la ciudad en una inmueble m\xe1s peque\xf1o por m\xe1s que esto le quite la posibilidad de acceder a alguna oportunidad o servicio con la misma facilidad. Esto se puede ver en gran extensi\xf3n en pa\xedses como Estados Unidos o Canad\xe1, pero tambi\xe9n en Uruguay con Ciudad de la Costa.=,=p>>Formato: Los compradores est\xe1n dispuestos a pagar por conseguir una vivienda que sea acorde a sus necesidades actuales o a sus perspectivas de futuro como ser\xeda el caso de una familia que tiene expectativas de tener m\xe1s hijos y busca una casa lo suficientemente grande para ello. Dentro de estas caracter\xedsticas se destacan algunas como el n\xfamero de habitaciones disponibles, la cantidad de ba\xf1os y si el inmueble tiene garage o no. Por otra parte, el formato en si de la vivienda est\xe1 relacionado fuertemente a su precio, no es lo mismo una casa que un apartamento con todas las variantes que estos pueden tener.=,=p>>Estado: El estado de una propiedad es un factor fundamental para su precio siendo esta es la raz\xf3n por la cual es conveniente econ\xf3micamente reacondicionar una propiedad antes de venderla. Por otra parte, a mayor antig\xfcedad las construcciones son m\xe1s propensas a tener defectos y por lo tanto se valora la modernidad.=,=br=,=h2>>Entendimiento del negocio=,=hr=,=p>>Este caso de estudio tiene como finalidad poder desarrollar el modelo CRISP-DM sobre un problema con fines acad\xe9micos y no teniendo como objetivo un deployment o que forme parte de un producto m\xe1s grande.=,=p>>Se realizar\xe1 preparaci\xf3n de datos, modelado y evaluaci\xf3n para predecir el valor de viviendas a partir de caracter\xedsticas de las propiedades o de sus entornos utilizando Python y librer\xedas como SciKitLearn, Pandas, Numpy, Seaborn, entre otras. El c\xf3digo completo al que se hace referencia en este trabajo se encuentra disponible en la \xfaltima secci\xf3n de Anexo.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>Para este proyecto se cuenta con datos de venta de propiedades de la localidad de 66.000 habitantes llamada Ames del estado de Iowa Estados Unidos entre los a\xf1os 2006 y 2010 obtenidos del Ames City Assessor\xb4s Office y preprocesados por Dean De Cock PhD en Probabilidad de la Iowa State University. Los registros se encuentran en dos datasets, uno de training y uno de test.=,=h6>>Descargas:=,=a>>Descripci\xf3n de datos>>data_description.txt>>d=,=br=,=a>>train.csv>>train.csv>>d=,=br=,=a>>test.csv>>test.csv>>d=,=br=,=br=,=h4>>An\xe1lisis de atributos=,=p>>La estructura de los datasets a utilizar tiene 78 predictores m\xe1s la variable objetivo num\xe9rica continua en el caso del de train. Esta se querr\xe1 predecir con el modelado a desarrollar, siendo de esta manera un problema supervisado de regresi\xf3n.=,=p>>De las 78 variables de entrada es muy importante tener en consideraci\xf3n sus diferentes tipos que en este caso son definidos en la descripci\xf3n de datos para cada uno. De esta manera se cuenta con:=,=p>>- 44 categ\xf3ricas (23 nominales y 21 ordinales)=,=p>>- 34 num\xe9ricas (19 continuas y 15 discretas)=,=p>> Aunque las variables nominales y ordinales son categ\xf3ricas se diferencian en que las primeras son ordenables mientras que las segundas son simplemente caracter\xedsticas cuyos valores dificimente son comparables entre ellos en el sentido de que uno se pueda considerar mayor o menor que el otro de por s\xed.=,=p>>Una mejor explicaci\xf3n se podr\xeda dar considerando atributos del problema con ExterQual (ordinal) y Foundation (nominal).=,=p>>- ExterQual: hace referencia a la calidad de los materiales del exterior de la casa y sus valores posibles son Ex (excelente), Gd (bueno), TA (promedio), Fa (justo), Po (pobre) por lo cual es f\xe1cil de trasladar a una escala en la cual los primeros valores son mayores a los \xfaltimos dado que es indiscutible que en cualquier caso una calidad excelente es mejor que una promedio o una pobre.=,=p>>- Foundation: describe a los cimientos de una casa contando con las clases BrkTil (ladrillos y tejas), CBlock (bloque de cemento), PConc (concreto), Slab (losa), Stone (piedra) y Wood (madera). Dado que en algunos terrenos podr\xeda ser mejor contar con cimientos de un tipo y en otros de otro con lo que el orden depender\xeda de circunstancias ajenas a la variable en si dificilmente pueda ser ordenable.=,=br=,=h4>>An\xe1lisis de ejemplos=,=p>>En cuanto a la cantidad de ejemplos, hay 1460 datos etiquetados en el train y 1459 sin etiquetar en el test. El caso de estudio se enfocar\xe1 en trabajar con el dataset de entrenamiento dado que este nos permite medir la performance al desarrollar modelos de predicci\xf3n comparando los valores obtenidos por los mismos con las etiquetas brindadas. De esta forma todo el an\xe1lisis que se realizar\xe1 a continuaci\xf3n refiere a ese conjunto de datos.=,=br=,=h5>>Datos Faltantes=,=p>>Analizando los datos faltantes se puede notar que todos corresponden a la variables continua LotFrontage (28), GarageYrBlt (81) y MasVnrArea (8).=,=img>>7.jpg>>25%>>150 =,=p>>A pro\xf3sito, es conveniente destacar que en muchas variables categ\xf3ricas existe la palabra NA como uno de los valores posibles siendo un ejemplo de ellos GarageType en cuyo caso se utiliza para referir a que las propiedades que no tienen garage. Estas observaciones con NA no deben ser confundidas con datos faltantes.=,=br=,=h5>>Correlaciones=,=p>>Es interesante observar la correlaci\xf3n que mantienen tanto cada uno con la variable objetivo como entre s\xed. =,=p>>Estudiaremos la matriz de correlaci\xf3n generada a partir del coeficiente de Pearson que mide la correlaci\xf3n lineal entre dos variables en una escala entre -1 y 1 interpret\xe1ndose como cuanto m\xe1s cerca del cero menor dependencia lineal y cuanto m\xe1s lejos mayor ya sea directa en el caso de la aproximaci\xf3n al 1 u opuesta en los acercamientos al -1. Este coeficiente solo puede ser calculado para variables num\xe9ricas o en su defecto, como se har\xe1 a continuaci\xf3n, se pueden agregar tambi\xe9n a las ordinales tras convertirlas a una escala num\xe9rica. =,=p>>Filtrando entre los atributos que tienen una correlaci\xf3n mayor a 0.5 o menor a -0.5 respecto a la variable objetivo seg\xfan Pearson se obtienen de forma ordenada los siguientes 13 predictores.=,=img>>ames-1.jpg>>25%>>250=,=br=,=br=,=p>>Realizando los c\xe1lculos para las correlaciones con la variable objetivo con 55 predictores entre los num\xe9ricos y los ordinales tras haber sido codificados para que tambi\xe9n lo sean, solamente 13 muestran una predicci\xf3n superior a 0.5 con esta lo cual es un 23%. Adem\xe1s, estas realaciones todas son de forma directa y ninguna opuesta.=,=p>>A continuaci\xf3n se calculan las correlaciones lineales entre los predictores resultando esta matriz de Pearson.=,=img>>ames-2.jpg>>100%>>400=,=br=,=br=,=p>>En cuanto a las correlaciones entre los propios predictores se obtuvieron valores positivos en todos los casos con lo cual todos tienen una cierta dependencia lineal directa por m\xe1s m\xednima que pueda ser. Se consideraron a los valores superiores a >0.66 como correlaciones altas lo cual se da entre los pares GrLivArea y TotRmsAvgGrd con 0.82, GarageArea y GarageCars con 0.88, TotalBsmtSF y 1stFlrSF con 0.81.=,=br=,=h5>>Outliers=,=p>>Estudiando las distribuciones de los atributos que tienen alta correlaci\xf3n con la variable a predecir y baja correlaci\xf3n entre s\xed se pueden observar outliers para GrLivArea en sus valores sobre 4000 (esto es expresado por el propio Dean De Cock en su paper sobre el problema) y en TotalBsmtSF por sobre 3000 como se muestra en las representaciones a continuaci\xf3n.=,=img>>ames-3.jpg>>50%>>300=,=img>>ames-4.jpg>>50%>>300=,=br=,=br=,=br=,=h5>>Sesgos=,=p>>Visualizando los histogramas se pudieron ver sesgos hacia la derecha en YearBuilt y YearRemodAdd los cuales corresponden respectivamente al a\xf1o de construcci\xf3n y al de la \xfaltima remodelaci\xf3n de las propiedades. Se puede ver que en el registro hay bastantes m\xe1s datos de casas hechas o remodeladas hace pocos a\xf1os con un 40% y un 59% de la muestra con a\xf1o posterior a 1980 en cada caso.=,=img>>ames-5.jpg>>50%>>300=,=img>>ames-6.jpg>>50%>>300=,=br=,=br=,=br=,=h5>>Distribuciones de atributos nominales=,=p>>Al realizar el an\xe1lisis de las variables nominales es \xfatil observar sus distintas categor\xedas en relaci\xf3n a la variable objetivo lo cual es posible con diagramas de caja que nos dejan visualizar cuanta variabilidad hay en el dataset entre las clases de una categor\xeda nominal respecto a la variable objetivo con el rango, la mediana y los datos at\xedpicos que tienen. Si la variabilidad es mucha entonces es una buena variable para aprender de ella.=,=p>>Los diagramas m\xe1s destacables se encuentran posteriormente. =,=p>>-Neighborhood=,=img>>ames-13.jpg>>80%>>400=,=br=,=br=,=p>>-HouseStyle=,=img>>ames-14.jpg>>80%>>300 =,=br=,=br=,=p>>-Foundation=,=img>>ames-8.jpg>>80%>>300=,=br=,=br=,=h2>>Preparaci\xf3n de datos=,=hr=,=p>>=,=h5>>Conjuntos de entrenamiento generados=,=p>>Se armaron 3 conjuntos de datos para entrenamiento distintas caracter\xedsticas para aplicar modelos sobre ellos y poder realizar comparaciones.=,=p>>- Datos de entrenamiento 1: Contiene todas los predictores del dataset con las variables num\xe9ricas y categ\xf3ricas codificadas y sin datos faltantes.=,=p>>- Datos de entrenamiento 2: Con atributos seleccionados como se explicar\xe1 a continuaci\xf3n, variables num\xe9ricas y categ\xf3ricas codificadas, pero sin que se les haya realizado m\xe1s pre procesamiento. Su objetivo es probar el feature selection hecho.=,=p>>- Datos de entrenamiento 3: Comprende los mismos atributos seleccionados para el caso anterior, variables num\xe9ricas y categ\xf3ricas codificadas pero adem\xe1s sobre este se realizaron otras t\xe9cnicas de pre procesamiento de datos mostradas debajo. Su finalidad es probar los m\xe9todos de pre processing aplicados. =,=br=,=h5>>Carga de datos=,=p>>Los datos fueron cargados y se reemplazaron los valores con NA de las variables categ\xf3ricas por una palabra que significa que no hay eso de lo que trata el predictor lo cual puede tratarse de garages, s\xf3tanos, etc.=,=br=,=h5>>Codificaci\xf3n atributos ordinales (para datos de entrenamiento 1, 2 y 3)=,=p>>Los atributos ordinales se codificaron transformando sus escalas a discretas con el OrdinalEncoder() de SKLearn, lo importante aqu\xed es mantener el ordenamiento y que no pase como en Python que codifica al rev\xe9s y luego al calcular las correlaciones por ejemplo con la variable objetivo se obtiene el valor opuesto al que deber\xeda pero igualmente esto puede arreglarse f\xe1cil como se muestra en el Anexo.=,=br=,=h5>>Selecci\xf3n de atributos num\xe9ricos y ordinales (para datos de entrenamiento 2 y 3)=,=p>>Nos quedamos con los predictores que mantuvieran una correlaci\xf3n superior a 0.5 o inferior a -0.5 respecto al atributo objetivo y  una correlaci\xf3n con las dem\xe1s que sea menor a 0.66 o mayor a -0.66 en todos los casos. Obteniendo las siguientes variables.=,=img>>vars.jpg>>100%>>40=,=br=,=br=,=br=,=h5>>Selecci\xf3n de atributos nominales (para datos de entrenamiento 2 y 3) =,=p>>Se observaron diagramas diagramas de caja analizando cuales brindan la mayor variabilidad respecto a la variable objectivo. De esto se consiguieron los predictores HouseStyle, Foundation, CentralAir y Neighborhood. Los dem\xe1s se descartaron por por no poder justificar que tengan una variabilidad considerable entre sus distintas clases respecto a la variable objetivo que sea determinante para el problema.=,=br=,=h5>>Codificaci\xf3n de atributos nominales (para datos de entrenamiento 1, 2 y 3)=,=p>>Los atributos nominales fueron codificados con one hot encoding que se encuentra en SKLearn y en Pandas. Lo que hace es generar dummy variables que no son otra cosa que nuevos atributos binarios por cada uno de los diferentes valores que pueden serguir las variables nominales seleccionadas lo cual hace que las variable original no pierda su naturaleza nominal de no tener un orden definido.=,=br=,=h5>>Remoci\xf3n de outliers (para datos de entrenamiento 3)=,=p>>Tras haber seleccionado los atributos con los que se trabajar\xe1 y fundament\xe1ndonos en el an\xe1lisis hecho con antelaci\xf3n se prosigi\xf3 a eliminar outliers quitando los valores de GrLivArea<4000 y TotalBsmtSF>3000.=,=h6>>GrLivArea=,=img>>con outliers 1.JPG>>50%>>250=,=img>>sin outliers 1.jpg>>50%>>250=,=br=,=br =,=h6>>TotalBsmtSF=,=img>>con outliers 2.jpg>>50%>>250=,=img>>sin outliers 2.jpg>>50%>>250=,=br=,=br=,=h5>>Tratamiento del sesgo (para datos de entrenamiento 3)=,=p>>Se trat\xf3 el sesgo en los atributos YearBuilt y YearRemodAdd con transformaciones logar\xedtmicas.=,=h6>>YearBuilt=,=img>>yb sin log.jpg>>50%>>250=,=img>>yb con log.jpg>>50%>>250=,=br=,=br=,=h6>>YearRemodAdd=,=img>>yra sin log.jpg>>50%>>250=,=img>>yra con log.jpg>>50%>>250=,=br=,=br=,=br=,=h5>>Datos faltantes (para datos de entrenamiento 1)=,=p>>Sustituimos por el promedio de los atributos en las filas que conten\xedan datos faltantes para el caso 1. Los conjuntos 2 y 3 no ten\xedan missing values.=,=img>>1antes.jpg>>30%>>150=,=img>>1despues.jpg>>28%>>150=,=br=,=br=,=br=,=h5>>Estandarizaci\xf3n (para datos de entrenamiento 3)=,=p>> Se estandarizaron las variables num\xe9ricas y ordinales para que sus escalas no sean determinantes.=,=h2>>Modelado=,=hr=,=p>>En la decisi\xf3n de los modelos a desarrollar es fundamental considerar el tipo de problema el cual en este caso consiste en predecir el valor de una propiedad a partir de un conjunto de predictores y por lo tanto se trata de uno supervisado y de regresi\xf3n. Existen un conjunto de algoritmos que se pueden emplear en estas condiciones entre los cuales se encuentran regresi\xf3n lineal considerando tambi\xe9n sus variaciones Lasso y Ridge, \xe1rboles de decisi\xf3n, random forests, gradient boosting regression, k-nearest neighbors, support vector regression, entre otros. Para decidir entre los modelos aplicables es necesario ver la estructura de los datos que tenemos en este problema en particular.=,=p>>Como se estudi\xf3 en la secci\xf3n de an\xe1lisis de datos con los coeficientes de Pearson y los diagramas, los predictores mantienen una relaci\xf3n lineal con la salida lo cual se acent\xfaa para los atributos seleccionados y trabajados en el data pre processing. Como dicho algoritmo asume una relaci\xf3n lineal entre las entradas y la salida, aplicar regresi\xf3n lineal para este caso es una buena idea por lo que ser\xe1 empleada a continuaci\xf3n. Adem\xe1s, aplicaremos el algoritmo de ensamble Random Forest para hacer una comparaci\xf3n entre ambos.  =,=p>>Los modelos seleccionados ser\xe1 entrenado con los 3 conjuntos de datos de entrenamiento. Para estos se utilizar\xe1 cross validation con 10 pliegos de forma tal de evitar el problema en el que la parte de testeo pueda llegar a ser m\xe1s f\xe1cil o m\xe1s dificil para alg\xfan modelo en espec\xedfico como podr\xeda pasar con la t\xe9cnica hold out.=,=br=,=h2>>Resultados=,=hr=,=p>>Para la medici\xf3n de resultados se usar\xe1n dos coeficientes, el primero es RMSE o root mean squared error que mide la ra\xedz cudrada del promedio de los cuadrados de las diferencias entre las predicciones y los datos etiquetados del dataset train al cuadrado por lo que cuanto menor mejor.=,=p>>Se consiguieron las siguientes cifras.=,=h5>>Regresi\xf3n lineal=,=p>>- Datos de entrenamiento 1: RMSE = 25935=,=p>>- Datos de entrenamiento 2: RMSE = 25744=,=p>>- Datos de entrenamiento 3: RMSE = 24911=,=br=,=h5>>Random Forest=,=p>>Con 1000 \xe1rboles y m\xe1ximo de atributos a considerar en los splits de 20.=,=p>>- Datos de entrenamiento 1: RMSE = 24564=,=p>>- Datos de entrenamiento 2: RMSE = 27302=,=p>>- Datos de entrenamiento 3: RMSE = 27692=,=br=,=h2>>Conclusiones=,=hr=,=p>>=,=p>>En los resultados se puede visualizar que seleccionar atributos tanto ordinales como nominales cuidadosamente y aplicar otros m\xe9todos de pre procesamiento de datos como estandarizaci\xf3n, remoci\xf3n de outliers y transformaciones merece la pena para el caso de la regresi\xf3n lineal dado que esto optimiz\xf3 el par\xe1metro utilizados para medir la performance lo cual se puede ver tanto del caso 1 al 2 como del 2 al 3.=,=p>>En cuanto a Random Forest se observa que da mejores resultados con los atributos originales sin realizar seleccion de atributos ni otros m\xe9todos de procesamiento de datos. Esto puede deberse a que con estas modificaciones se pierda parte de la buena representaci\xf3n del problema el cual es el \xfanico requerimiento exigido por este algoritmo en cuanto a la preparaci\xf3n de datos.=,=p>>Con lo anterior se comprueba en un caso practico que lo que puede ser una buena preparaci\xf3n de datos para un algoritmo de machine learning puede ser mala para otro. De esta forma, para obtener una mejor performance para un problema no solo basta con aplicar distintos algoritmos a un mismo conjunto de prueba preparado y observar cual da mejor sino que tambi\xe9n es necesario probar diferentes preparaciones de datos a ser aplicadas a los distintos algoritmos de machine learning de forma tal de sacarles el m\xe1ximo provecho.=,=p>>Por otra parte, resulta interesante que a partir del desarrollo realizado tanto en el \xe1n\xe1lisis de datos por ejemplo al ver la alta correlaci\xf3n que hay entre el tama\xf1o disponible y el precio de la propiedad como en la preparaci\xf3n de datos y modelado siendo un caso conreto la selecci\xf3n de ciertas variables nominales que mejoran mucho el rendimiento de los modelos como es el caso del barrio, se logra ver como afectan en un caso real las caracter\xedsticas de las propiedades que los expertos mencionan como las m\xe1s importantes lo que se analiz\xf3 en un principio en la secci\xf3n de contexto. Se puede ver como al incluir y procesar correctamente esos factores que ellos m\xe1s destacan es lo que m\xe1s termina determinando que al armar nuestros modelos estos hagan mejores predicciones.=,=br=,=h2>>Anexo=,=hr=,=iframe>>100%>>550","content":"h2>>Context=,=p>>The real estate industry is one of the industries that generates the most activity and growth in the world. In the United States, according to annualized data from the Bureau of Economic Analysis (BEA) for the second quarter of 2021, the sector linked to housing is one of the industries that generates the greatest economic activity in that country with a GDP of 2,908 trillion dollars. According to data also annualized from the IMF for October 2021, this figure exceeds the GDP of countries such as Spain, Italy, Brazil, Australia, Canada, Russia, South Korea and is equivalent to 48.38 times that of Uruguay.=,=p>>The value buying a property depends on many factors, among which experts highlight the following.=,=p>>Location: One of the main ones is linked to the famous phrase of the real estate industry \u201clocation, location, location\u201d making reference to the fact that a large part of the value of a property depends on the quality of opportunities and services that can be obtained by living in that place, such as the possibilities of obtaining a good job, security, proximity to quality schools, etc.=,=p>>Space: It is a fundamental factor to determine its value, with special emphasis on usable space, which is the one in which the area corresponding to attics, basements and places that people rarely use in their day to day are not counted. Many people choose to live in suburbs where they can have larger interiors, gardens or funds than to live in the center of the city in a smaller property even though this deprives them of the possibility of accessing any opportunity or service with the same ease. This can be seen to a great extent in countries such as the United States or Canada, but also in Uruguay with Ciudad de la Costa.=,=p>>Format: Buyers are willing to pay to get a home that meets their current needs or to their future prospects, as would be the case of a family that has expectations of having more children and is looking for a house large enough to do so. Some of these characteristics stand out, such as the number of rooms available, the number of bathrooms and whether the property has a garage or not. On the other hand, the format of the house itself is strongly related to its price, a house is not the same as an apartment with all the variants that these may have.=,=p>>Status: The status of a property is a fundamental factor for its price, this being the reason why it is economically convenient to recondition a property before selling it. On the other hand, the older the constructions are, the more likely they are to have defects and therefore modernity is valued.=,=br=,=h2>>Understanding the business=,=hr=,=p>>This case of The purpose of this study is to be able to develop the CRISP-DM model on a problem for academic purposes and not with a deployment objective or as part of a larger product.=,=p>>Data preparation, modeling and evaluation will be carried out to predict the value of homes from characteristics of the properties or their environments using Python and libraries such as SciKitLearn, Pandas, Numpy, Seaborn, among others. The complete code referred to in this work is available in the last section of the Annex.=,=br=,=h2>>Data knowledge=,=hr=,=p>>For this project we have Property sales data for the town of 66,000 inhabitants called Ames in the state of Iowa United States between 2006 and 2010 obtained from the Ames City Assessor\'s Office and preprocessed by Dean De Cock PhD in Probability from Iowa State University. The records are in two datasets, one for training and one for test.=,=h6>>Downloads:=,=a>>Data description>>data_description.txt>>d=,=br=,=a>>train.csv>>train.csv>>d=,=br=,=a>>test.csv>>test.csv>>d=,=br=,=br=,=h4>>Analysis of attributes =,=p>>The structure of the datasets to be used has 78 predictors plus the continuous numerical target variable in the case of the train predictor. This will be predicted with the modeling to be developed, thus being a supervised regression problem.=,=p>>Of the 78 input variables, it is very important to take into account their different types, which in this case are defined in the description of data for each. In this way we have:=,=p>>- 44 categorical (23 nominal and 21 ordinal)=,=p>>- 34 numerical (19 continuous and 15 discrete)=,=p>> Although the nominal and Ordinals are categorical, they differ in that the former are orderable while the latter are simply characteristics whose values \u200b\u200bare hardly comparable between them in the sense that one can be considered greater or less than the other by itself.=,=p>> A better explanation could be given considering attributes of the problem with ExterQual (ordinal) and Foundation (nominal).=,=p>>- ExterQual: refers to the quality of the materials outside the house and its possible values \u200b\u200bare Ex ( excellent), Gd (good), TA (average), Fa (fair), Po (poor) for which it is easy to transfer to a scale in which the first values \u200b\u200bare greater than the last since it is indisputable that in any case excellent quality is better than average or poor.=,=p>>- Foundation: describes the cimie nts of a house with the classes BrkTil (bricks and tiles), CBlock (cement block), PConc (concrete), Slab (slab), Stone (stone) and Wood (wood). Given that in some areas it might be better to have foundations of one type and in others of another, so the order would depend on circumstances unrelated to the variable itself, it can hardly be ordered.=,=br=,=h4>>Analysis of examples=,=p>>As for the number of examples, there are 1460 labeled data in the train and 1459 unlabeled data in the test. The case study will focus on working with the training dataset since it allows us to measure performance when developing prediction models by comparing the values \u200b\u200bobtained by them with the labels provided. In this way, all the analysis that will be carried out below refers to that data set.=,=br=,=h5>>Missing Data=,=p>>Analyzing the missing data, it can be seen that they all correspond to the continuous variables LotFrontage (28), GarageYrBlt (81) and MasVnrArea (8).=,=img>>7.jpg>>25%>>150 =,=p>>By the way, it is convenient to point out that in many categorical variables there is the word NA as one of the possible values \u200b\u200bbeing an example of them GarageType in which case it is used to refer to properties that do not have a garage. These observations with NA should not be confused with missing data.=,=br=,=h5>>Correlations=,=p>>It is interesting to observe the correlation that each one maintains both with the target variable and with each other. =,=p>>We will study the correlation matrix generated from the Pearson coefficient, which measures the linear correlation between two variables on a scale between -1 and 1, interpreted as the closer to zero, the less linear dependence, and the further, greater. be direct in the case of the approach to 1 or opposite in the case of approaches to -1. This coefficient can only be calculated for numerical variables or, failing that, as will be done below, they can also be added to the ordinal variables after converting them to a numerical scale. =,=p>>Filtering between the attributes that have a correlation greater than 0.5 or less than -0.5 with respect to the target variable according to Pearson, the following 13 predictors are obtained in an orderly manner.=,=img>>ames-1.jpg >>25%>>250=,=br=,=br=,=p>>Performing the calculations for the correlations with the target variable with 55 predictors between the numeric and the ordinal ones after having been coded so that they are also ordinal, only 13 show a prediction greater than 0.5 with this one, which is 23%. In addition, these relationships are all direct and none are opposite.=,=p>>Next, the linear correlations between the predictors are calculated, resulting in this Pearson matrix.=,=img>>ames-2.jpg>>100% >>400=,=br=,=br=,=p>>Regarding the correlations between the predictors themselves, positive values \u200b\u200bwere obtained in all cases, with which they all have a certain direct linear dependence, no matter how minimal it may be . Values \u200b\u200bgreater than >0.66 were considered high correlations, which occurs between the pairs GrLivArea and TotRmsAvgGrd with 0.82, GarageArea and GarageCars with 0.88, TotalBsmtSF and 1stFlrSF with 0.81.=,=br=,=h5>>Outliers=, =p>>Studying the distributions of the attributes that have a high correlation with the variable to be predicted and a low correlation with each other, outliers can be observed for GrLivArea in its values \u200b\u200babove 4000 (this is expressed by Dean De Cock himself in his paper on the problem) and TotalBsmtSF above 3000 as shown in the renderings below.=,=img>>ames-3.jpg>>50%>>300=,=img>>ames-4.jpg>>50% >>300=,=br=,=br=,=br=,=h5>>Biases=,=p>>Viewing the histograms, biases to the right could be seen in YearBuilt and YearRemodAdd, which correspond respectively to the year of construction and to the last remodeling of the properties. It can be seen that in the registry there is much more data on houses built or remodeled a few years ago, with 40% and 59% of the sample having a year after 1980 in each case.=,=img>>ames-5.jpg >>50%>>300=,=img>>ames-6.jpg>>50%>>300=,=br=,=br=,=br=,=h5>>Nominal attribute distributions=,= p>>When performing the analysis of the nominal variables, it is useful to observe their different categories in relation to the target variable, which is possible with box plots that allow us to visualize how much variability there is in the dataset between the classes of a nominal category with respect to the target variable with the range, median and outliers they have. If the variability is high then it is a good variable to learn from.=,=p>>The most notable diagrams are found later. =,=p>>-Neighborhood=,=img>>ames-13.jpg>>80%>>400=,=br=,=br=,=p>>-HouseStyle=,=img>>ames-14.jpg>>80%>>300 =,=br=,=br=,=p>>-Foundation=,=img>>ames-8.jpg>>80%>>300=,=br=,=br=,=h2>>Data preparation=,=hr=,=p>>=,=h5>>Training sets generated=,=p>>3 data sets were assembled for training different characteristics to apply models on them and be able to make comparisons.=,=p>>- Training data 1: Contains all the predictors of the dataset with the coded numerical and categorical variables and without missing data.=,=p>>- Training data 2: With attributes selected as explained below, numeric and categorical variables coded, but without further pre-processing. Its objective is to test the feature selection made.=,=p>>- Training data 3: It includes the same attributes selected for the previous case, numeric and categorical variables encoded, but also other data pre-processing techniques shown were performed on this below. Its purpose is to test the applied pre-processing methods. =,=br=,=h5>>Loading data=,=p>>The data was loaded and the values \u200b\u200bwith NA of the categorical variables were replaced by a word that means that there is not what the predictor is about. which can be garages, basements, etc.=,=br=,=h5>>Coding ordinal attributes (for training data 1, 2 and 3)=,=p>>Ordinal attributes were coded transforming their scales to discrete with SKLearn\'s OrdinalEncoder(), the important thing here is to maintain the ordering and not to happen as in Python that encodes backwards and then when calculating the correlations, for example with the target variable, the opposite value is obtained from what it should be, but this can still be easily fixed as shown in the Annex.=,=br=,=h5>>Selection of numerical and ordinal attributes (for training data 2 and 3)=,=p>>We kept the predictors that maintained a higher correlation to 0.5 or less than -0.5 with respect to the objective attribute and a correlation with the others that are a less than 0.66 or greater than -0.66 in all cases. Obtaining the following variables.=,=img>>vars.jpg>>100%>>40=,=br=,=br=,=br=,=h5>>Nominal attribute selection (for training data 2 and 3) =,=p>>Box plot diagrams were observed, analyzing which ones provide the greatest variability with respect to the objective variable. From this, the predictors HouseStyle, Foundation, CentralAir and Neighborhood were obtained. The others were discarded because they could not justify that they have considerable variability between their different classes with respect to the objective variable that is decisive for the problem.=,=br=,=h5>>Coding of nominal attributes (for training data 1 , 2 and 3)=,=p>>The nominal attributes were encoded with the one hot encoding found in SKLearn and Pandas. What it does is generate dummy variables that are nothing more than new binary attributes for each of the different values \u200b\u200bthat the selected nominal variables can have, which means that the original variables do not lose their nominal nature of not having a defined order.= ,=br=,=h5>>Removal of outliers (for training data 3)=,=p>>After having selected the attributes with which we will work and basing ourselves on the analysis done in advance, we proceeded to eliminate outliers by removing the values \u200b\u200bof GrLivArea<4000 and TotalBsmtSF>3000.=,=h6>>GrLivArea=,=img>>con outliers 1.JPG>>50%>>250=,=img>>sin outliers 1.jpg>>50% >>250=,=br=,=br =,=h6>>TotalBsmtSF=,=img>>con outliers 2.jpg>>50%>>250=,=img>>sin outliers 2.jpg>>50%>>250=,=br=,=br=,=h5>>Treatment of bias (for training data 3)=,=p>>Treatment of bias in YearBuilt and YearRemodAdd attributes with logarithmic transformations.=,=h6>>YearBuilt=,=img>>yb sin log.jpg>>50%>>250=,=img>>yb con log.jpg>>50%>>250=,=br=,=br=,=h6>>YearRemodAdd=,=img>>yra sin log.jpg>>50%>>250=,=img>>yra con log.jpg>>50% >>250=,=br=,=br=,=br=,=h5>>Missing data (for training data 1)=,=p>>We substitute the average of the attributes in the rows that contained missing data for case 1. Sets 2 and 3 had no missing values.=,=img>>1antes.jpg>>30%>>150=,=img>>1despues.jpg>>28%>>150=,= br=,=br=,=br=,=h5>>Standardization (for training data 3)=,=p>> Numerical and ordinal variables were standardized so that their scales are not determinant.=,=h2>> Modeling=,=hr=,=p>>In deciding which models to develop, it is essential to consider the type of problem, which in this case consists of predicting the value of a property from a set of predictors and therefore it is a supervised one and regression. There is a set of algorithms that can be used in these conditions, among which are linear regression considering also its Lasso and Ridge variations, decision trees, random forests, gradient boosting regression, k-nearest neighbors, support vector regression, among others. To decide between the applicable models it is necessary to see the structure of the data we have in this particular problem.=,=p>>As studied in the data analysis section with the Pearson coefficients and the plots, the predictors maintain a linear relationship with the output, which is accentuated for the attributes selected and worked on in the data pre-processing. As said algorithm assumes a linear relationship between the inputs and the output, applying linear regression for this case is a good idea, so it will be used below. In addition, we will apply the Random Forest ensemble algorithm to make a comparison between the two. =,=p>>The selected models will be trained with the 3 training data sets. For these, cross validation will be used with 10 specifications in order to avoid the problem in which the testing part may become easier or more difficult for a specific model, as could happen with the hold out technique.=,=br =,=h2>>Results=,=hr=,=p>>Two coefficients will be used to measure the results, the first is RMSE or root mean squared error, which measures the square root of the average of the squares of the differences between the predictions and the labeled data of the dataset are trained squared, so the smaller the better.=,=p>>The following figures were obtained.=,=h5>>Linear regression=,=p>>- Training data 1: RMSE = 25935=,=p>>- Training data 2: RMSE = 25744=,=p>>- Training data 3: RMSE = 24911=,=br=,=h5>>Random Forest=,=p>>With 1000 trees and a maximum of attributes to consider in the splits of 20.=,=p>>- Training data 1: RMSE = 24564=,=p>>- Training data 2: RMSE = 27302=,=p >>- Training data 3: RMSE = 2769 2=,=br=,=h2>>Conclusions=,=hr=,=p>>=,=p>>In the results it can be seen that selecting both ordinal and nominal attributes carefully and applying other methods of pre-processing Data such as standardization, removal of outliers and transformations is worthwhile for the case of linear regression since this optimized the parameter used to measure performance, which can be seen both from cases 1 to 2 and from 2 to 3.=,= p>>Regarding Random Forest, it is observed that it gives better results with the original attributes without performing attribute selection or other data processing methods. This may be due to the fact that with these modifications part of the good representation of the problem is lost, which is the only requirement demanded by this algorithm in terms of data preparation.=,=p>>With the above it is verified in a practical case that what may be good data preparation for one machine learning algorithm may be bad for another. In this way, to obtain a better performance for a problem, it is not only enough to apply different algorithms to the same prepared test set and observe which one gives the best, but it is also necessary to test different data preparations to be applied to the different machine algorithms. learning in such a way as to get the most out of them.=,=p>>On the other hand, it is interesting that based on the development carried out both in data analysis, for example, when seeing the high correlation between the available size and the price of the property as in the preparation of data and modeling, being a concrete case the selection of certain nominal variables that greatly improve the performance of the models, as is the case of the neighborhood, it is possible to see how they affect the characteristics of the properties in a real case that the experts mention as the most important what was analyzed at the beginning in the context section. You can see how correctly including and processing those factors that stand out the most is what ends up determining that when building our models they make better predictions.=,=br=,=h2>>Annex=,=hr=,=iframe >>100%>>550"},{"unidad":"Caso","unit":"Case","titulo":"Segmentaci\xf3n de clientes","title":"Customer segmentation","descripcion":"","description":"","contenido":"h2>>Contexto=,=p>>Para cualquier empresa resulta fundamental distinguir c\xf3mo son sus clientes de forma tal de que les permita generar estrategias para ciertos p\xfablicos en particular como sacar una nueva l\xednea de productos pensando en cierto sector o aprovecharse de poder hacer uso del marketing por microsegmentos que se basa en conocer distintos tipos de clientes y desarrollar campa\xf1as espec\xedficas para cada uno de estos sectores que muchas veces est\xe1n motivados a consumir por razones distintas, entre otras razones.=,=p>>M\xe1s all\xe1 de que una estrategia o campa\xf1a sea buena o mala, con alto o bajo presupuesto, resulta impresindible que est\xe9 dirigida al p\xfablico correcto y para ello los datos y su an\xe1lisis con herramientas de machine learning no supervisado como clustering nos pueden ser de gran ayuda al permitirnos distinguir subgrupos en un conjunto de datos que bien pueden corresponder a los diferentes clases de clientes de una compa\xf1\xeda.=,=br=,=h2>>Entendimiento de negocio=,=hr=,=p>>En este caso de estudio se desarrollar\xe1 el modelo CRISP-DM sobre el problema no supervisado de segmentaci\xf3n de los clientes de un shopping con fines acad\xe9micos sin tener como fin un deployment o que un producto m\xe1s grande. Se realizar\xe1 preparaci\xf3n de datos, modelado y evaluaci\xf3n para encontrar clusters o subgrupos a partir de valores acerca de diferentes cualidades de los compradores ayud\xe1ndonos de la herramienta de miner\xeda de datos RapidMiner.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>En este caso de estudio se posee un conjunto de datos obtenidos de Kaggle que ser\xe1n estudiados con la herramienta RapidMiner. Este cuenta con 200 ejemplos acerca de caracter\xedsticas de los clientes de una shopping representadas por los atributos dispuestos a continuaci\xf3n junto a sus descripciones e histogramas.=,=p>>- CustomerID (num\xe9rico): Simplemente un identificador \xfanico del cliente.=,=p>>- Gender (nominal): Si el cliente es masculino o femenino. La proporci\xf3n es de 56% de mujeres frente a 44% de hombres.=,=img>>customer seg-1.jpg>>35%>>300=,=br=,=br=,=p>>- Age (num\xe9rico): Representa la edad del cliente La proprci\xf3n. El grupo etario que contiene m\xe1s individuos se sit\xfaa desde en la d\xe9cada de los 26 a los 35 a\xf1os.=,=img>>customer seg-2.jpg>>70%>>450=,=br=,=br=,=p>>- Annual Income (num\xe9rico): Corresponde al salario anual del cliente en miles de d\xf3lares. Los datos m\xe1s repetidos se encuentran en el centro de la distribuci\xf3n yendo desde los 50 hasta los 90 mil d\xf3lares al a\xf1o.=,=img>>customer seg-3.jpg>>70%>>450=,=br=,=br=,=p>>- Spending Score (num\xe9rico): Puntaje vinculado al consumo del cliente. Tiene un rango entre (0,100) y los valores m\xe1s repetidos se encuentran en el centro de la distribuci\xf3n.=,=img>>customer seg-4.jpg>>90%>>450=,=br=,=br=,=p>>*Observaci\xf3n: Entre los datos no se hallan valores faltantes ni outliers.=,=h6>>Descargas:=,=a>>Datos>>Mall_Customers.csv>>d=,=br=,=br=,=h2>>Preparaci\xf3n de datos=,=hr=,=p>>Para algoritmos de clustering como k-means es importante verificar que no hay valores faltantes dado que no los soporta, eliminar outliers por ser muy sensible a ellos, manejar una baja dimensionalidad, solamente utilizar variables num\xe9ricas porque medir las distancias que terminan determinando las agrupaciones es m\xe1s \xf3ptimo de esta manera y normalizar para que las escalas de los atributos no sean determinantes en los c\xe1lculos.=,=p>>De esta forma quitamos el par\xe1metro CustomerID al no ser de inter\xe9s para la formaci\xf3n de agrupaciones considerando que es diferente para cada fila, cambiamos el atributo Gender de categ\xf3rico a num\xe9rico con un dummy encoding y quitamos outliers en base a distancias euclideanas. Respecto a los valores faltantes no se realiz\xf3 ninguna t\xe9cnica de procesamiento al no ser necesario para este caso particular y no se aplic\xf3 estandarizaci\xf3n debido a que los predictores num\xe9ricos tienen unidades que utilizan rangos rangos similares.=,=br=,=h2>>Modelado=,=hr=,=p>>Empleamos k-means en RapidMiner cambiando la cantidad de clusteres comprendiendo los casos de k=2, k=3, k=4 y k=5 pero manteniendo siempre el mismo n\xfamero de iteraciones del algoritmo con max runs = 10. Para cada caso se analiz\xf3 la interpretabilidad del resultado obtenido con scatter plots y la performance seg\xfan la distancia promedio de los puntos respecto a los centroides de los clusters.=,=h6>>Proceso en RapidMiner=,=img>>customer seg-5.jpg>>100%>>600=,=a>>Descargar proceso>>Customer Segmentation.rmp>>d=,=br=,=br=,=h2>>Resultados=,=hr=,=p>>Observando las gr\xe1ficas resultantes de los modelos creados se lleg\xf3 a que las mejores visualizaciones para darle significado a los clusters se obten\xedan de Spending Score contra Annual Income, Spending Score en funci\xf3n de Age y gr\xe1ficos de barra de los clusters con colores para diferenciar a Gender.=,=h6>>K=2=,=p>>Avg. within centroid distance: -0.193=,=img>>customer seg-6.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-7.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-8.jpg>>75%>>400=,=br=,=br=,=h6>>K=3=,=p>>Avg. within centroid distance: -0.156=,=img>>customer seg-9.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-10.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-11.jpg>>75%>>400=,=br=,=br=,=h6>>K=4=,=p>>Avg. within centroid distance: -0.119=,=img>>customer seg-12.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-13.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-14.jpg>>75%>>400=,=br=,=br=,=h6>>K=5=,=p>>Avg. within centroid distance: -0.102=,=img>>customer seg-15.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-16.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-17.jpg>>75%>>400=,=br=,=br=,=h6>>Gr\xe1fica de Avg. within centroid distance en relaci\xf3n con k=,=img>>customer seg-18.jpg>>75%>>400=,=br=,=br=,=h2>>Conclusiones=,=hr=,=p>>Observado los resultados se puede llegar a que los clusters se van diferenciando visualmente m\xe1s y m\xe1s a medida que se aumenta el valor para k hasta llegar a 5. Si uno contin\xfaa aumentando esta cifra se empieza a complicar distinguir las agrupaciones seg\xfan caracter\xedsticas de los clientes. Igualmente con k=5 para todos los clusteres hay gran cantidad de registros de g\xe9nero masculino y femenino sin diferenciarse largamente entre s\xed lo que podr\xeda mostrar que no existen enormes diferencias en el comportamiento de compra entre ambos casos.=,=p>> Con k=5 a partir de las tres gr\xe1ficas del caso de pueden distinguir cualidades de cada una de las agrapaciones que aparecen:=,=p>>- Cluster 0 (naranja): Clientes de ingresos medios (40-70), todas las edades (18-70) y un puntaje de gasto medio (40-60).=,=p>>- Cluster 1 (negro): Clientes de altos ingresos (70-110), j\xf3venes (25-40) y de puntaje de gasto alto (60-100).=,=p>>- Cluster 2 (azul): Clientes de bajos ingresos (15-40), j\xf3venes (18-35) y de puntaje de gasto alto (60-100).=,=p>>- Cluster 3 (violeta): Clientes de altos ingresos (70-105), mediana edad (30-60) y puntaje de gasto bajo (0-40).=,=p>>- Cluster 4 (verde): Clientes de ingresos bajos (15-40), mediana edad (30-60) y puntaje de gasto bajo (0-40).=,=p>>Por otra parte, se puede ver como disminuye el promedio de distancias de los puntos de los clusteres respecto a sus centroides al subir el valor de k hasta llegar a 5 gener\xe1ndose un comportamiento asint\xf3tico que se manifiesta en la \xfaltima gr\xe1fica.","content":"h2>>Contexto=,=p>>For any company it is essential to distinguish what its customers are like in such a way that it allows them to generate strategies for certain audiences in particular, such as launching a new line of products with a certain sector in mind or taking advantage of being able to do use of micro-segment marketing that is based on knowing different types of customers and developing specific campaigns for each of these sectors that are often motivated to consume for different reasons, among other reasons.=,=p>>Whether a strategy or campaign is good or bad, with a high or low budget, it is essential that it is aimed at the right audience and for this data and its analysis with unsupervised machine learning tools such as clustering can be of great help to us by allowing us to distinguish subgroups in a set of data that may well correspond to the different classes of customers of a company.=,=br=,=h2>>Business understanding=,=hr=,=p>>En es The case of study will develop the CRISP-DM model on the unsupervised problem of customer segmentation in a shopping center for academic purposes, without the purpose of a deployment or a larger product. Data preparation, modeling and evaluation will be carried out to find clusters or subgroups from values \u200b\u200babout different qualities of buyers with the help of the RapidMiner data mining tool.=,=br=,=h2>>Data knowledge=,=hr=,=p>>In this case study we have a set of data obtained from Kaggle that will be studied with the RapidMiner tool. This has 200 examples of customer characteristics of a shopping mall represented by the attributes listed below along with their descriptions and histograms.=,=p>>- CustomerID (numeric): Simply a unique identifier of the customer.=,= p>>- Gender (nominal): Whether the client is male or female. The proportion is 56% of women compared to 44% of men.=,=img>>customer seg-1.jpg>>35%>>300=,=br=,=br=,=p>>- Age (numeric): Represents the customer\'s age The proportion. The age group that contains the most individuals is between the ages of 26 and 35.=,=img>>customer seg-2.jpg>>70%>>450=,=br=,=br=,=p>>- Annual Income (numeric): Corresponds to the client\'s annual salary in thousands of dollars. The most repeated data is found in the center of the distribution ranging from 50 to 90 thousand dollars a year.=,=img>>customer seg-3.jpg>>70%>>450=,=br=,= br=,=p>>- Spending Score (numeric): Score linked to customer consumption. It has a range between (0.100) and the most repeated values \u200b\u200bare in the center of the distribution.=,=img>>customer seg-4.jpg>>90%>>450=,=br=,=br=,=p>>*Note: There are no missing values \u200b\u200bor outliers in the data.=,=h6>>Downloads:=,=a>>Data>>Mall_Customers.csv>>d=,=br=,=br=,=h2>>Data preparation=,=hr=,=p>>For clustering algorithms such as k-means it is important to verify that there are no missing values \u200b\u200bsince it does not support them, eliminate outliers as it is very sensitive to them, handle a low dimensionality, only use numerical variables because measuring the distances that end up determining the groupings is more optimal in this way and normalize so that the scales of the attributes are not decisive in the calculations.=,=p>>In this way we remove the CustomerID parameter, since it is not of interest for the formation of groups considering that it is different for each row, we change the Gender attribute from categorical to numeric with a dummy encoding and remove or utliers based on Euclidean distances. Regarding the missing values, no processing technique was performed as it was not necessary for this particular case and no standardization was applied because the numerical predictors have units that use similar range ranges.=,=br=,=h2>>Modeling =,=hr=,=p>>We use k-means in RapidMiner changing the number of clusters including the cases of k=2, k=3, k=4 and k=5 but always maintaining the same number of iterations of the algorithm with max runs = 10. For each case, the interpretability of the result obtained with scatter plots and the performance according to the average distance of the points with respect to the centroids of the clusters were analyzed.=,=h6>>Process in RapidMiner=,=img>>customer seg-5.jpg>>100%>>600=,=a>>Download Process>>Customer Segmentation.rmp>>d=,=br=,=br=,=h2>>Results=,=hr =,=p>>Observing the resulting graphs of the created models, it was found that the best visualizations to give meaning to the clusters were obtained from Spending Score with tra Annual Income, Spending Score as a function of Age and bar graphs of the clusters with colors to differentiate Gender.=,=h6>>K=2=,=p>>Avg. within centroid distance: -0.193=,=img>>customer seg-6.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-7.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-8.jpg>>75%>>400=,=br=,=br=,=h6>>K=3=,=p >>Avg. within centroid distance: -0.156=,=img>>customer seg-9.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-10.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-11.jpg>>75%>>400=,=br=,=br=,=h6>>K=4=,=p >>Avg. within centroid distance: -0.119=,=img>>customer seg-12.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-13.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-14.jpg>>75%>>400=,=br=,=br=,=h6>>K=5=,=p >>Avg. within centroid distance: -0.102=,=img>>customer seg-15.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-16.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-17.jpg>>75%>>400=,=br=,=br=,=h6>>Avg. plot within centroid distance in relation to k=,=img>>customer seg-18.jpg>>75%>>400=,=br=,=br=,=h2>>Conclusions=,=hr=,=p>>Observed the As a result, the clusters can be visually differentiated more and more as the value for k is increased until it reaches 5. If one continues to increase this figure, it becomes difficult to distinguish the groups according to the characteristics of the clients. Likewise, with k=5 for all the clusters, there is a large number of male and female records without differing greatly from each other, which could show that there are no huge differences in purchasing behavior between the two cases.=,=p>> With k =5 From the three graphs of the case, qualities of each of the groups that appear can be distinguished:=,=p>>- Cluster 0 (orange): Medium-income clients (40-70), all ages ( 18-70) and a medium spending score (40-60).=,=p>>- Cluster 1 (black): High-income clients (70-110), young people (25-40) and spending score high (60-100).=,=p>>- Cluster 2 (blue): Low-income clients (15-40), young (18-35) and high spending score (60-100).=,=p>>- Cluster 3 (purple): Clients with high income (70-105), middle age (30-60) and low spending score (0-40).=,=p>>- Cluster 4 (green ): Clients with low income (15-40), middle age (30-60) and low spending score (0-40).=,=p>>On the other hand, it can be e see how the average distance of the cluster points decreases with respect to their centroids as the value of k rises until it reaches 5, generating an asymptotic behavior that is manifested in the last graph."}]'),K2=[{path:"",redirectTo:"home",pathMatch:"full"},{path:"home",component:q2},{path:"menu/:idMenu",component:W2},{path:"blog/:idMenu/:idBlog",component:(()=>{class t{constructor(n,r){this.router=n,this.route=r,this.title="json-file-read-angular",this.ArticlesList=G2}ngOnInit(){console.log(this.route.snapshot.params);var n=0;let r,i,o,s,a,l=document.getElementById("name"),c=document.getElementById("sub"),d=document.getElementById("description"),u=document.getElementById("post"),p=this.route.snapshot.paramMap,h=p.get("idMenu"),f=h,m=p.get("idBlog");var g=this.router;let b=this.router.parseUrl(this.router.url).queryParamMap.get("lang");for(a=this.ArticlesList.find(v=>v.unidad===h&&v.titulo===m),"en"==b&&(f=a.unit,m=a.title,r=a.description,i=a.content),"es"==b&&(r=a.descripcion,i=a.contenido),"Caso"!=h&&"Case"!=h&&(l.innerText=f,l.style.fontSize=window.matchMedia("(min-width: 768px)").matches?"medium":"small",l.style.color="white",l.addEventListener("click",function(){g.navigate(["menu",h]);let v=g.parseUrl(g.url),S=g.createUrlTree(["menu",h]);S.queryParams.lang=v.queryParamMap.get("lang"),g.navigateByUrl(S)})),c.innerText=m,c.style.color="white",c.style.marginTop="1rem",c.style.fontWeight="700",c.style.fontSize=window.matchMedia("(min-width: 768px)").matches?"xxx-large":"x-large",d.innerText=r,d.style.fontSize=window.matchMedia("(min-width: 768px)").matches?"medium":"small",o=i.split("=,=");n<o.length;){let v;switch(s=o[n].split(">>").map(S=>S.trim()),v=document.createElement(s[0]),s[0]){case"p":v.setAttribute("align","justify"),v.textContent=s[1],v.style.color="white",v.style.fontSize=window.matchMedia("(min-width: 768px)").matches?"medium":"small";break;case"h1":case"h2":case"h3":case"h4":case"h5":case"h6":v.textContent=s[1],v.style.color="white";break;case"code":v.textContent=s[1].substring(1),v.setAttribute("class","python"),v.style.color="pink",v.style.fontSize=window.matchMedia("(min-width: 768px)").matches?"medium":"x-small";break;case"a":v.innerHTML=s[1],s.length>2?(v.setAttribute("download",s[2]),v.setAttribute("href","assets/downloads/"+s[2]),v.style.color="skyblue"):v.setAttribute("href",s[2]),v.style.fontSize=window.matchMedia("(min-width: 768px)").matches?"medium":"small";break;case"hr":v.style.color="white";break;case"script":v.setAttribute("src",s[1]);break;case"iframe":v.setAttribute("src","data:text/html;charset=utf-8,<head><base target='_blank'/></head><body><script src='https://gist.github.com/federicobecona/95ce34e328222ae483fb8d5025ea95a8.js'><\/script></body>"),v.setAttribute("width",s[1]),v.setAttribute("height",s[2]);break;case"img":v.setAttribute("src","assets/images/"+s[1]),v.setAttribute("width",s[2]),v.setAttribute("height","auto")}u.appendChild(v),n++}}}return t.\u0275fac=function(n){return new(n||t)(x(Re),x(qn))},t.\u0275cmp=Wn({type:t,selectors:[["app-blog"]],decls:7,vars:0,consts:[[1,"container"],["id","article",1,"blog-post"],["id","name",1,"blog-post-title"],["id","sub",1,"blog-post-title"],["id","description"],["id","post"]],template:function(n,r){1&n&&(_(0,"div",0),_(1,"article",1),Q(2,"a",2),Q(3,"h2",3),Q(4,"p",4),Q(5,"hr"),Q(6,"div",5),C(),C())},styles:['.blog-header[_ngcontent-%COMP%]{line-height:1;border-bottom:1px solid #e5e5e5}.blog-header-logo[_ngcontent-%COMP%]{font-family:"Playfair Display",Georgia,"Times New Roman",serif;font-size:2.25rem}.blog-header-logo[_ngcontent-%COMP%]:hover{text-decoration:none}h1[_ngcontent-%COMP%], h2[_ngcontent-%COMP%], h3[_ngcontent-%COMP%], h4[_ngcontent-%COMP%], h5[_ngcontent-%COMP%], h6[_ngcontent-%COMP%]{font-family:"Playfair Display",Georgia,"Times New Roman",serif}.display-4[_ngcontent-%COMP%]{font-size:2.5rem}@media (min-width: 768px){.display-4[_ngcontent-%COMP%]{font-size:3rem}}a[_ngcontent-%COMP%]:hover{cursor:pointer;font-weight:500}.nav-scroller[_ngcontent-%COMP%]{position:relative;z-index:2;height:2.75rem;overflow-y:hidden}.nav-scroller[_ngcontent-%COMP%]   .nav[_ngcontent-%COMP%]{display:flex;flex-wrap:nowrap;padding-bottom:1rem;margin-top:-1px;overflow-x:auto;text-align:center;white-space:nowrap;-webkit-overflow-scrolling:touch}.nav-scroller[_ngcontent-%COMP%]   .nav-link[_ngcontent-%COMP%]{padding-top:.75rem;padding-bottom:.75rem;font-size:.875rem}.card-img-right[_ngcontent-%COMP%]{height:100%;border-radius:0 3px 3px 0}.flex-auto[_ngcontent-%COMP%]{flex:0 0 auto}.h-250[_ngcontent-%COMP%]{height:250px}@media (min-width: 768px){.h-md-250[_ngcontent-%COMP%]{height:250px}}.blog-pagination[_ngcontent-%COMP%]{margin-bottom:4rem}.blog-pagination[_ngcontent-%COMP%] > .btn[_ngcontent-%COMP%]{border-radius:2rem}.blog-post[_ngcontent-%COMP%]{margin-bottom:4rem;margin-top:5%}.blog-post-title[_ngcontent-%COMP%]{margin-bottom:.5rem}.blog-post-meta[_ngcontent-%COMP%]{margin-bottom:1.25rem;color:#727272}.blog-footer[_ngcontent-%COMP%]{padding:2.5rem 0;color:#727272;text-align:center;background-color:#f9f9f9;border-top:.05rem solid #e5e5e5}.blog-footer[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]:last-child{margin-bottom:0}.bd-placeholder-img[_ngcontent-%COMP%]{font-size:1.125rem;text-anchor:middle;-webkit-user-select:none;user-select:none}.container[_ngcontent-%COMP%]{margin-top:6%}@media (min-width: 1279){.bd-placeholder-img-lg[_ngcontent-%COMP%]{font-size:3.5rem}.container[_ngcontent-%COMP%]{width:75%}}@media (max-width: 1279px){.container[_ngcontent-%COMP%]{width:85%;margin-top:10%}}@media (max-width: 840px){.container[_ngcontent-%COMP%]{width:95%;margin-top:15%}}.code[_ngcontent-%COMP%]{font-family:Consolas,"courier new";color:#dc143c;background-color:#f1f1f1;padding:2px;font-size:105%}h2[_ngcontent-%COMP%]{color:#fff}p[_ngcontent-%COMP%]{color:#fff}hr[_ngcontent-%COMP%]{color:#fff}']}),t})()}];let Y2=(()=>{class t{}return t.\u0275fac=function(n){return new(n||t)},t.\u0275mod=Gn({type:t}),t.\u0275inj=dn({imports:[[$0.forRoot(K2)],$0]}),t})(),Q2=(()=>{class t{constructor(n){this.router=n,this.title="portfolio",this.router.routeReuseStrategy.shouldReuseRoute=function(){return!1}}ngOnInit(){window.addEventListener("scroll",function(){document.querySelector("nav").classList.toggle("sticky-navbar",window.scrollY>0)});let r=this.router.parseUrl(this.router.url).queryParamMap.get("lang");if(null==r){let i=this.router.createUrlTree(["home"]);i.queryParams.lang="en",this.router.navigateByUrl(i)}"en"==r&&this.setEnglish(),"es"==r&&this.setSpanish()}goHome(){let n=this.router.parseUrl(this.router.url),r=this.router.createUrlTree([""]);r.queryParams.lang=n.queryParamMap.get("lang"),this.router.navigateByUrl(r)}goToMenu(n){let r=this.router.parseUrl(this.router.url),i=this.router.createUrlTree(["menu",n]);i.queryParams.lang=r.queryParamMap.get("lang"),this.router.navigateByUrl(i)}goToBlog(n){let r=this.router.parseUrl(this.router.url),i=this.router.createUrlTree(["blog","Caso",n]);i.queryParams.lang=r.queryParamMap.get("lang"),this.router.navigateByUrl(i)}goToEnglish(){let n=this.router.parseUrl(this.router.url);n.queryParams.lang="en",this.router.navigateByUrl(n),this.setEnglish()}goToSpanish(){let n=this.router.parseUrl(this.router.url);n.queryParams.lang="es",this.router.navigateByUrl(n),this.setSpanish()}setEnglish(){document.getElementById("title00").innerText="Home",document.getElementById("cases").innerText="Case studies",document.getElementById("exercises").innerText="Exercises",document.getElementById("title1").innerText="Ames housing",document.getElementById("title2").innerText="Heart disease",document.getElementById("title3").innerText="Customer segmentation",document.getElementById("title4").innerText="Data preparation",document.getElementById("title5").innerText="Linear algorithms",document.getElementById("title6").innerText="Non linear algorithms",document.getElementById("title7").innerText="Unsupervised",document.getElementById("title8").innerText="Ensembles"}setSpanish(){document.getElementById("title00").innerText="Inicio",document.getElementById("cases").innerText="Casos de estudio",document.getElementById("exercises").innerText="Ejercicios",document.getElementById("title1").innerText="Bienes ra\xedces en Ames Iowa",document.getElementById("title2").innerText="Enfermedad Card\xedaca",document.getElementById("title3").innerText="Segmentaci\xf3n de clientes",document.getElementById("title4").innerText="Preparaci\xf3n de datos",document.getElementById("title5").innerText="Algoritmos lineales",document.getElementById("title6").innerText="Algoritmos no lineales",document.getElementById("title7").innerText="No supervisado",document.getElementById("title8").innerText="Ensambles"}}return t.\u0275fac=function(n){return new(n||t)(x(Re))},t.\u0275cmp=Wn({type:t,selectors:[["app-root"]],decls:61,vars:0,consts:[[1,"navbar","navbar-expand-md","navbar-dark","fixed-top","bg-dark"],[1,"container-fluid"],["id","row"],["id","title",1,"navbar-brand","block",3,"click"],["id","title00",1,"navbar-brand","block",3,"click"],[1,"dropdown","block"],["id","cases","type","button","data-bs-toggle","dropdown","aria-expanded","false",1,"btn","btn-secondary","dropdown-toggle"],["aria-labelledby","dropdownMenuButton1",1,"dropdown-menu"],["id","title1",1,"dropdown-item",3,"click"],["id","title2",1,"dropdown-item",3,"click"],["id","title3",1,"dropdown-item",3,"click"],["id","exercises","type","button","data-bs-toggle","dropdown","aria-expanded","false",1,"btn","btn-secondary","dropdown-toggle"],["id","title4",1,"dropdown-item",3,"click"],["id","title5",1,"dropdown-item",3,"click"],["id","title6",1,"dropdown-item",3,"click"],["id","title7",1,"dropdown-item",3,"click"],["id","title8",1,"dropdown-item",3,"click"],["type","button","data-bs-toggle","collapse","data-bs-target","#navbarCollapse","aria-controls","navbarCollapse","aria-expanded","false","aria-label","Toggle navigation",1,"navbar-toggler"],[1,"navbar-toggler-icon"],["id","navbarCollapse",1,"collapse","navbar-collapse"],[1,"separator"],["id","form-container",1,"d-flex"],[1,"navbar-brand","lang",3,"click"],["src","assets/images/uk.png"],["src","assets/images/spain.png"],["id","pipe",1,"navbar-brand"],["id","myname",1,"navbar-brand"],["href","https://www.linkedin.com/in/federicobecona","target","_blank"],["src","https://iconsplace.com/wp-content/uploads/_icons/ffffff/256/png/linkedin-icon-18-256.png"],["href","https://github.com/federicobecona","target","_blank"],["src","https://icon-library.com/images/github-icon-white/github-icon-white-6.jpg"]],template:function(n,r){1&n&&(_(0,"header"),_(1,"nav",0),_(2,"div",1),_(3,"div",2),_(4,"a",3),le("click",function(){return r.goHome()}),B(5,"Machine Learning Portfolio"),C(),_(6,"a",4),le("click",function(){return r.goHome()}),B(7,"Home"),C(),_(8,"div",5),_(9,"button",6),B(10,"Case studies"),C(),_(11,"ul",7),_(12,"li"),_(13,"a",8),le("click",function(){return r.goToBlog("Bienes ra\xedces en Ames Iowa")}),B(14,"Ames housing"),C(),C(),_(15,"li"),_(16,"a",9),le("click",function(){return r.goToBlog("Enfermedad card\xedaca")}),B(17,"Heart disease"),C(),C(),_(18,"li"),_(19,"a",10),le("click",function(){return r.goToBlog("Segmentaci\xf3n de clientes")}),B(20,"Customer segmentation"),C(),C(),C(),C(),_(21,"div",5),_(22,"button",11),B(23,"Exercises"),C(),_(24,"ul",7),_(25,"li"),_(26,"a",12),le("click",function(){return r.goToMenu("Preparaci\xf3n de datos")}),B(27,"Data preparation"),C(),C(),_(28,"li"),_(29,"a",13),le("click",function(){return r.goToMenu("Algoritmos lineales")}),B(30,"Linear algorithms"),C(),C(),_(31,"li"),_(32,"a",14),le("click",function(){return r.goToMenu("Algoritmos no lineales")}),B(33,"Non linear algorithms"),C(),C(),_(34,"li"),_(35,"a",15),le("click",function(){return r.goToMenu("No supervisado")}),B(36,"Unsupervised"),C(),C(),_(37,"li"),_(38,"a",16),le("click",function(){return r.goToBlog("Ensambles")}),B(39,"Ensembles"),C(),C(),C(),C(),C(),_(40,"button",17),Q(41,"span",18),C(),_(42,"div",19),Q(43,"hr",20),_(44,"div",21),_(45,"a",22),le("click",function(){return r.goToEnglish()}),Q(46,"img",23),C(),_(47,"a",22),le("click",function(){return r.goToSpanish()}),Q(48,"img",24),C(),C(),Q(49,"hr",20),_(50,"div",21),_(51,"p",25),B(52,"|"),C(),_(53,"p",26),B(54,"Federico Beco\xf1a"),C(),_(55,"div"),_(56,"a",27),Q(57,"img",28),C(),_(58,"a",29),Q(59,"img",30),C(),C(),C(),C(),C(),C(),C(),Q(60,"router-outlet"))},directives:[sp],styles:["a[_ngcontent-%COMP%]:hover{cursor:pointer;font-weight:500}div.dropdown[_ngcontent-%COMP%]{margin-top:5px;margin-right:6%}button[_ngcontent-%COMP%]{background-color:#212529;padding:2pt 2%}li[_ngcontent-%COMP%]{background-color:#212529;border-color:#212529}a[_ngcontent-%COMP%]{color:#fff}ul[_ngcontent-%COMP%]{padding:0;border:0pt}img[_ngcontent-%COMP%]{margin-top:17px;margin-right:2px;width:30px;height:30px}img[_ngcontent-%COMP%]:hover{opacity:50%}#pipe[_ngcontent-%COMP%]{margin-top:11px;margin-left:4px;margin-right:11px}#form-container[_ngcontent-%COMP%]{justify-content:center}.lang[_ngcontent-%COMP%]{margin-top:-20px}#myname[_ngcontent-%COMP%]{margin-top:11px;margin-right:7px}#container[_ngcontent-%COMP%]{padding-left:2%;padding-top:5px;padding-bottom:5px}nav[_ngcontent-%COMP%]{width:100%;border-bottom:solid thin white;position:relative;position:fixed;padding:0;margin:0;border:0px}nav.sticky-navbar[_ngcontent-%COMP%]{opacity:.9;padding:0;margin:0;border:0px}header[_ngcontent-%COMP%]{width:100%}div[_ngcontent-%COMP%]{background-color:#16191b}.dropdown-menu[_ngcontent-%COMP%]{border:solid thin rgb(255,255,255,.25)}.dropdown-item[_ngcontent-%COMP%]{border:solid thin rgb(255,255,255,.25)}#row[_ngcontent-%COMP%]{display:flex;flex-direction:row;justify-content:left;margin-left:2%}#leftContainer[_ngcontent-%COMP%]{justify-content:left}#navbarCollapse[_ngcontent-%COMP%]{justify-content:right}@media (min-width: 900px){#title[_ngcontent-%COMP%], #pipe[_ngcontent-%COMP%]{display:block}#title00[_ngcontent-%COMP%], .separator[_ngcontent-%COMP%]{display:none}}@media (max-width: 900px){#title00[_ngcontent-%COMP%], .separator[_ngcontent-%COMP%]{display:block}#title[_ngcontent-%COMP%], #pipe[_ngcontent-%COMP%]{display:none}}@media (max-width: 767px){#navbarCollapse[_ngcontent-%COMP%]{padding-top:15px}.lang[_ngcontent-%COMP%]{margin-top:-7px}#container[_ngcontent-%COMP%]{padding-top:7px;padding-bottom:7px}#myname[_ngcontent-%COMP%]{margin-top:-5px}img[_ngcontent-%COMP%]{margin-top:0}}.navbar-toggler[_ngcontent-%COMP%]{margin-top:2%;margin-bottom:2%}#title00[_ngcontent-%COMP%]{margin-right:10%}#title[_ngcontent-%COMP%]{margin-right:8%}.separator[_ngcontent-%COMP%]{color:#fff;opacity:.25;margin-top:2%;margin-bottom:2%}#navbarCollapse[_ngcontent-%COMP%]{padding:0%}.navbar-brand[_ngcontent-%COMP%]{margin-right:5px}"]}),t})(),Z2=(()=>{class t{}return t.\u0275fac=function(n){return new(n||t)},t.\u0275mod=Gn({type:t,bootstrap:[Q2]}),t.\u0275inj=dn({providers:[],imports:[[JR,Y2]]}),t})();(function(){if(Ay)throw new Error("Cannot enable prod mode after platform setup.");Iy=!1})(),ZR().bootstrapModule(Z2).catch(t=>console.error(t))}},Ai=>{Ai(Ai.s=950)}]);