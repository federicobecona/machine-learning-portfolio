[
    {
        "unidad":"Preparación de datos",
        "titulo":"Normalización sobre dataset Wine en RapidMiner",
        "descripcion":"En RapidMiner se realizó una normalización sobre el dataset Wine y se comparó la performance de aplicar Naive Bayes usando el dataset normalizado y no normalizado con un split con el 70% de ejemplos para training y 30% para test en ambos casos.",
        "contenido":"p>>- Se creó un proceso en RapidMiner con dos canales, uno que utiliza el dataset normalizado con transforamción Z y uno que no.=,=p>>- En ambos canales se realizó un split con 70% de datos para entrenamiento y 30% para test. =,=p>>- Para cada caso se entrenó un modelo de Naive Bayes para clasificación por lo cual hubo que convertir el atriburto Wine de numérico a polinómico.=,=p>>- Se le midió performance del modelo.=,=br=,=h3>>Resultados=,=img>>UT2-PD1-3.jpg>>100%>>400=,=br=,=a>>Descargar modelo>>UT2 PD1 ej2.rmp>>d=,=br=,=br=,=p>>Caso normalizado=,=img>>UT2-PD1-4.jpg>>100%>>200=,=br=,=br=,=p>>Caso sin normalizar=,=img>>UT2-PD1-5.jpg>>100%>>200=,=br=,=br=,=p>>Se obtuvo un apenas poco de mayor presición para el caso del dataset normalizado lo cual se debe al algoritmo utilizado. Naive Bayes calcula frecuencias para hallar las clases de predicción y al estandarizar los datos, las frecuencias se mantienen de una forma similar."
    },
    {   
        "unidad":"Preparación de datos",
        "titulo":"Tutoriales de preparación de datos en RapidMiner",
        "descripcion":"En este ejercicio se hicieron tutoriales de missing values, normalización, detección de outliers, modelado, scoring, split y validación, cross validation y visual model comparison para la herramienta RapidMiner.",
        "contenido":"h3>> Tutorial Handling Missing Values =,=p>>=,=img>>UT2-PD1-1.jpg>>100%>>200=,=br=,=a>>Descargar archivo>>Tutorial Handling Missing Values.rmp>>d =,=br=,=br=,=h3>> Tutorial Normalization and Outlier detection =,=img>>UT2-PD1-2.jpg>>100%>>150=,=br=,=a>>Descargar archivo>>Tutorial Normalization and Outlier detection.rmp>>d=,=br=,=br=,=h3>> Modeling =,=img>>UT2-PD2-1.jpg>>100%>>300 =,=br=,=a>>Descargar archivo>>Tutorial Modeling.rmp>>d=,=br=,=br=,=br=,=h3>> Scoring =,=img>>UT2-PD2-2.jpg>>100%>>250 =,=br=,=a>>Descargar archivo>>Tutorial Scoring.rmp>>d=,=br=,=br=,=br=,=h3>> Test Splits and Validation =,=img>>UT2-PD2-3.jpg>>100%>>175 =,=br=,=a>>Descargar archivo>>Tutorial Test Splits and Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Cross Validation =,=img>>UT2-PD2-4-1.jpg>>100%>>200=,=br=,=br=,=img>>UT2-PD2-4-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>> Tutorial Cross Validation.rmp>>d =,=br=,=br=,=br=,=h3>> Visual Model Comparison =,=img>>UT2-PD2-5-1.jpg>>100%>>125=,=br=,=br=,=img>>UT2-PD2-5-2.jpg>>100%>>150 =,=br=,=a>>Descargar archivo>>Tutorial Visual Model Comparison.rmp>>d"
    },
    {
        "unidad":"Preparación de datos",
        "titulo":"Pre procesamiento de datos en Wine con Python",
        "descripcion":"Para este caso se aplican técnicas de pre procesamiento como normalización, estandarización y split en datos de training y test. Luego se obtienen estadísticas para el dataset Wine utiliando funciones de Python con SciKitLearn y la librería Pandas.",
        "contenido":"p>>- Se descargó el dataset Wine.=,=p>>- Se imprimieron las columnas de las primeras 10 filas.=,=p>>- Se convirtieron los valores numéricos de string a float.=,=p>>- Se obtuvieron los valores mínimos y máximos para cada columna.=,=p>>- Se obtuvo la media y la desviación estándar de los valores de cada columna.=,=p>>- Se normalizaron y se estandarizaron los valores del dataset original.=,=p>>- Se dividió el dataset en conjuntos de entrenamiento y testing.=,=br=,=h3>>Código=,=code>>.from csv import reader=,=br=,=code>>.from math import sqrt=,=br=,=code>>.from random import randrange=,=br=,=code>>.import pandas as pd=,=br=,=code>>.import copy=,=br=,=code>>.=,=br=,=code>>.columns = ['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']=,=br=,=code>>.=,=br=,=code>>.def load_csv(filename):=,=br=,=code>>.\u00a0    dataset = list()=,=br=,=code>>.\u00a0    with open(filename, 'r') as file:=,=br=,=code>>.\u00a0\u00a0        csv_reader = reader(file)=,=br=,=code>>.\u00a0\u00a0        for row in csv_reader:=,=br=,=code>>.\u00a0\u00a0\u00a0            if not row:=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0                continue=,=br=,=code>>.\u00a0\u00a0\u00a0            dataset.append(row)=,=br=,=code>>.\u00a0\u00a0        return dataset=,=br=,=code>>.=,=br=,=code>>.def str_column_to_float(dataset, column):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        row[column] = float(row[column].strip())=,=br=,=code>>.=,=br=,=code>>.def dataset_minmax(dataset):=,=br=,=code>>.\u00a0    minmax = list()=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        value_min = min(col_values)=,=br=,=code>>.\u00a0\u00a0        value_max = max(col_values)=,=br=,=code>>.\u00a0\u00a0        minmax.append([value_min, value_max])=,=br=,=code>>.\u00a0    return minmax=,=br=,=code>>.=,=br=,=code>>.def column_means(dataset):=,=br=,=code>>.\u00a0    means = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        col_values = [row[i] for row in dataset]=,=br=,=code>>.\u00a0\u00a0        means[i] = sum(col_values) / float(len(dataset))=,=br=,=code>>.\u00a0    return means=,=br=,=code>>.=,=br=,=code>>.def column_stdevs(dataset, means):=,=br=,=code>>.\u00a0    stdevs = [0 for i in range(len(dataset[0]))]=,=br=,=code>>.\u00a0    for i in range(len(dataset[0])):=,=br=,=code>>.\u00a0\u00a0        variance = [pow(row[i]-means[i], 2) for row in dataset]=,=br=,=code>>.\u00a0\u00a0        stdevs[i] = sum(variance)=,=br=,=code>>.\u00a0\u00a0        stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]=,=br=,=code>>.\u00a0    return stdevs=,=br=,=code>>.=,=br=,=code>>.def normalize_dataset(dataset, minmax):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])=,=br=,=code>>.=,=br=,=code>>.=,=br=,=code>>.def standardize_dataset(dataset, means, stdevs):=,=br=,=code>>.\u00a0    for row in dataset:=,=br=,=code>>.\u00a0\u00a0        for i in range(len(row)):=,=br=,=code>>.\u00a0\u00a0\u00a0            row[i] = (row[i] - means[i]) / stdevs[i]=,=br=,=code>>.=,=br=,=code>>.def train_test_split(dataset, split=0.60):=,=br=,=code>>.\u00a0    train = list()=,=br=,=code>>.\u00a0    train_size = split * len(dataset)=,=br=,=code>>.\u00a0    dataset_copy = list(dataset)=,=br=,=code>>.\u00a0    while len(train) < train_size:=,=br=,=code>>.\u00a0\u00a0        index = randrange(len(dataset_copy))=,=br=,=code>>.\u00a0\u00a0        train.append(dataset_copy.pop(index))=,=br=,=code>>.\u00a0    return train, dataset_copy=,=br=,=code>>.=,=br=,=code>>.input_file = \"wine.csv\"=,=br=,=code>>.dataset = load_csv(input_file)=,=br=,=code>>.print(\"\n- Primeras 10 filas:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(dataset[i])=,=br=,=code>>.strToFloatDataset = copy.deepcopy(dataset)=,=br=,=code>>.for i in range(len(columns)+1):=,=br=,=code>>.\u00a0    str_column_to_float(strToFloatDataset,i)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset con floats:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(strToFloatDataset[i])=,=br=,=code>>.minmax = dataset_minmax(strToFloatDataset)=,=br=,=code>>.means = column_means(strToFloatDataset)=,=br=,=code>>.stdevs = column_stdevs(strToFloatDataset, means)=,=br=,=code>>.print(\"\n- Estadísticas: \")=,=br=,=code>>.for i in range(len(columns)):=,=br=,=code>>.\u00a0    print(\"*\"+columns[i] + \": min \" + str(minmax[i][0]) + \",  max \" + str(minmax[i][1]) + \", media \" + str(means[i]) + \", desviación estándar \" + str(stdevs[i]))=,=br=,=code>>.normalized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.standarized_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.train_test_split_dataset = copy.deepcopy(strToFloatDataset)=,=br=,=code>>.normalize_dataset(normalized_dataset, minmax)=,=br=,=code>>.standardize_dataset(standarized_dataset, means, stdevs)=,=br=,=code>>.train, test = train_test_split(train_test_split_dataset)=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset estandarizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(standarized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset normalizado:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(normalized_dataset[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.for i in range(10):=,=br=,=code>>.\u00a0    print(train[i])=,=br=,=code>>.print(\"\n- Primeras 10 filas dataset de train:\")=,=br=,=code>>.\u00a0for i in range(10):    =,=br=,=code>>.\u00a0    print(test[i])=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD3-1.jpg>>100%>>300=,=img>>UT2-PD3-2.jpg>>100%>>300=,=img>>UT2-PD3-3.jpg>>100%>>300=,=img>>UT2-PD3-4.jpg>>100%>>600=,=img>>UT2-PD3-5.jpg>>100%>>600"
    },
    {
        "unidad":"Preparación de datos",
        "titulo":"Data pre processing y cálculo de probabilidades de sucesos sobre el dataset Titanic con Python",
        "descripcion":"Sobre el dataset Titanic se realizó un tratamiento de missing values con reemplazo, remoción, sustitución por categoría única y selección de atributos. Posteriormente se realizaron consultas probabilísticas sobre los datos.",
        "contenido":"p>> - Se importaron librerías y paquetes.=,=p>> - Se cargó el dataset y se los mostró.=,=p>> - Se gestionaron los datos faltantes.=,=p>> - Se graficaron los datos según Age vs Survived y Fare vs Survived.=,=code>>.import pandas as pd=,=br=,=code>>.import matplotlib=,=br=,=code>>.import matplotlib.pyplot as plt=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.print(\"-----DATASET------\")=,=br=,=code>>.print(dataset.to_string())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----MISSSING VALUES------\")=,=br=,=code>>.print(\"*Antes de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.#Se reemplazan los missing values de edad por la media=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan las columnas body y cabin por altos porcentajes =,=br=,=code>>.# de missing values siendo estos 90.7% y 77% respectivamente=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\"], axis = 1, inplace = True)=,=br=,=code>>.=,=br=,=code>>.#Se eliminan la fila sin valor para fare y las dos filas=,=br=,=code>>.# Sin valor para embarked=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=['embarked'], how='all', inplace=True)=,=br=,=code>>.=,=br=,=code>>.#Se asigna una categoría única para los missing values de boay y home.dest=,=br=,=code>>.dataset[\"boat\"].fillna('U', inplace = True)=,=br=,=code>>.dataset[\"home.dest\"].fillna('U', inplace = True)=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"*Despues de data pre-procesing\")=,=br=,=code>>.print(pd.isnull(dataset).sum())=,=br=,=code>>.=,=br=,=code>>.print()=,=br=,=code>>.print(\"-----GRAFICAS------\")=,=br=,=code>>.print(\"- x = Age, y = Survived\")=,=br=,=code>>.print(\"- x = Fare, y = Survived\")=,=br=,=code>>.colors = (\"red\", \"blue\")=,=br=,=code>>.plt.scatter(dataset['age'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.plt.scatter(dataset['fare'], dataset['survived'], s=10, c=dataset['survived'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-1.jpg>>300>>600=,=br=,=img>>UT2-PD4-2.jpg>>400>>300=,=img>>UT2-PD4-3.jpg>>400>>300   =,=br=,=br=,=p>>- Se calculó la probabilidad de que una persona sobreviva dados su sexo y clase de pasajero para luego aplicarlo a una serie de valores concretos.=,=p>> - Se calculó la probabilidad de que un niño de 10 años o menos de 3ra clase sobreviva.=,=code>>.import pandas as pd=,=br=,=code>>.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=code>>.=,=br=,=code>>.def probSgivenGandC(G, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSGC = len(dataset[(dataset['sex'] == G) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probGC = sizeGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSGC = sizeSGC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSGC/probGC=,=br=,=code>>.print(\"\n-Probabilidades:\")=,=br=,=code>>.print(\"*female, class 1: \" + str(probSgivenGandC(\"female\",1)))=,=br=,=code>>.print(\"*female, class 2: \" + str(probSgivenGandC(\"female\",2)))=,=br=,=code>>.print(\"*female, class 3: \" + str(probSgivenGandC(\"female\",3)))=,=br=,=code>>.print(\"*male, class 1: \" + str(probSgivenGandC(\"male\",1)))=,=br=,=code>>.print(\"*male, class 2: \" + str(probSgivenGandC(\"male\",2)))=,=br=,=code>>.print(\"*male, class 3: \" + str(probSgivenGandC(\"male\",3)))=,=br=,=code>>.=,=br=,=code>>.def probSgivenMaxAandC(A, C):=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    size = len(dataset)=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    sizeSMaxAC = len(dataset[(dataset['age'] <= A) & (dataset['pclass'] == C) & (dataset['survived'] == 1)])=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probMaxAC = sizeMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    probSMaxAC = sizeSMaxAC/size=,=br=,=code>>.\u00a0\u00a0\u00a0\u00a0    return probSMaxAC/probMaxAC=,=br=,=code>>.print(\"*Less than 3yo, class 3: \" + str(probSgivenMaxAandC(10,3)))=,=br=,=br=,=h3>>Resultados=,=img>>UT2-PD4-4.jpg>>500>>200"
    },
    {
        "unidad":"Preparación de datos",
        "titulo":"Selección de atributos para dataset Sonar",
        "descripcion":"Ejercicio con selección de atributos en RapidMiner sobre el dataset Sonar.",
        "contenido":"p>>En la herramienta RapidMiner usaremos el dataset Sonar y compararemos los resultados de performance al aplicar un simple algoritmo de Naive Bayes como línea base, Forward Selection con Naive Bayes y Backward Elimination con Naive Bayes empleando cross validation con 5 folds, muestreo estratificado y la misma seed para los tres casos.=,=h4>>Modelo=,=img>>aa4.jpg>>100%>>600=,=a>>Descargar modelo>>UT4-TA9.rmp>>d=,=br=,=br=,=h4>>Gráficas=,=p>>Se observa una tendencia en las frecuencias capturadas por el sonar. Vemos una amplitud y varianza clara en ambas clases. El problema que se observa a primera vista, es que las frecuencias se solapan demasiado, haciendo muy difícil su diferenciación.=,=img>>UT4-PD9-1.jpg>>100%>>400=,=br=,=br=,=img>>UT4-PD9-2.jpg>>100%>>400=,=br=,=br=,=h4>>Resultados=,=br=,=h6>>Línea base Naive Bayes=,=img>>UT4-PD9-3.jpg>>900>>130=,=br=,=br=,=h6>>Forward Selection=,=img>>UT4-PD9-4.jpg>>900>>130=,=p>>Mediante este algoritmo de feature selection se redujeron los atributos utilizados y la precisión de predicciones para la clase mina mejoró sustancialmente, esto quiere decir que usando solo los atributos relevantes, que representan una mayor variación en la información la precisión aumentó. Los atributos elegidos fueron los 12, 15, 17 y 18.=,=h6>>Backward Elimination=,=img>>UT4-PD9-5.jpg>>900>>130=,=p>>Este algoritmo de feature selection eliminó 8 atributos y con ello se mejoró la performance respecto a la línea base al igual que en el caso anterior."
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Regresión lineal simple en hoja de cálculo",
        "descripcion":"Utilizando una planilla electrónica se generó un modelo de regresión linear simple calculando los coeficientes en el primer ejercicio y por descenso de gradiente en el segundo ejercicio. Para ambos casos se usaron los modelos para hacer predicciones.",
        "contenido":"p>>La regresión lineal es el algoritmo de machine learning mejor entendido consistiendo en una representación lineal entre las entradas x y la salida y para calcular predicciones en casos supervisados de regresión.=,=p>>Cuando hay una sola variable de entrada se llama regresión lineal simple teniendo una forma de tipo y = B0 + B1 x X y cuando hay más lleva el nombre de regresión lineal múltiple siendo por ejemplo y = B0 + B1 x X1 + B2 x X2 para el caso de dos predictores.=,=p>>El entrenamiento de modelos se basa en hallar los coeficientes que mejor aproximen una relación lineal entre las entradas y la salida y suele consistir en utilizar ecuaciones estadísticas para el caso simple y mínimos cuadrodos ordinarios (ordinary least squares) o descenso de gradiente si hay múltiples predictores. El descenso de gradiente optimiza los coeficientes de la relación lineal iterando sobre los datos de entrenamiento para minimizar el error del modelo e ir aproximándo los parámetros cada vez mejor según un parámetro de aprendizaje alfa. Los coeficientes comienzan valiendo 0 en la primer iteración.=,=p>>Para realizar predicciones con un modelo de regresión lineal basta con resolver la ecuación con los datos de entrada utilizando los coeficientes hallados en el entrenamiento.=,=p>>Es importante mencionar que se requieren muchos pasos para preparar los datos en el caso de la regresión lineal convertir todas las variables nominales a numéricas en caso de tenerlas, valorar si existe una relación lineal entre las entradas y la salida, remover las variables de entrada correlacionadas entre sí, observar si hay distribuciones gaussianas entre las variables y considerar utilizar transformaciones logarítmicas o box cox y reescalar las entradas por estandarización o normalización. =,=p>>Para mostrar estos conceptos de forma practica comenzaremos hallando los coeficientes estadísticamente para el caso de una regresión lineal simple con los siguientes valores de entrada.=,=img>>aa8.jpg>>75%>>500=,=br=,=br=,=p>>- Se utilizarán las fórmulas:=,=img>>UT3-PD1-1.jpg>>600>>100=,=br=,=br=,=img>>UT3-PD1-2.jpg>>500>>50=,=br=,=br=,=p>>Los valores obtenidos fueron B0 = 1.466667 y B1 = 0.342857.=,=p>>Por otra parte, se realizó un método alternativo con la siguiente ecuación para calcular B1 y B0 a partir de este al igual que antes llegando a los mismos valores.=,=img>>UT3-PD1-4.jpg>>400>250 =,=p>> =,=p>> -Se aplicó el modelo a los valores de entrenamiento.=,=img>>aa9.jpg>>25%>>200=,=br=,=br=,=p>>-Se estimó el error de predicción RMSE llegando a un valor de 1.101 con esta ecuación:=,=img>>UT3-PD1-3.jpg>>30%>>100=,=br=,=br=,=p>> -Se añadió una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo hallado a los valores de la anterior. A partir de estas se hizo un gráfico para mostrar la recta de ajuste.=,=img>>aa10.jpg>>75%>>600=,=br=,=br=,=p>>Ahora nos enfocaremos en mostrar como se realiza descenso de gradiente para un ejemplo practico también de regresión lineal simple usando los mismos datos para X e Y que en el caso anterior.=,=p>>-Se estimaron los coeficientes del modelo simple de regresión lineal Y = B0 + B1 x X realizando 24 iteraciones. En un principio los coeficientes B0 y B1 comienzan con 0 y luego para cada época estos se modifican según los datos de la época anterior de esta manera:=,=p>> B0 = B0 de la época anterior - alfa * error de predicción época anterior=,=p>>B1 = B1 de la época anterior - alfa * error de predicción época anterior=,=p>>Usando alfa = 0.01 tras todas las iteraciones se llegö a los valores B0 = 0.21781 y B1 = 0.6382.=,=p>>-Se aplicó el modelo hallado a los valores de entrada obteniendo las siguientes cifras de predicción.=,=img>>aa11.jpg>>25%>>200.=,=p>>-Calculando el error medio cuadrático RMSE empleando la misma fórmula que en el caso anterior se llegó a RMSE = 1.230204 =,=p>>-Se realizó el gráfico error de predicción contra iteraciones.=,=img>>aa12.jpg>>60%>>400=,=br=,=br=,=p>>- Nuevamente se añadió una columna con valores de x entre 0 y 8 con paso 0.1 y otra con el resultado de aplicar el modelo a las entradas.=,=img>>aa13.jpg>>75%>>600=,=br=,=br=,=p>>Comparando los dos métodos para llegar al modelo de regresión lineal simple se observa que en el primero se obtuvo un menor error RMSE y que la segunda tiene una pendiente mayor al anterior.=,=p>>La hoja de trabajo donde se realizaron los dos ejercicios explicados anteriormente y en la que se encuentran los resultados correspondientes puede descargarla en el siguiente enlace =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD1.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Regresión logística en planilla electrónica",
        "descripcion":"En una planilla electrónica se visualizará una función logística y se genararán modelos de regresión logística con descenso de gradiente estocástico.",
        "contenido":"p>>La regesíón logística es un algoritmo que resuelve problemas supervisados de clasificación binaria en base a la función logística la cual tiene la siguiente forma y gráfica.=,=img>>aaa1.jpg>>250>>100=,=br=,=br=,=img>>aaa2.jpg>>75%>>375.=,=p>>En el caso de la regresión lineal se utilza la siguiente función de tipo logística..=,=img>>a1.jpg>>250>>100=,=br=,=br=,=p>>A partir de esta se entrena hallando los valores de los coeficientes con descenso de gradiente estocástico y tras ello se realizan predicciones reemplazando los valoes en la función obtenida. Siempre los resultados obtenidos estarán en el rango (0,1) por lo que definiendo un cierto valor en ese rango se determinan dos subrangos en los que a partir del resultado obtenido de la función evaluada con ciertos datos de entrada entrada se los clasifica como de una clase u otra.=,=p>>Para aplicar regresión logística es conveniente remover outliers, tener distribuciones gaussianas para lo que puede contribuir realizar transformar logarítmicas o box cox y quitar entradas correlacionadas.=,=p>>En esta ocasiön veremos un ejercicio práctico en el que se generará un modelo de regresión logística con el que realizar predicciones a partir del siguiente conjunto de datos.=,=img>>a2.jpg>>30%>>400=,=br=,=br=,=p>>Se estimaron los coeficientes del modelo de regresión logística Y = B0+B1xX1+B2xX2 con el algoritmo de descenso de gradiente estocástico utilizando un coeficiente alpha = 0.3 y repitiendo el proceso para 10 épocas lo que equivale a hacer 10 iteraciones sobre cada ejemplo del dataset.=,=p>>Para ver cómo van mejoran las predicciones al aumentar las épocas se realizaron las gráficas de exactitud vs épocas y RMSE VS épocas generándose las siguientes visualizaciones.=,=img>>a3.jpg>>100%>>300=,=br=,=br=,=p>>La hoja de trabajo donde se realizaron los procedimientos anteriores puede ser descargada en el siguiente enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"LDA en hoja de cálculo",
        "descripcion":"Empleando una planilla electrónica se generará un modelo de análisis de discriminante lineal LDA y con él se hará la predicción de clase para un conjunto de datos con un atributo y una salida con dos clases.",
        "contenido":"p>>El algoritmo de Linear Discriminant analysis se utiliza para problemas de clasificación sin tener la restricción de ser solo para variables nominales como ocurria en Logistic Regression. Este modelo casume que los datos tienen distribución gaussiana y que los atributos tienen la misma varianza.=,=p>>Asumiendo los puntos anteriores LDA estima la media y la varianza para cada clase utilizando las siguientes ecuaciónes.=,=img>>a4.jpg>>30%>>90=,=img>>a5.jpg>>40%>>90=,=p>>LDA hace predicciones estimando las probabilidades de que un input pertenezca a una clase utilizando el discriminante de las diferentes clases cuya teoría proviene del teorema de Bayes y su fórmula para la clase k se encuentra a continuación.=,=img>>a6.jpg>>60%>>100=,=p>>Siendo P(k) el ratio de instancias con esa clase en el dataset.=,=p>>Finalmente, para un ejemplo se le designa la clase cuyo discriminante de un valor más alto.=,=p>>A partir de la teoría anterior se procede a realizar un ejemplo practico que se muestra contiguamente.=,=p>>- Se insertaron los valores de un dataset con variables numéricas X para la entrada e Y para la salida. =,=p>> - Se graficaron los datos separando según las clases de salida.=,=img>>a7.jpg>>60%>>400=,=br=,=br=,=p>> - Se calcularon, siendo n el número de clases, las medias para cada clase k y la varianza de x con las ecuaciones que se mostraron anteriormente.=,=p>> - Se calculó la predicción de clases usando: =,=img>>UT3-PD3-4.jpg>>400>200 =,=p>> - Se utilizó la ecuación para calcular los discriminantes de x para ambas clases Y = 0 e Y = 1 y finalmente se realizaron las predicciones de clase para los datos comparando los discriminantes. La exactitud alcanzada por el modelo generado fue calculada dando un valor de 0.65.=,=p>>La hoja de trabajo donde se hizo la práctica puede descargarse en este enlace: =,=a>>Descargar planilla de trabajo>>Algoritmos lineales PD3.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Regresión lineal con descenso de gradiente",
        "descripcion":"En este ejercicio se usa una planilla electrónica para minimizar una función siguiendo los gradientes de la función de costo y luego armar un modelo de regresión lineal.",
        "contenido":"p>>- Se representó en la planilla un dataset con una variable de entrada X y una de salida Y.=,=p>> - Se graficaron los datos. =,=img>>UT3-PD4-1.jpg>>500>400=,=p>> - Se realizó el procedimiento de descenso de gradiente con alpha=0.01 en 24 iteraciones hallando valores para los coeficientes B0 y B1 que en una regresión lineal siguen la relación Y = B1xX + B0, la predicción para dicho modelo y el error de predicción. =,=p>> - Se graficó el error de predicción vs iteraciones. =,=img>>UT3-PD4-2.jpg>>500>400=,=p>> - Se calculó el error medio cuadrático RMSE. =,=p>> - Se generaron nuevos valores de entrada para X entre 0 y 8 con un paso 0.1 y se predijo su valor de Y a partir de un modelo de regresión lineal usando los coeficientes hallados al finalizar el procedimento mencionado anteriormente. El resultado fue graficado. =,=img>>UT3-PD4-3.jpg>>500>400 =,=p>> - Se analizaron los datos de entrada desde la óptica de los requerimientos para aplicar un método de regresión lineal.=,=br=,=h3>>Resultados=,=p>> La planilla de trabajo donde se realizaron los pasos descriptos anteriormente se encuentra accesible para la descarga con el enlace que se encuentra a continuación:=,=a>>Descargar plantilla>>Algoritmos lineales PD4.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Comparación de LDA con Python y RapidMiner para el dataset Sports",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificación a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.",
        "contenido":"h2>>Ejercicio 1=,=p>>- Se descargó un dataset de prueba Sample.csv.=,=p>>- Se leyó el archivo CSV y se graficaron los datos utilizando pyplot de matplotlib.=,=p>>- Se dividió el conjunto de datos en una parte de entrenamiento y otra de prueba.=,=p>>- Se creó y entrenó un modelo LDA.=,=p>>- Se predijeron las clases para los datos de prueba y se compararon los resultados.=,=p>>- Se imprimió el reporte de clasificación y la matriz de confusión=,=code>>.=,=br=,=code>>.input_file = \"sample.csv\"=,=br=,=code>>.df = pd.read_csv(input_file, header=0)=,=br=,=code>>.print(df.values)=,=br=,=code>>.=,=br=,=code>>.colors = (\"orange\", \"blue\")=,=br=,=code>>.plt.scatter(df['x'], df['y'], s=300, c=df['label'],cmap=matplotlib.colors.ListedColormap(colors))=,=br=,=code>>.plt.show()=,=br=,=code>>.=,=br=,=code>>.X = df[['x', 'y']].values=,=br=,=code>>.y = df['label'].values=,=br=,=code>>.=,=br=,=code>>.train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25,random_state=0, shuffle=True)   =,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(train_X, train_y)=,=br=,=code>>.=,=br=,=code>>.y_pred = lda.predict(test_X)=,=br=,=code>>.print(\"Predicted vs Expected\")=,=br=,=code>>.print(y_pred)=,=br=,=code>>.print(test_y)=,=br=,=code>>.=,=br=,=code>>.print(classification_report(test_y, y_pred, digits=3))=,=br=,=code>>.print(confusion_matrix(test_y, y_pred)).=,=br=,=br=,=h3>>Resultados=,=img>>UT3-PD5-1.jpg>>600>>400=,=img>>UT3-PD5-2.jpg>>700>>350=,=br=,=br=,=h2>>Ejercicio 2=,=p>>- Se descargó el dataset sports_Training.csv.=,=p>>- Se eliminaron las filas con atributo CapacidadDecision menores de 3 y mayores a 100. =,=p>>- Se transformaron los atributos string a números. =,=p>> - Se utilizó el modelo para clasificar los datos del archivo sports_Scoring.csv.=,=p>>- Se realizó el procedimiento equivalente en la herramienta RapidMiner.=,=br=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = ['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']=,=br=,=code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training['CapacidadDecision'] >= 3) & (data_training['CapacidadDecision'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring['CapacidadDecision'] >= 3) & (data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Esquema proceso RapidMiner=,=img>>UT3-PD5-3.jpg>>900>>300=,=br=,=a>>Descargar proceso>>Algoritmos Lineales PD5.rmp>>d=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Titanic",
        "descripcion":"Se realizará un análisis del problema y de los datos del dataset Titanic para a partir de este crear un modelo de regresión logística con la herramienta RapidMiner y otro equivalente con SciKitLearn, medir sus performances y compararlas.",
        "contenido":"h2>>Problema=,=p>>A partir de datos respecto al hundimiento de Titanic se pretende predecir si una persona con determinadas características sobrevivió o no por lo cual es un problema supervisado de clasificación binaria. Para este se propone una solución con un algoritmo lineal de regresión logística utilizando CrossValidation para el testeo de su performance.=,=h2>>Análisis de datos=,=p>>El dataset cuenta con 13 atributos de entrada más la variable objetivo \"survived\" para los cuales se cuenta con 1310 observaciones.=,=h4>>Atributos=,=p>>- pclass: Clase del pasajero. Tipo integer.=,=p>>- name. Nombre del pasajero. Tipo string.=,=p>>- sex. Sexo del viajante. Tipo string.=,=p>>- age. Edad de la persona. Tipo real. Hay 263 datos faltantes para esta variable.=,=p>>- sibsp: Número de hermanos a bordo. Tipo entero.=,=p>>- parch: Número de padres a bordo. Tipo entero.=,=p>>- ticket: Código del ticket del pasajero. Tipo string.=,=p>>- fare: Tarifa pagada. Tipo real. Hay un dato faltante para esta entrada.=,=p>>- cabin. Tipo string. Cabina del viajero. Tipo string. Tiene 1014 missing values. =,=p>>- embarked: Puerto en el que embarcó. Tipo string. Faltan dos de estos datos.=,=p>>- boat: Código del bote salvavida que usó la persona. Tipo string. Cuenta con 823 missing values.=,=p>>- body: Número de cuerpo si falleció. Tipo integer. Cuenta con 1188 missing values. =,=p>>- home.dest: Lugar de destino. Tipo string. Tiene 564 missing values.=,=p>>- survived: Si sobrevivió o no. Tipo integer.=,=br=,=h3>>Preparación de datos=,=br=,=h5>>Missing Values=,=p>>Entre las entradas hay variables con alto porcentaje de datos faltantes por lo que no se contó con ellas siendo estas body con un 90.7% y cabin con un 77.4%, para la variable age se reemplazaron los valores faltantes por el promedio, se eliminaron las filas con missing values para fare y embarked y se le asignó una categoría única para los missing values de boat y home.dest. Luego de todo este proceso se eliminaron todos los missing values.=,=h5>>Outliers=,=p>>El algoritmo de regresión logística es altamente influenciable a los outliers y por lo tanto tras observar los histogramas de los datos para los distintos parámetros, que se encuentran a continuación, se eliminaron aquellos con una alguna/s de las siguientes condiciones age>=70, sibsp>=6, parch>=6 o fare>=350.=,=img>>caso-2-2.jpg>>300>>150=,=img>>caso-2-3.jpg>>300>>150=,=br=,=img>>caso-2-4.jpg>>300>>150=,=img>>caso-2-5.jpg>>300>>150=,=h5>>Correlación=,=p>>Observando la matriz de correlación se observó que hay una correlación grande entre home.dest y pclass con 0.887 y otra bastante significativa con un 0.612 entre fare y pclass de forma inversamente proporcional. Por lo tanto se decidió quitar pclass.=,=img>>caso-2-6.jpg>>600>>200=,=br=,=br=,=h5>>Selección de atributos=,=p>>Se descartaron las variables name y ticket por ser ids.=,=h3>>Modelo=,=p>>Se realizó el procedimiento de data pre-processing descripto anteriormente y luego se utilizó un algoritmo de regresión logística con CrossValidation de 10 folds y muestreo estratificado para el testeo.=,=h5>>Diseño de RapidMiner=,=img>>caso-2-7.jpg>>800>>400=,=p>>A continuación se mostrará la configuración bloque a bloque del esquema de RapidMiner y se presentará a la vez el código del modelo idéntico creado en SciKitLearn.=,=h5>>Retrieve Titanic=,=img>>caso-2-rm-1.jpg>>400>>75.=,=br=,=code>>.input_file = \"titanic.csv\"=,=br=,=code>>.dataset = pd.read_csv(input_file, header=0)=,=br=,=br=,=h5>>Select Attributes=,=img>>caso-2-rm-2.jpg>>600>>400.=,=br=,=code>>.dataset.drop(labels = [\"body\", \"cabin\",\"name\",\"pclass\",\"ticket\"], axis = 1, inplace = True)=,=br=,=br=,=h5>>Primer replace Missing Values=,=img>>caso-2-rm-3.jpg>>400>>175=,=br=,=code>>.dataset[\"age\"].fillna(dataset[\"age\"].mean(), inplace = True)=,=br=,=br=,=h5>>Segundo replace Missing Values=,=img>>caso-2-rm-4.jpg>>600>>400=,=br=,=code>>.dataset[\"boat\"].fillna('0', inplace = True)=,=br=,=code>>.dataset[\"home.dest\"].fillna('0', inplace = True)=,=br=,=br=,=h5>>Filter Examples=,=img>>caso-2-rm-5.jpg>>600>>400.=,=br=,=code>>.dataset = dataset[dataset['sibsp'] <= 5]=,=br=,=code>>.dataset = dataset[dataset['parch'] <= 5]=,=br=,=code>>.dataset = dataset[dataset['age'] <= 70]=,=br=,=code>>.dataset = dataset[dataset['fare'] <= 350]=,=br=,=code>>.dataset.dropna(subset=['fare'], how='all', inplace=True)=,=br=,=code>>.dataset.dropna(subset=['embarked'], how='all', inplace=True)=,=br=,=br=,=h5>>Nominal to Numerical=,=img>>caso-2-rm-6.jpg>>600>>400..=,=br=,=code>>.le = LabelEncoder()=,=br=,=code>>.le.fit(dataset[\"sex\"])=,=br=,=code>>.encoded_sex_training = le.transform(dataset[\"sex\"])=,=br=,=code>>.dataset[\"sex\"] = encoded_sex_training=,=br=,=code>>.le.fit(dataset[\"embarked\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"embarked\"])=,=br=,=code>>.dataset[\"embarked\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"home.dest\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"home.dest\"])=,=br=,=code>>.dataset[\"home.dest\"] = encoded_embarked_training=,=br=,=code>>.le.fit(dataset[\"boat\"])=,=br=,=code>>.encoded_embarked_training = le.transform(dataset[\"boat\"])=,=br=,=code>>.dataset[\"boat\"] = encoded_embarked_training=,=br=,=br=,=h5>>Cross Validation=,=img>>caso-2-rm-7.jpg>>100%>>150.=,=br=,=code>>.X = dataset.drop(labels=[\"survived\"], axis=1)=,=br=,=code>>.y = dataset[\"survived\"]=,=br=,=code>>.X, y = make_classification(n_samples=1283, n_features=10)=,=br=,=code>>.cv = StratifiedKFold(n_splits=10, random_state=None, shuffle=True)=,=br=,=code>>.model = LogisticRegression()=,=br=,=code>>.scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)=,=br=,=code>>.print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))=,=br=,=br=,=h4>>Resultados=,=p>>Se obtuvo un muy alto nivel de presición en RapidMiner con un 96.88% +/- 1.1%. =,=img>>caso-2-9.jpg>>100%>>200=,=br=,=br=,=img>>caso-2-10.jpg>>200>>25=,=p>> En cuanto al modelo de SciKitLearn se obtuvo una presición de 94.5% +/- 1.2%. De esta forma se puede destacar que se obtuvieron presiciones similares para de los modelos idénticos de regresión logística aplicados al dataset Titanic realizados en las dos herramientas.=,=br=,=a>>Descargar modelo RapidMiner>>Titanic Logistic Regression.rmp>>d=,=br=,=a>>Descargar código SciKitLearn>>Titanic Logistic Regression.py>>d"
    },
    {
        "unidad":"Algoritmos lineales",
        "titulo":"Deportes",
        "descripcion":"Utilizaremos Análisis de Discriminante Lineal (LDA) con Sckit-learn de Python y RapidMiner para entrenar y realizar la clasificación a partir del dataset de Sports para finalmente comparar las predicciones realizadas por ambos modelos.",
        "contenido":"h3>>Problema=,=p>>Según características personales se desea predecir cual es el mejor deporte para una persona por lo cual este es un problema supervisado de clasificación. Para este se planteará un modelado con Linear Descriminant Analysis a continuación.=,=h3>>Análisis de datos=,=p>>El dataset tiene 8 columnas de entrada numéricas y la variable objetivo categórica \"DeportePrimario\" conteniendo 493 filas.=,=h4>>Atributos=,=p>>- Edad=,=p>>- Fuerza=,=p>>- Velocidad =,=p>>- Lesiones=,=p>>- Vision =,=p>>- Resistencia =,=p>>- Agilidad =,=p>>- CapacidadDecision =,=p>>- DeportePrimario=,=br=,=h3>>Preparación de datos=,=br=,=h5>>Missing Values=,=p>>En el dataset no hay datos faltantes.=,=h5>>Outliers=,=p>>Para LDA es importante quitar los outliers y tras observar los histogramas de los atributos se decidió quitar los datos con CapacidadDecision<3 o CapacidadDecision>100. La distribución para esta variable es la siguiente.=,=img>>caso-3-1.jpg>>300>>200=,=h4>>Estándarización=,=p>>El algoritmo a utilizar asume que las variables de entrada tienen la misma varianza por lo que es buena idea estandarizar los datos para que tengan media 0 y varianza 1.=,=h3>>Modelo=,=p>>El procedimiento descripto anteriormente y el modelo LDA fueron realizados de forma equivalente en RapidMiner y SciKitLearn. Se contó con un dataset de entrenamiento y uno de prueba para el cual realizar predicciones.=,=h4>>Diseño de RapidMiner=,=img>>UT3-PD5-3.jpg>>900>>300=,=br=,=br=,=h4>>Código SciKitLearn=,=code>>.import pandas as pd=,=br=,=code>>.from sklearn.discriminant_analysis import LinearDiscriminantAnalysis=,=br=,=code>>.from sklearn.preprocessing import StandardScaler=,=br=,=code>>.=,=br=,=code>>.labels = ['Edad','Fuerza','Velocidad','Lesiones','Vision','Resistencia','Agilidad','CapacidadDecision']=,=br=,=code>>.=,=br=,=code>>.input_file = \"sports_Training.csv\"=,=br=,=code>>.input_file2 = \"sports_Scoring.csv\"=,=br=,=code>>.data_training = pd.read_csv(input_file, header=0)=,=br=,=code>>.data_scoring = pd.read_csv(input_file2, header=0)=,=br=,=code>>.=,=br=,=code>>.data_training = data_training[(data_training['CapacidadDecision'] >= 3) & (data_training['CapacidadDecision'] <= 100)]=,=br=,=code>>.data_scoring = data_scoring[(data_scoring['CapacidadDecision'] >= 3) & (data_scoring['CapacidadDecision'] <= 100)]=,=br=,=code>>.=,=br=,=code>>.scaler = StandardScaler()=,=br=,=code>>.data_training[labels] = scaler.fit_transform(data_training[labels])=,=br=,=code>>.data_scoring[labels] = scaler.fit_transform(data_scoring[labels])=,=br=,=code>>.=,=br=,=code>>.X = data_training[labels].values=,=br=,=code>>.Y = data_training['DeportePrimario'].values=,=br=,=code>>.=,=br=,=code>>.lda = LinearDiscriminantAnalysis()=,=br=,=code>>.lda = lda.fit(X, Y)=,=br=,=code>>.=,=br=,=code>>.Xsco = data_scoring[labels].values=,=br=,=code>>.y_pred = lda.predict(Xsco)=,=br=,=code>>.df = pd.DataFrame({'Prediccion': y_pred})=,=br=,=code>>.writer = pd.ExcelWriter('Prediccion.xlsx', engine='xlsxwriter')=,=br=,=code>>.df.to_excel(writer, sheet_name='Sheet1')=,=br=,=code>>.writer.save()=,=br=,=br=,=h3>>Comparación de resultados entre Scikitlearn y RapidMiner=,=p>>Se obtuvo un 98.81% de valores de predicción iguales para los modelos realizados con Sckitlearn y RapidMiner.=,=a>>Descargar proceso>>Sports LDA.rmp>>d=,=br=,=a>>Descargar código SciKitLearn>>Sports LDA.py>>d=,=br=,=a>>Descargar plantilla comparativa>>Comparación Rapid Miner Sci Kit Learn.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"CART para clasificación binaria simple",
        "descripcion":"Emplearemos un modelo CART para un problema de clasificación binaria simple con dos variables de entrada X1 y X2 y una variable de salida Y.",
        "contenido":"p>>Los árboles de decisión son un importante algoritmo de predicción de machine learning para predicción supervisada de tanto para clasificación como para regresión. Esta estructura es la misma de algoritmos y estructuras de datos, árboles binarios, pero en este caso cada nodo representa una sola variable de entrada x y un split point para esa variable. Los nodos hojas se diferencian en que contienen un valor de salida el cual es utilizado para hacer la predicción. Tras tener entrenado el árbol es muy sencillo hacer prediccíones, solo basta con recorrer el árbol con los datos de entrada a etiquetar.=,=p>>Para entrenar los árboles de decisión recursivamente se divide el espacio de la entrada de forma binaria utilizando un algoritmo voraz el cual consiste en probar diferentes splits utilizando una funcion de costos y quedarse con el que lo minimice.=,=p>>En los problemas de regression la funcion de costos a minimizar es la suma de error cuadrático.=,=img>>pds0.jpg>>250>>75=,=br=,=br=,=p>>En los problemas de clasificación se utiliza el índice de Gini que indica que tan puros son los nodos hoja.=,=img>>aa1.jpg>>250>>75=,=br=,=br=,=p>>El criterio de freno del alguritmo recursivos más común es utilizar un mínimo de instancias asignadas a cada nodo hoja, cuando al realizar un splir se encuentra con un número menor a ese no lo hace y ese nodo se designa hoja. Cuantos más nodos por hoja más genérico es el árbol y al tener menos es más específico lo que puede tender al overfitting.=,=p>>A continuación se realizará un ejercicio en una hoja de cálculo que muestra como realizar una partición en base al índice de Gini para armar un árbol de decisión sencillo que luego será utilizado para realizar predicciones.=,=p>>- Se ingresó el dataset de entrada y se graficaron los datos diferenciando las clases Y = 0 e Y = 1. =,=img>>UT4-PD1-1.jpg>>600>>200=,=p>> - Se utilizó la variable X1 para armar el modelo CART con un valor para el punto de división de X1 = 2.771244718 calculando el índice Gini del modelo producido. =,=p>> - Se repitió el proceso anterior con un punto de división de X1 = 6.642287351 y se obtuvo el valor del coeficiente Gini del CART generado además de una descripción del árbol de decisión generado.=,=img>>aa2.jpg>>100%>>400=,=p>> Se utilizó el CART producido para hacer predicciones de un dataset de test y se calculó la exactitud.=,=img>>aa3.jpg>>50%>>300=,=br=,=br=,=p>> Las planillas de trabajo en las que se hicieron los pasos explicados están accesibles para descargar con los siguientes enlaces:=,=a>>Descargar plantilla entrenamiento>>Algoritmos no lineales PD1-1.xlsx>>d=,=br=,=a>>Descargar plantilla predicciones>>Algoritmos no lineales PD1-2.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"Árboles de decisión en KNIME",        
        "descripcion":"Estudiaremos la construcción de un modelo de árbol de decisión de regresión simple en la herramienta KNIME.",
        "contenido":"img>>UT4-PD2-1.jpg>>100%>500=,=br=,=br=,=h3>>Análisis=,=br=,=h5>>Descripción=,=p>> Este workflow utiliza el dataset Iris dividiéndolo con muestreo estratíficado en una parte de entrenamiento de un 80% y otra de prueba de un 20% para el caso de un modelo de árbol de regresión simple del cual se obtiene la performance y se la visualiza a través de una es gráfica. En este modelo a partir del ancho y el largo del sépalo, el largo de los pétalos y la clase de una flor entre Iris-setosa, Iris-versicolor y Iris virgínica predice el ancho de sus pétalos. =,=h5>> Componentes =,=h6>>File reader=,=p>>El operador equivalente a FileReader en RapidMiner es Retrieve. Estos se diferencian en que el primero deja editar características del dataset luego de ser incluido en el proyecto como los tipos para las variables y en RM esto se configura solo al importar el dataset. En el dataset sepal length, sepal width, petal length y pethal width son doubles y class es una string.=,=img>>UT4-PD2-2.jpg>>500>>650=,=br=,=br=,=h6>>Partitioning=,=p>> El operador partitioning ofrece elegir el tamaño de la partición y la forma en la que se quiere separar determinando por ejemplo si se eligen los primeros valores o si se hace muestreo estartíficado, da la opción para usar una seed como en RM, permite alternativas acerca de las Flow Variables que funcionan para a hacer variar ciertas configuraciones en el nodo de forma dinámica con cada ejecución y deja a elección políticas sobre el uso de la memoria. =,=p>> El operador equivalente a Patitioning en RapidMiner es Split. La principal diferencia es que mientras que en el componente de KNIME se hace una partición por cada unidad del componente, en RM se permite realizar varias a la vez.=,=img>>UT4-PD2-3.jpg>>500>>400=,=br=,=br=,=h6>>Simple Regression Tree Learner.=,=p>> El proceso utiliza un algoritmo base de árbol de regresión simple siguiendo el algoritmo descripto en “Classification and Regression Tress” (Breiman et al, 1984) con algunas simplifaciones como no pruning, no necesariamente árboles binarios, tratar de encontrar la mejor dirección para los missing values, etc. En estos árboles de regesión el valor de predicción el valor para cada nodo hoja es la media de los registros dentro de ella y la predicción mejora cuanto menor sea la varianza de los valores dentro de una hoja. Por lo tanto, para armarlo en cada nodo se hacen splits que minimicen la suma de errores cuadráticos de los hijos. El operador Simple Regression Tree Learner soporta predictores de tipo numérico y categórico aunque solo soporta target columns de tipo numérico.=,=p>> Los parámetros que se pueden configurar del algoritmo son determinar el uso o no splits binarios para los atributos nominales, la forma en la que se manejan los missing values siendo XGBoost un algoritmo que calcula la mejor dirección para los valores faltantes y Surrogate que calcula para cada Split otros alternativos que mejoran la aproximación, el límite para la profundidad del árbol, el mínimo de valores que puede tener un nodo para que el Split se intente y el mínimo de registros que un nodo hijo puede tener.=,=img>>UT4-PD2-4.jpg>>100%>>800=,=br=,=br=,=h6>>Simple Regression Tree Predictor=,=p>> Este operador recibe por un lado el modelo entrenado y por otro los datos de test, cómo salida tiene las predicciones realizadas. Permite modificar manualmente la columna de predicción, utlizar Flow Variables y decidir sobre políticas de memoria. =,=img>>UT4-PD2-5.jpg>>400>>300=,=br=,=br=,=h6>>Line Plot=,=p>> Este componente muestra una gráfica que compara los valores de predicción con la salida conocida para esos inputs del dataset de entrenamiento.=,=p>> Los parámetros que se pueden editar son el número de filas a mostrar, el límite del valor nominal a partir del cual una columna sea ignorada y la opción de colocar Flow Variables.=,=img>>UT4-PD2-6.jpg>>400>>300=,=br=,=br=,=img>>UT4-PD2-7.jpg>>500>>500=,=br=,=br=,=h6>>Numeric Scorer=,=p>> El operador funciona realizando los cálculos para los valores de coeficiente de terminación, media de error absoluto, error cuadrático medio y desviación media con signo de la predicción realizada.=,=img>>UT4-PD2-8.jpg>>500>>400=,=br=,=br=,=img>>UT4-PD2-9.jpg>>250>>125"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"Comparación de Decision Trees en herramientas de ML",
        "descripcion":"Comparación de árboles de decisión entre RapidMiner y Weka aplicados al dataset Iris. Además se analiza el componente mencionado en Azure ML Studio, KNIME y Python Sci kit learn.",
        "contenido":"p>>Se utiliza el dataset Iris y las herramientas RapidMiner y Weka para hacer una comparación de la performance de sus modelos de árboles de decisión midiéndola con un Split de datos de un 70% y un 30% para entrenamiento y test respectivamente en ambas.=,=h4>>RapidMiner=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. C4.5=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión. =,=h6>>Parámetros=,=p>>- Criterion: gain_ratio. Selecciona el criterio que se usará para seleccionar los atributos sobre los que hacer los splits.=,=p>>- Maximal Depth: 10. La máxima profundidad del árbol.=,=p>>- Apply pruning: Activado. Si se aplica pruning o no.=,=p>>- Confidence: 0.1. Nivel de confianza usado para el error pesimista del cálculo de pruning.=,=p>>- Apply prepruning: Activado. Si se aplica prepruning o no=,=p>>- Minimal gain: 0.01. La ganancia de un nodo se calcula antes del Split. Se hace el Split si la ganancia es mayor al minimal gain.=,=p>>- Minimal leaf size: 2. Tamaño mínimo de observaciones por hoja.=,=p>>- Minimal size for split: 4. El tamaño de un nodo es el número de ejemplos en él. Solo se hace el Split para obtener nodos con un tamaño mayor al minimal size for Split.=,=p>>- Number of alternatives for pruning: 3. Numero de nodos alternativos testeados para un Split cuando el prepruning previene un Split.=,=br=,=h4>>Weka=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. C4.5.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión.=,=h6>>Parámetros=,=p>>- batchSize: 100. Numero de instancias a procesar si la predicción batch está siendo realiza.=,=p>>- Debug: False. Si es verdadero el clasificador podría poner info adicional como salida en consola.=,=p>>- doNotCheckCapabilities: False. Si es verdadero las capacidades del clasificador no son checkeadas antes de la compilación=,=p>>- initialCount: 0.0. Valor inicial del contador de la clase.=,=p>>- MaxDepth: 10. Máxima profundidad del árbol, con -1 no hay restricción.=,=p>>- MinNum: 2.0. Mínimo peso total para las instancias de una hoja.=,=p>>- minVariancePrep: 0.001. Mínima proporción de la varianza de todos los datos que necesita estar en un nodo para que se haga un Split.=,=p>>- noPruning. False. Si se realiza pruning.=,=p>>- numDecimalPlaces. 2. Número de posiciones decimales para usar en la salida del modelo.=,=p>>- numFolds: 3. Determina el tamaño de los datos usados para pruning.=,=p>>- Seed. 1. La semilla usada para la aleatoriedad de los datos.=,=p>>- spreadIntialCount. False. Distribuir el recuento inicial en todos los valores en lugar de utilizar el recuento por valor.=,=br=,=h4>>Resultados=,=p>>Se obtuvo una performance de un 91.11% en el caso de RapidMiner y un 96% en Weka utilizando los parámetros que se muestran en la sección anterior para cada uno.=,=h6>>RapidMiner=,=img>>UT4-PD5-1.jpg>>100%>>200=,=br=,=br=,=h6>>Weka=,=img>>UT4-PD5-2.jpg>>800>>800=,=br=,=br=,=h4>>Otras herramientas=,=br=,=h5>>Azure Machine Learning Studio=,=p>>Tipo de problema. Clasificación y regresión.=,=p>>Algoritmo base. Se utiliza una versión mejorada de los árboles de decisión con una gradient boosting machine.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden ser numéricas o nominales. Se exige una variable objetivo nominal para clasificación y numérica para de regresión.=,=h6>>Parámetros=,=p>>- Maximum number of leaves per tree. Número máximo de hojas del árbol=,=p>>- Minimum number of samples per leaf node. Mínimo número de ejemplos en un nodo hoja.=,=p>>- Learning rate. Ritmo de aprendizaje.=,=p>>- Total num trees constructed. Número de árboles construidos al entrenar el algoritmo.=,=p>>- Random number seed. Semilla de entrenamiento.=,=p>>- Allow unknown categorical levels. Seleccionado crea un nuevo nivel para cada atributo categórico.=,=br=,=h5>>KNIME=,=p>>Tipo de problema. Clasificación.=,=p>>Algoritmo base. C4.5.=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden solo ser numéricas o nominales. La variable objetivo solo puede ser nominal.=,=h6>>Parámetros=,=p>>- Class column. Selecciona la variable objetivo.=,=p>>- Quality measure. Para seleccionar la medida de calidad para la cual se calcularan los Splits. Las opciones son Gini index y Gain Ratio.=,=p>>- Pruning method. Pruning reduce el tamaño del árbol y evita el overfitting.=,=p>>- Reduced error pruning. Si se checkea se usa un simple método de pruning.=,=p>>- Min number records per node. Mínimo numero de registros requeridos por nodo.=,=p>>- Number records to store for view. Selecciona el número de registros guardados en un tree para la view.=,=p>>-  Average Split point. Al checkearla el valor para Split con atributos numéricos se determina según la media de los valores que separan a las dos particiones.=,=p>>- Number threads. Permite multiprocesamiento.=,=p>>- Skip nominal columns without domain information. Seleccionada las columnas nominales que no información de valores de dominio se saltean.=,=p>>- Force root split column. Seleccionada, el primer split es calculado en la columna elegida sin evaluar ninguna otra para posibles splits.=,=p>>- Binary nominal splits. Al seleccionarla a los atributos nominales se les hacen splits binarios.=,=p>>- Max nominal. Número máximo de valores nominales.=,=p>>- Filter invalid attribute values in child nodes. Habilitando esta opción se hace un post procesamiento del tree y se filtran checkeos inválidos.=,=p>>- No true child strategy: Opciones para cuando el valor de los atributos de un nodo es desconocido.=,=p>>- Missing value strategy. Opciones para los valores faltantes.=,=br=,=h5>>Python Sci kit learn=,=p>>Tipo de problema. Clasificación y regresión=,=p>>Algoritmo base. CART=,=p>>Características requeridas de atributos y label. Las variables de entrada pueden solo ser numéricas. La variable objetivo puede ser nominal o numérica.=,=h6>>Parámetros=,=p>>- Criterio. Función para medir la calidad del Split. Puede ser Gini o entropy=,=p>>- Splitter. Estrategia utilizada para elegir el Split en cada nodo. Las opciones son mejor o random.=,=p>>- Max Depth. Máxima profundidad del árbol.=,=p>>- Min samples. Mínimo de ejemplos requeridos para hacer un Split generando un nodo interno.=,=p>>- Min samples leaf. Mínimo de ejemplos requeridos para hacer un Split generando un nodo hoja.=,=p>>- Min weight fraction. Fracción de peso mínimo del total de peso requerido en un nodo hoja.=,=p>>- Max features. Máximo número de variables de entrada considerado para hacer el mejor Split.=,=p>>- Random_state. Controla la aleatoriedad del estimador.=,=p>>- Max_leaf_node. Máximo número de nodos hoja.=,=p>>- Min_impurity_decrease: Se le hará un Split a un nodo si el Split da un decenso de la impureza mayor o igual a este valor.=,=p>>- Class_weight: Pesos asociados a las clases.=,=p>>- Ccp_alpha: Parámetro de complehidad usado para el pruning de mínimo costo-complejidad."
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"SVM lineal con descenso de sub gradiente",        
        "descripcion":"En una hoja de cálculo analizaremos el algoritmo de Support Vector Machines lineal con descenso de sub gradiente y lo utilizaremos para realizar predicciones.",
        "contenido":"p>>Support Vector machines es uno de los algoritmos de machine learning más populares. Este modelo se basa en la definición de hiperplanos n dimensionales, siendo n el número de variables de entrada, para dividir los puntos ejemplos en dos clases aceptando solo variables de predicción binomiales de forma tal de que se maximice el margen de la separación entre clases. De todas maneras se puede permitir dejar que algunos puntos violen la línea de separación a partir de un coeficiente C se determina la magnitud de este fenómeno.=,=p>>La división de las clases en SVM depende de los llamados kernels. Estos determinan el tipo de hiperplano con el que se dividirá el espacio n dimensinal. Algunos ejemplos son lineal, polinomial, radial, etc.=,=p>>Proximamente estudiaremos un caso de SVM dividiendo un espacio bidimensional al tener solamente las variables de entrada x1 y x2, utilizando un kernel lineal formando así nada más ni nada menos que una recta que se deberá definir.  =,=p>>- Se graficaron los datos en dos series para Y=1 y para Y=-1.=,=img>>UT4-PD3-1.jpg>>750>>250=,=br=,=br=,=p>>- Para hallar los coeficientes B1 y B2 del modelo SVM lineal B0 + B1 x X1 + B2 x X2 = 0 se utilizó el método del descenso de sub-gradiente. El coeficiente B0 fue descartado por lo cual la recta resultante pasa por el origen.=,=p>>Para aplicar este algoritmo se comienza con los coeficientes B1 y B2 en 0 y posteriormente se calcula un primer valor de salida con la fórmula dispuesta a continuación.=,=img>>UT4-PD3-2.jpg>>450>>50=,=p>>Si el valor de salida es mayor a 1 el patrón de entrenamiento no es un vector de soporte y por lo tanto se aplica la siguiente función para b1 y b2=,=img>>UT4-PD3-3.jpg>>250>>75=,=p>>En caso contrario se aplica la siguiente fórmula sobre los valores de los vectores utilizando en este caso lambda=0.45 y donde t es la iteración actual=,=img>>UT4-PD3-4.jpg>>550>>75=,=p>>- Este procedimiento fue realizado por 16 épocas en las que para cada una se itera sobre todo el dataset.=,=p>>- Se realizó una gráfica de la exactitud en función de las épocas obteniendo el siguiente resultado.=,=img>>UT4-PD3-5.jpg>>750>>350=,=br=,=br=,=p>>- Tras todas las iteraciones los coeficientes obtenidos fueron B1=0.55 Y B2=-0.72 obteniendo el plano 0.55xX1 -0.72xX2 = 0. Este se utilizó para calcular las predicciones con los datos de entrenamiento y se obtuvo un 100% de exactitud.=,=img>>aa7.jpg>>50%>>200=,=br=,=br=,=a>>Descargar planilla de trabajo>>Algoritmos no lineales PD3.xlsx>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"SVM no lineal en RapidMiner",
        "descripcion":"En este ejercicio se analizará el componente SVM de RapidMiner y se lo utilizará para resolver un problema no separable linealmente.",
        "contenido":"p>>El operador SVM utiliza la implementación de Java de support vector machine mySVM de Stefan Rueping pudiendo ser usado para regresión y clasificación. Este fue empleado para resolver un problema no separable linealmente primero con los parámetros por defecto y luego se lo resolvió modificando el kernel a polinomial.=,=p>> A continuación se describen y muestran los valores de los parámetros utilizados del operador y luego los resultados de performance obtenidos para ambos casos. =,=h5>>Parámetros=,=p>>- Kernel type: Tipo de función kernel a utilizar en el algoritmo. Valor inicial dot, final polynomial.=,=p>>- Kernel cache: Fija el tamaño de cache para las evaluaciones kernel. Valor: inicial 200, final 200.=,=p>>- C: Es una constante de complejidad que fija la tolerancia a clasificación errónea, cuando más alta más suaves son los límites y cuanto baja estos son más duros. Si es demasiado alta puede dar overfitting y si es muy baja puede dar over generalization. Valor: inicial 0.0, final 0.0.=,=p>>- Convergence épsilon: Especifica la precisión de las KKT conditions. Valor: inicial 0.01, final 0.01.=,=p>>- Max iterations: Número máximo de iteraciones. Valor: inicial 100000, final 100000.=,=p>>- Scale: Si está activado los valores se escalan. Valor: inicial activado, final activado.=,=p>>- Lpos: Factor para constante de complejidad del SVM caso positivos. Valor: inicial 1.0, final 1.0.=,=p>>- Lneg: Factor para constante de complejidad del SVM caso negativos. Valor: inicial 1.0, final 1.0.=,=p>>- Épsilon: Constante de insensibilidad. Valor: inicial 0.0, final 0.0.=,=p>>- Épsilon plus: Parámetro parte de la función de pérdida. Valor: inicial 0.0, final 0.0.=,=p>>- Épsilon minus: Parámetro parte de la función de pérdida. Valor: inicial 0.0, final 0.0.=,=p>>- Balance cost: Adapta Cpos y Cneg al tamaño relative de las clases. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss pos: Usa pérdida cuadrática para desviación positiva. Valor: inicial desactivado, final desactivado.=,=p>>- Quadratic loss neg: Usa pérdida cuadrática para desviación negativa. Valor: inicial desactivado, final desactivado.=,=br=,=h5>>Resultados=,=br=,=h6>>Caso inicial=,=img>>UT4-PD4-1.jpg>>900>>150=,=br=,=br=,=h6>>Caso final=,=img>>UT4-PD4-2.jpg>>900>>150=,=br=,=br=,=a>>Descargar proceso de RapidMiner>>UT4-TA7.rmp>>d"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"Naive Bayes en planilla electrónica",
        "descripcion":"Implementación de un modelo de Naive Bayes en una planilla electrónica.",
        "contenido":"p>>El modelo Naive Bayes para clasificación en problemas binarios o multiclase se caracteriza por asumir que los atributos son independientes entre sí (lo cual rara vez acontece) y en basarse en el teorema de Bayes.=,=h6>>Teorema de Bayes=,=p>>Este es utilizado para calcular probabilidad de una hipótesis dado un suceso de la siguiente manera.=,=img>>UT4-PD6-1.jpg>>350>>100=,=p>>Siendo=,=p>>- P(h|d) la probabilidad de la hipótesis h dado el susceso d (conocida como la probabilidad a posteriori).=,=p>>- P(d|h) la probabilidad del suceso d dado que la hipótesis h sea cierta.=,=p>>- P(h) la probabilidad de que la hipótesis h sea cierta (conocida como la probabilidad a priori).=,=p>>- P(d) la probabilidad de que suceda el suceso d más allá de que se cumpla h o no.=,=p>>En Naive Bayesian se calcula las probabilidades a priori para cada clase, las probabilidades para cada combinación entre un valor de un atributo y una clase, se emplea el Teorema de Bayes para obtener la probabilidad a posteriori y finalmente se clasifica según la clase que produjo el valor más elevado.=,=p>>Ahora mostraremos el comportamiento del modelo Naive Bates empleándolo para el dataset Jugar Tenis prediciendo si se juega o no al tenis en condiciones metereológicas particulares definidas por los atributos de los datos.=,=h5>>Dataset=,=img>>UT4-PD6-2.jpg>>450>>300=,=br=,=br=,=p>>Se calcularon las siguientes probabilidades para generar el modelo tal y como se describió anteriormente pero para este caso particular.=,=img>>UT4-PD6-3.jpg>>650>>450=,=br=,=br=,=p>>Posteriormente se utilizó el modelo generado para realizar las predicciones que se encuentran a continuación.=,=img>>UT4-PD6-4.jpg>>650>>100"
    },
    {
        "unidad":"Algoritmos no lineales",
        "titulo":"KNN en hoja de cálculo y RapidMiner",
        "descripcion":"Empleo de KNN para realizar predicciones en una hoja de cálculo y RapidMiner.",
        "contenido":"p>>El algoritmo para clasificación y regresión K-Nearest Neighbors se diferencia de los demás en que no es un modelo aprendido sino que se realizan predicciones a partir del dataset de entrenamiento directamente. El procedimiento consiste en que para cada punto a predecir se busca entre las instancias más similares en base a medidas como distancia euclideana, Hamming, Manhattan, Jaccard, Minkowski, etc.=,=p>>Para utilizar KNN para predecir basta con elegir un valor de k y una medida de distancia que serán utilizados para hallar los k puntos más cercanos de entre los que están en los datos de entrenamiento respecto al ejemplo que se quiere predecir. A partir de allí para clasificación se compara las clases de estos k puntos y la más repetida será la designada como predicción y para regresión se asigna el promedian de la salida de los k valores más cercanos. Cabe destacacr que es importante normalizar si las escalas de los datos se encuentran en diferentes magnitudes para que esto no afecte la calidad de las predicciones de nuestro modelo.=,=p>>A continiuación aplicaremos el algoritmo KNN para un sencillo ejemplo de clasificación con distintos valores de k y distancia euclideana en una hoja de cálculo.=,=p>>- Graficamos de los datos en dos series para Y=0 y para Y=1=,=img>>UT4-PD8-1.jpg>>75%>>200=,=br=,=br=,=p>>- Agregamos el punto con (x1, x2)= (8.093607318, 3.365731514) para clasificarlo usando diferentes valores de k.=,=img>>aa5.jpg>>100%>>150=,=p>>K=3: Puntos más cercanos: (x1, x2) = (7.423436942, 4.696522875), (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Predicción Y = 1.=,=p>>K=2: Puntos más cercanos: (x1, x2) = (9.172168622, 2.511101045), (x1, x2) = (7.792783481, 3.424088941). Predicción Y = 1.=,=p>>K=1: Puntos más cercanos: (x1, x2) = (7.792783481, 3.424088941). Predicción Y = 1.=,=p>>- Añadimos el punto con (x1, x2)= (5, 2.5) para clasificarlo usando diferentes valores de k.=,=img>>aa6.jpg>>100%>>150=,=p>>K=3: Puntos más cercanos: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (3.110073483, 1.781539638), (x1, x2) = (5.745051997, 3.533989803). Predicción Y = 0.=,=p>>K=2: Puntos más cercanos: (x1, x2) = (3.393533211, 2.331273381), (x1, x2) = (5.745051997, 3.533989803). Predicción Y = N/A.=,=p>>K=1: Puntos más cercanos: (x1, x2) = (5.745051997, 3.533989803). Predicción Y = 1.=,=a>>Descargar planilla electrónica>>KVecinosMasCercanos.xlsx>>d=,=br=,=br=,=p>>Ahora estudiaremos al algoritmo KNN en RapidMiner para el dataset de clasificación Iris observando cómo es el operador y cuáles son sus parámetros en esta herramienta.=,=h6>>Modelo=,=img>>UT4-PD8-2.jpg>>700>>300=,=br=,=br=,=h6>>Gráfica=,=img>>UT4-PD8-3.jpg>>100%>>450=,=br=,=br=,=h6>>Consideraciones=,=p>>Se puede observar que los datos correspondientes a la clase iris-setosa se encuentran bastante separados de los de las otras dos. En el caso de estos últimos iris-versicolor e iris-virginica, si bien tienen un poco de solapamiento también se pueden ver áreas diferenciables.=,=h6>>Preparación de datos=,=p>>Es conveniente estandarizar los datos con un range transformation entre 0 y 1.=,=h6>>Operador KNN en RapidMiner=,=p>>Voto ponderado: Si se activa este parámetro los valores de distancia entre los ejemplos se tienen en cuenta para la predicción. Es útil para ponderar las contribuciones de los vecinos de forma tal que los vecinos más cercanos contribuyan más que los más lejanos.=,=p>>Tipos de medición: Este parámetro se utiliza para seleccionar el tipo de medida que se utilizará para encontrar los vecinos más cercanos. Las opciones son las siguientes.=,=p>>- MixedMeasures: Se utiliza para calcular distancias en el caso de atributos tanto nominales como numéricos.=,=p>>- NominalMeasure: Se usa para el caso de solo atributos nominales.=,=p>>- NumericalMeasure: Se usa para el caso de solo atributos numéricos.=,=p>>- BregmannDivergences: Se selecciona para emplear diveregencias de Bregmann como tipos de medidas de cercanía.=,=p>>Funciones de medición:=,=p>>- EuclideanDistance. Dist = Sqrt ( Sum_(j=1) [y(1,j)-y(2,j)]^2).=,=p>>- CanberraDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)| / (|y(1,j)|+|y(2,j)|).=,=p>>- ChebychevDistance. Dist = max_(j=1) (|y(1,j)-y(2,j)|).=,=p>>- CosineSimilarity. Mide el coseno del ángulo entre los vectores de atributos de los dos ejemplos.=,=p>>- DiceSimilarity. La DiceSimilarity para atributos numéricos se calcula como 2 * Y1Y2 / (Y1 + Y2). Y1Y2 = Suma sobre el producto de valores = Suma_ (j = 1) y (1, j) * y (2, j). Y1 = Suma de los valores del primer Ejemplo = Suma_ (j = 1) y (1, j) Y2 = Suma de los valores del segundo Ejemplo = Suma_ (j = 1) y (2, j).=,=p>>- DynamicTimeWarpingDistance. Se calcula la distancia en una ruta de deformación óptima desde el vector atributo del primer ejemplo al segundo ejemplo.=,=p>>- InnerProductSimilarity. Dist = -Similarity = -Sum_(j=1) y(1,j)*y(2,j).=,=p>>- JaccardSimilarity. Dist = Y1Y2/(Y1+Y2-Y1Y2).=,=p>>- KernerlEuclideanDistance. La distancia se calcula mediante la distancia euclidiana de los dos Ejemplos, en un espacio transformado. La transformación está definida por el kernel y los parámetros correspondientes elegidos.=,=p>>- ManhattanDistance. Dist = Sum_(j=1) |y(1,j)-y(2,j)|.=,=p>>- MaxProductSimilarity. Dist = -Similarity = -max_(j=1) (y(1,j)*y(2,j)).=,=p>>- OverlapSimilarity. Dist = minY1Y2 / min (Y1, Y2).=,=h6>>Resultados=,=p>>K=3, euclidean distance:=,=img>>UT4-PD8-4.jpg>>100%>>150=,=p>>K=2, euclidean distance:=,=img>>UT4-PD8-5.jpg>>800>>150=,=p>>K=3, manhattan distance:=,=img>>UT4-PD8-7.jpg>>800>>150=,=p>>K=2, manhattan distance:=,=img>>UT4-PD8-6.jpg>>800>>150=,=br=,=br=,=h6>>Conclusiones=,=p>>Con distancia euclideana, al variar el valor de k se obtuvieron diferentes valores de performance, pero esto no sucedió en los ejemplos realizados para la distancia manhattan.=,=a>>Descargar proceso de RapidMiner>>UT4-TA8.rmp>>d"
    },
    {
        "unidad":"No supervisado",
        "titulo":"DBSCAN en Wine",
        "descripcion":"Aplicamos el algoritmo de clustering DBSCAN para encontrar agrupaciones en el dataet Wine",
        "contenido":"p>>El algoritmo no supervisado de clustering DBSCAN se basa en las densidades de ejemplos para definir agrupaciones. Se fundamenta en descubrir áreas de alta densidad de datos rodeadas de otras de baja densidad según los parámetros de radio épsilon (ε) y el mínimo de puntos que puede haber en clúster. A partir de estos valores claifica a los puntos en 3 tipos, puntos de núncleo los cuales están en una región de alta densidad de al menos un punto, los de borde que están a un a distancie ε de algún punto y los de ruido son aquellos que caen fuera de las dos clases anteriores. De esta manera, el usuario puede modificar ambos parámetros según su conveniencia pero no definirá la cantidad de clusteres que el modelo hallará como sucede con kmeans.=,=p>>Para ver un caso de empleo de DBSCAN nos ayudaremos de RapidMiner para aplicarlo a los atributos petal-width y petal height del dataset Iris. Para estos casos se mantendrá constante el mínimo de puntos con 5 y se irá modificando el valor de ε en distintas iteraciones para luego analizar como varían los resultados obtenidos.=,=h6>>Proceso RapidMiner=,=img>>dbscan-m.jpg>>100%>>250=,=a>>Descargar proceso>>ut5 ta1 ej2.rmp>>d=,=br=,=br=,=h6>>Con ε=1.0=,=img>>dbscan-0.jpg>>100%>>550=,=br=,=br=,=h6>>Con ε=0.3=,=img>>dbscan-1.jpg>>100%>>550=,=br=,=br=,=h6>>Con ε=0.2=,=img>>dbscan-2.jpg>>100%>>550=,=br=,=br=,=p>>Se observa como el cluster 0 corresponde a puntos de ruido. Al ir disminuyendo el radio cada vez aparecen más puntos de ruido y nuevos clusteres, con ε=1.0 y ε=0.3 hay solo 2 pero a partir de ε=0.2 se suma uno nuevo y si se continúa realizando este descenso en el parámetro cada vez aparecerán más."
    },
    {
        "unidad":"No supervisado",
        "titulo":"PCA para dataset Cereals",
        "descripcion":"Estudio del algoritmo PCA utilizando los datos de Cereals",
        "contenido":"p>>Principal Component Analyisis (PCA) es un algoritmo que reduce los atributos en algunos atributos principales que contienen la mayor variabilidad transformando variables existentes en componentes principales o nuevas variables que no tienen correlación entre sí, de forma acumulada explican la mayor cantidad de varianza entre los datos y son revertibles a las variables originales por factores de peso. Otras de sus características son que solamente se puede aplicar para variables numéricas y que se pierde el significado de las variables del problema con los componentes principales.=,=p>>A continuación realizaremos un ejemplo en RapidMiner sobre el dataset cereals que cuenta con 77 ejemplos cuyas variables son las siguientes.=,=p>>- Name (categórica): Nombre del cereal=,=p>>- mfr (categórica): Manufacturero del cereal (A = American Home Food Products; G = General Mills; K = Kelloggs; N = Nabisco; P = Post; Q = Quaker Oats; R = Ralston Purina)=,=p>>- type (categórico): frío o caliente.=,=p>>- protein (numérico): proteínas del cereal.=,=p>>- fat (numérico): grasas del cereal.=,=p>>- sodium (numérico): miligramos de sodio.=,=p>>- fiber (numérico): gramos de fibra dietética.=,=p>>- carbo (numérico): gramos de carbohidratos complejos.=,=p>>- sugars (numérico): gramos de azúcar.=,=p>>- potass (numérico): miligramos de potasio.=,=p>>- vitamines (numérico): viraminas y minerales indicando el porcentaje típico de las recomendaciones del FDA.=,=p>>- shelf (numérico): Estante (1,2,3,4).=,=p>>- Weight (numérico): Peso en onzas por porción.=,=p>>- Cups (numérico): Cantidad de tazas por porción.=,=p>>- Rating (numérico): Puntaje para los cereales.=,=a>>Descargar dataset>>Cereals.xls>>d=,=br=,=br=,=p>>Se importó el modelo y para poder aplicar el algoritmo se tuvieron que quitar los atributos categóricos Name, mfr y type y reemplazar los 4 valores faltantes por el promedio del atributo correspondiente debido a que PCA ni variables categóricas ni missing values.=,=p>>Luego del pre procesamiento anterior se colocó el operador PCA con 0.95 de variance threshold que refiere a la cantidad de varianza que buscará explicar por las variables que seleccione en los componentes principales generados y keep reduction en dimensionality reduction por lo que los atributos con varianza mayor al umbral se eliminan del dataset.=,=h6>>Esquema RapidMiner =,=img>>pca-0.jpg>>80%>>200=,=br=,=a>>Descargar modelo>>pca.rmp>>d=,=br=,=br=,=p>>Los resultados obtenidos son los siguientes.=,=img>>pca-1.jpg>>100%>>400=,=br=,=br=,=img>>pca-2.jpg>>100%>>400=,=br=,=br=,=p>>Observando la primer imagen se puede concluir que se supera a la varianza acumulada establecida de en variance threshold de 0.95 contando con tan solo con los primeros componentes al llegar 0.966. En la segunda representación se ve el peso que tiene cada atributo en cada componente lo cual nos permite observar que rating, calories, potasium y vitamins son los atributos que más aportan en los 3 primeros componentes principales y por lo tanto los más importantes."
    },
    {
        "unidad":"Caso",
        "titulo":"Ensambles",
        "descripcion":"",
        "contenido":"p>>Los algoritmos de ensamble se caracterizan por generar múltiples modelos independientes de machine learning y combinar sus predicciones para obtener mejores performances.=,=h6>>Bagging=,=p>>Es un método de ensamble que se basa en crear muchas sub muestras tomadas con reemplazo, entrenar distintos modelos en ellos de alta varianza (típicamente CARTs) y dado un nuevo dataset calcular el promedio para cada predicción en el caso de problemas de regresión o mediante voto para clasificación. Para el caso de los árboles de decisión, al usar bagging nos preocupamos menos porque uno en particular haga overfitting de los datos de forma tal de que se puede permitir que los árboles indivualmente crezcan profundamente y sin poda. =,=h6>>Random Forest=,=p>>Es uno de los algoritmo más potentes de machine learing. Se basa en árboles de decisión que utiliza bagging pero modificando a los árboles de decisión en que en lugar de buscar los split points de forma óptima se limita este procedimiento a aprender de una muestra random de los predictores con el fin de que los árboles generados sean más independientes entre sí.=,=h6>>Boosting=,=p>>Es otra técnica de ensamble que busca generar un modelo fuerte a partir de varios clasificadores débiles. Esto se hace a partir de crear un modelo a partir de los datos de entrenamiento, luego creando un segundo modelo que mejore los errores del primer modelo y así hasta que la predicción es perfecta (pudiendo causar overfitting) o hasta que un máximo de modelos son añadidos.=,=h6>>Ada Boost=,=p>>Es un algoritmo basado en boosting que es muy útil para clasificación binaria y el modelo débil que utiliza típicamente son árboles de decisión de un nivel. Estos se entrenan poniendo pesos a cada ejemplo del dataset de entrenamiento, añadiendo peso si los modelos débiles clasifican equivocadamente para darle más importancia en la siguiente iteración a esa instancia, y calculando una ponderación para el modelo débil que luego que luego se utiliza en cada predicción que este modelo haga. Lo cual se repite hasta un criterio de parada o hasta que no se pueda seguir mejorando la performance. Las predicciones se realizan calculando el promedio de las predicciones ponderadas de los modelos débiles clasificando según se obtiene un valor positivo o negativo.=,=p>>A continuación aplicaremos Random Forest y Ada Boost con árboles de decisión aplicado al sencillo dataset de clasificación binaria Banking en RapidMiner. Mediremos sus preformances y compararemos sus curvas ROC.=,=h5>>Dataset=,=p>>El conjunto de datos banking marketing cuenta con 17 atributos correspondientes a características de clientes para predecir si un cliente invierte en un depósito a plazo fijo según una campaña de marketing telefónico. Entre los datos hay 4521 ejemplos y no se encuentran missing values.=,=p>>Los atributos son las siguientes:=,=p>>- age (numérico): La edad del cliente.=,=p>>- job (categórico): tipo de trabajo entre admin, unknown, unemployed, management, housemaid, entrepreneur, student, blue-collar, self-employed, retired, technician, services.=,=p>>- marital (categórico): Estado civil entre married, divorced (divorciado y viudo se condieran dentro de esta clase) y single.=,=p>>- education (categórico): Nivel educativo entre unknown, secondary, primary y tertiary.=,=p>>- default (categórico): Si tiene el crédito en default entre yes y no.=,=p>>- balance (numérico): promedio de balance anual en euros.=,=p>>- loan (categórico): si el cliente tiene préstamos entre yes y no.=,=p>>- contact (categorico): Medio de contacto entre unknown, telephone y cellular.=,=p>>- day (numérico): Día del último contacto.=,=p>>- month (categórico): Mes del último contacto.=,=p>>- duration (numérico): Duración del último contacto en segundos.=,=p>>- campaign (numérico): número de conactos realizados con el cliente esta campaña.=,=p>>- pdays (numérico): número de días desde que el cliente fue llamado desde una campaña previa en la que -1 significa que nunca fue contactado.=,=p>>- previous (numérico): número de contactos realizados antes de esta campaña con el cliente.=,=p>>- poutcome (categórico): Resultado de la última campaña de marketing entre unknown, other, failure y success.=,=p>>- y (categórico): El cliente se sucribió al plazo fijo entre yes y no.=,=h5>>Operador=,=h6>>Descripciones=,=img>>1.jpg>>75%>>500=,=br=,=br=,=img>>2.jpg>>75%>>500=,=br=,=br=,=h6>>Parámetros=,=p>>-Random Forest: Cuenta con number of trees que determina la cantidad de árboles generados, voting strategy utilizado para elegir la estrategia de predicción entre los árboles del modelo y las configuraciones propias del componente de árbol de decisión para seleccionar cualidades de aquellos que lo componen.=,=p>>-AdaBoost: Iterations utilizado para fijar el número máximo de iteraciones del algoritmo.=,=h5>>Modelo=,=img>>3.jpg>>75%>>600=,=br=,=br=,=h5>>Resultados=,=p>>Performance Random Forest.=,=img>>4.jpg>>90%>>175=,=br=,=br=,=p>>Performance AdaBoost=,=img>>5.jpg>>90%>>175=,=br=,=br=,=p>>Comparación entre curvas ROC=,=img>>6.jpg>>100%>>600=br=,=br=,=br=,=h4>>Conclusiones=,=p>>Observamos que para este caso particular tanto los valores de accuracy, precisión de clases y recall de categorías dieron practicamente iguales con leves ventajas para Random Forest aunque ambos algoritmos dieron muy buenos resultados en todos exceptuando la precisión y sobre todo el recall para la clase yes fallando los dos en lo mismo.=,=p>>En donde si se pueden ver más diferencias es en la comparación de las curvas ROC. En esta se puede observar que la curva de AdaBoost tiene un mucho mayor crecimiento de verdaderos positivos respecto a falsos positivos que Random Forest con lo cual resulta ser un mejor modelo."
    },
    {
        "unidad":"Caso",
        "titulo":"Enfermedad Cardíaca",
        "descripcion":"",
        "contenido":"img>>cardiaca-0.jpg>>100%>>550=,=br=,=hr=,=br=,=h2>>Contexto=,=hr=,=p>>Las enfermedades cardiovasculares son la principal causa de muerte en el mundo según la Organización Mundial de la Salud (OMS) llevándose un estimado de 17.9 millones de vidas cada año lo cual supone un 32% de todas las muertes anuales alrededor del planeta. En el Uruguay según datos oficiales del Ministerio de Salud Pública (MSP), para cada año del quinquenio (2015-2019)  anterior a la pandemia del SARS-CoV2, conocido coloquialmente como coronavirus, estas enfermedades motivaron entre un 25% y un 27% de la totalidad de decesos conforme a lo que pudo identificar nuestro sistema de salud.=,=p>>Entre las principales causas de las enfermedades coronarias identificadas por el Centro de Control y Prevención de Enfermedades de los Estados Unidos (CDC) se encuentran alta presión arterial, colesterol elevado, fumar, diabetes, sobrepeso u obesidad, dietas poco saludables, poca actividad física y excesivo uso consumo de alcohol.=,=p>>Por otra parte, según datos del CDC se observa que esta enfermedad se da con mayor frecuencia para los hombres que para las mujeres con un 13.6% frente a un 8.4% de las respectivas poblaciones habiendo reportado padecimientos coronarios y que también se da con más ocurrencia al envejecer llegando hasta la franja a partir de los 65 años en la que un 17% de esa población reportó tenerla.=,=p>>La mayor sugerencia realizada por el sector de la salud al respecto de los padecimientos del corazón es tratar de prevenirlos lo máximo posible teniendo una vida saludable. De todas maneras, existen tratamientos como prescripción de cambios de rutina, medicación y hasta procedimientos quirúrgicos dependiendo de los diversos factores que estén causando la enfermedad, pero para todos ellos es fundamental tener un diagnóstico acertado y a tiempo con un médico.=,=p>>=,=br=,=h2>>Entendimiento del negocio=,=hr=,=p>>En este caso de estudio se desarrollarán todos los pasos CRISP-DM para la problemática supervisada y de clasificación acerca de la predicción de enfermedades cardíacas con fines académicos, sin que exista inicialmente la finalidad de poner en producción el resultado conseguido. A continuación se pasará por las etapas de entendimiento de datos, preparación, modelado y evaluación para la realización de predicciones acerca de si un paciente cuenta o no con una enfermedad del corazón a partir de algunas de sus características empleando para la herramienta RapidMiner.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>Para el trabajo se cuenta con cuatro bases de datos vinculadas al diagnóstico de enfermedades del corazón y cualidades de los pacientes provenientes respectivamente de V.A Medical Center (California, Estados Unidos) autor: Robert Detrano M.D. Ph.D,  Cleveland Clinic Foundation (Ohio, Estados Unidos) autor: Robert Detrano M.D. Ph.D, University Hospital (Zurich, Suiza) autor: William Steinbrunn M.D y Hungarian Institute of Cardiology (Budapest, Hungría) autor: Andras Janosi M.D.=,=p>> Los datos fueron analizados en la herramienta RapidMiner y para ello hubo que realizarles un pre procesamiento en Python armando documentos en formato .csv con un formato con una cabecera conteniendo los nombres de los atributos y los registros de un paciente por fila a partir de cada uno de los cuatro archivos originales individualmente en los que los datos están separados por espacios y en varios renglones para cada individuo del cual se cuenta con información.=,=code>>.lines = open(\"switzerland.data\", \"r\", errors='ignore')=,=code>>.writer = open(\"cleanedSwitzerland.data\", 'w', encoding=\"cp437\", errors='ignore')=,=br=,=code>>.header = (\"id,ccf,age,sex,painloc,painexer,relrest,pncaden,cp,trestbps,htn,chol,smoke,cigs,years,\"=,=br=,=code>>.\u00a0\u00a0        \"fbs,dm,famhist,restecg,ekgmo,ekgday,ekgyr,dig,prop,nitr,pro,diuretic,proto,thaldur,\"=,=br=,=code>>.\u00a0\u00a0        \"thaltime,met,thalach,thalrest,tpeakbps,tpeakbpd,dummy,trestbpd,exang,xhypo,oldpeak,\"=,=br=,=code>>.\u00a0\u00a0        \"slope,rldv5,rldv5e,ca,restckm,exerckm,restef,restwm,exeref,exerwm,thal,thalsev,thalpul,\"=,=br=,=code>>.\u00a0\u00a0        \"earlobe,cmo,cday,cyr,num,lmt,ladprox,laddist,diag,cxmain,ramus,om1,om2,rcaprox,rcadist,\"=,=br=,=code>>.\u00a0\u00a0        \"lvx1,lvx2,lvx3,lvx4,lvf,cathef,junk\")=,=br=,=code>>.writer.write(header+'\n')=,=br=,=code>>.info = []=,=br=,=code>>.print(lines)=,=br=,=code>>.for line in lines:=,=br=,=code>>.  lineList = line[:-1].split(\" \")=,=br=,=code>>.  if'name' not in lineList:=,=br=,=code>>.\u00a0    info.extend(lineList)=,=br=,=code>>.  else:=,=br=,=code>>.\u00a0    info.extend(lineList[:-1])=,=br=,=code>>.\u00a0    infoWithMV = [\"?\" if x in [\"-9\",\"-9.\",\"-9.0\"]  else x for x in info]=,=br=,=code>>.\u00a0    infoToWrite = \",\".join(infoWithMV)=,=br=,=code>>.\u00a0    writer.write(infoToWrite+'\n')=,=br=,=code>>.\u00a0    info = []=,=br=,=code>>.writer.close()=,=br=,=code>>.lines.close()=,=br=,=br=,=h6>>Descargas:=,=a>>Descripción de datos>>cardiac description.txt>>d=,=br=,=a>>Datos>>cardiac data.zip>>d=,=br=,=br=,=p>>Los datasets cuentan con una totalidad de 920 ejemplos etiquetados para los cuales hay 76 atributos numéricos que especifican diversas características de los pacientes entre los que se encuentra la variable objetivo del problema llamada num la cual representa el diagnóstico de enfermedad cardíaca. Esta última tiene un valor de 0 si no hay presencia de enfermedad en el individuo y valores del 1 al 4 según diferentes tipos de padecimientos del corazón.=,=p>>A pesar de la gran cantidad de atributos 10 de ellos no fueron utilizados, tal y como se expresa en la descripción de los datasets, siendo estos thalsev, thalpul, earlobe, lvx1, lvx2, lvx3, lvx4, lvf, cathef y junk. Por otra parte, hay un gran número de predictores con valores faltantes los cuales vienen represntados con el valor -9. Existen 20 de ellos con más de un 50% de missing values respecto a la totalidad de los registros.=,=br=,=h2>>Preparación de datos=,=hr=,=p>>Lo primero que se realizó fue importar los datos correspondientes a las cuatro bases en la herramienta RapidMiner comprobando que todos se cargaban con los mismos tipos para cada uno de sus atributos. Posteriormente se procedió a juntarlas utilizando el componente Append y obteniendo así el conjunto con todos los datos etiquetados.=,=p>>Se declaró a los registros con valor -9 como missing values y primeramente se descartaron las columnas de id y las que tenían más de 50% de valores faltantes considerando que la mayor parte de estas concierne atributos no utilizados según lo expresado en la descripcion de los datos y a que estas pueden generar dificultades para el aprendizaje de los modelos a crear, los restantes fueron reemplazados por el promedio de los datos correspondientes a cada predictor según el caso.=,=p>>Otros procedimientos realizados fueron la remoción de outliers utilizando los operadores Detect Outliers y Filter Examples para quitar los 10 outliers más lejanos de los 10 vecinos más cercanos según distancia euclideana, el cambio de tipo de la variable objetivo de numérica a polinomial usando Numerical to Polynomial y la estandarización de todos los predictores con el componente Normalize.=,=br=,=h2>>Modelado=,=hr=,=p>>Para los modelos a desarrollar se consideraron aquellos supervisados y de clasificación que soportaran variables polinomiales como Naive Bayes, Árboles de Decisión, K-Nearest Neighbors y los algoritmos de ensamble siendo estos uno de tipo bagged como Random Forest y otro de boosting como Gradient Boosted Trees.=,=p>> De los estudiados fueron descartados K-Nearest Neighbors debido a la alta dimensionalidad de los datos y Árboles de Decisión al incluir Random Forest dado que este último es una evolucion del anterior en ser menos propenso al overfitting al estar compuesto por muchos árboles con predictores seleccionados randomicamente con reemplazo en cada uno brindando así generalización al modelo.=,=p>>Por lo tanto, se eligieron los modelos de Naive Bayes como línea de base, Random Forest y Gradient Boosting Trees, aplicando en todos los casos Cross Validation de 10 folds con la misma random seed local número 1992 y midiendo las performances para cada uno.=,=h6>>Proceso en RapidMiner=,=img>>cardiac-0.jpg>>100%>>550=,=a>>Descargar proceso>>Heart Disease.rmp>>d=,=br=,=br=,=h2>>Resultados=,=hr=,=p>>Para evaluar los modelos y medir su rendimiento se estudiaron las matrices de confusión obteniendo las siguientes tablas.=,=h6>>Naive Bayes=,=img>>cardiac-1.jpg>>100%>>250=,=br=,=br=,=h6>>Random Forest=,=img>>cardiac-2.jpg>>100%>>250=,=p>>Params: num trees = 1000, criterion = gain_ratio, min leaf size = 2, min size for split = 4 y max depth = 10.=,=h6>>Gradient Boosting Trees=,=img>>cardiac-3.jpg>>100%>>250=,=p>>Params: num trees = 50, max depth = 5, min rows = 10, min split imp. = 1.0E-5, num bins = 20, lear. rate = 0.01. =,=br=,=h2>>Conclusiones=,=hr=,=p>>Se puede observar que los que dan mejores resultados de accuracy son los de ensambles con Gradient Boosting Trees en la delantera y Random Forest muy cerca. Igualmente, si bien la performance global siempre es muy importante, para este caso de diagnóstico de enfermedades podría llegarse a considerar hasta de mayor relevancia la presición para la clase 0 que representa el pocentaje de acierto en la predicción de ausencia de enfermedad dado que no se pueden permitir altos valores de falsos positivos en problemas tan sensibles como la salud del corazón en la que si ocurren pueden llegar hasta ser terminales para los pacientes. En cuanto a esto se puede destacar que Naive Bayes tiene la presición más alta para la clase 0 aunque también es el que tiene menos recall para esta identificando la mayor cantidad de personas que no tienen enfermedades cardíacas como que si las tuvieran de entre los modelos realizados.=,=p>>Para decidir entre que modelo es mejor entre Naive Bayes y su notable predicción para la clase 0 y Gradient Boosting Machines y su increíble acurracy prediciendo mejor para todas las demás clases es necesario conocer que tan profunda es la diferenciar entre enfermedades de corazón tipo 4, 3, 2 y 1. Si la diferencia es grande en cuanto al tratamiento requerido y las posibilidades del paciente entonces el mejor algoritmo es Gradient Boosting Machines, pero si no es así entre estos modelos yo eligiría Naive Bayes para asegurarme de tener la menor cantidad de falsos negativos de la enfermedad. Lamentablemente no se dispone de esta información en el dataset y en un caso real habría que consultar equipos médicos para tomar la decisión adecuada."
    },
    {
        "unidad":"Caso",
        "titulo":"Ames Housing",
        "descripcion":"",
        "contenido":"img>>ames-0.jpg>>100%>>500=,=p>>Ames Iowa, Estados Unidos=,=hr=,=br=,=h2>>Contexto=,=hr=,=p>>La industria de bienes raíces o real estate es una de las que genera más actividad y crecimiento en el mundo. En los Estados Unidos según datos anualizados del Bureau of Economic Analyisis (BEA) para el segundo cuarto de 2021 el sector vinculado a la vivienda es una de las industrias que genera mayor actividad económica en dicho país con un PIB de 2.908 trillones de dólares. Conforme datos también anualizados del FMI de octubre 2021, esta cifra supera el PIB de países como España, Italia, Brasil, Australia, Canadá, Rusia, Corea del Sur y equivale a 48.38 veces el de Uruguay.=,=p>>El valor de compra de una propiedad depende de muchos factores entre los cuales los expertos destacan los siguientes.=,=p>>Localización: Uno de los principales está vinculado a la famosa frase de la industria de bienes raíces “location, location, location” haciendo referencia a que gran parte del valor de un inmueble depende la calidad de oportunidades y servicios que se puedan obtener viviendo en ese lugar como por ejemplo posibilidades de obtener un buen empleo, seguridad, proximidad a escuelas de calidad, etc.=,=p>>Espacio: Es un factor fundamental para determinar su valor haciendo especial énfasis en el espacio utilizable que es aquél en el que no se cuenta el área correspondiente a áticos, sótanos y los lugares que las personas rara vez utilizan en su día a día. Muchas personas eligen vivir en suburbios en los que pueden disponer de interiores, jardines o fondos más amplios que vivir en el centro de la ciudad en una inmueble más pequeño por más que esto le quite la posibilidad de acceder a alguna oportunidad o servicio con la misma facilidad. Esto se puede ver en gran extensión en países como Estados Unidos o Canadá, pero también en Uruguay con Ciudad de la Costa.=,=p>>Formato: Los compradores están dispuestos a pagar por conseguir una vivienda que sea acorde a sus necesidades actuales o a sus perspectivas de futuro como sería el caso de una familia que tiene expectativas de tener más hijos y busca una casa lo suficientemente grande para ello. Dentro de estas características se destacan algunas como el número de habitaciones disponibles, la cantidad de baños y si el inmueble tiene garage o no. Por otra parte, el formato en si de la vivienda está relacionado fuertemente a su precio, no es lo mismo una casa que un apartamento con todas las variantes que estos pueden tener.=,=p>>Estado: El estado de una propiedad es un factor fundamental para su precio siendo esta es la razón por la cual es conveniente económicamente reacondicionar una propiedad antes de venderla. Por otra parte, a mayor antigüedad las construcciones son más propensas a tener defectos y por lo tanto se valora la modernidad.=,=br=,=h2>>Entendimiento del negocio=,=hr=,=p>>Este caso de estudio tiene como finalidad poder desarrollar el modelo CRISP-DM sobre un problema con fines académicos y no teniendo como objetivo un deployment o que forme parte de un producto más grande.=,=p>>Se realizará preparación de datos, modelado y evaluación para predecir el valor de viviendas a partir de características de las propiedades o de sus entornos utilizando Python y librerías como SciKitLearn, Pandas, Numpy, Seaborn, entre otras. El código completo al que se hace referencia en este trabajo se encuentra disponible en la última sección de Anexo.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>Para este proyecto se cuenta con datos de venta de propiedades de la localidad de 66.000 habitantes llamada Ames del estado de Iowa Estados Unidos entre los años 2006 y 2010 obtenidos del Ames City Assessor´s Office y preprocesados por Dean De Cock PhD en Probabilidad de la Iowa State University. Los registros se encuentran en dos datasets, uno de training y uno de test.=,=h6>>Descargas:=,=a>>Descripción de datos>>data_description.txt>>d=,=br=,=a>>train.csv>>train.csv>>d=,=br=,=a>>test.csv>>test.csv>>d=,=br=,=br=,=h4>>Análisis de atributos=,=p>>La estructura de los datasets a utilizar tiene 78 predictores más la variable objetivo numérica continua en el caso del de train. Esta se querrá predecir con el modelado a desarrollar, siendo de esta manera un problema supervisado de regresión.=,=p>>De las 78 variables de entrada es muy importante tener en consideración sus diferentes tipos que en este caso son definidos en la descripción de datos para cada uno. De esta manera se cuenta con:=,=p>>- 44 categóricas (23 nominales y 21 ordinales)=,=p>>- 34 numéricas (19 continuas y 15 discretas)=,=p>> Aunque las variables nominales y ordinales son categóricas se diferencian en que las primeras son ordenables mientras que las segundas son simplemente características cuyos valores dificimente son comparables entre ellos en el sentido de que uno se pueda considerar mayor o menor que el otro de por sí.=,=p>>Una mejor explicación se podría dar considerando atributos del problema con ExterQual (ordinal) y Foundation (nominal).=,=p>>- ExterQual: hace referencia a la calidad de los materiales del exterior de la casa y sus valores posibles son Ex (excelente), Gd (bueno), TA (promedio), Fa (justo), Po (pobre) por lo cual es fácil de trasladar a una escala en la cual los primeros valores son mayores a los últimos dado que es indiscutible que en cualquier caso una calidad excelente es mejor que una promedio o una pobre.=,=p>>- Foundation: describe a los cimientos de una casa contando con las clases BrkTil (ladrillos y tejas), CBlock (bloque de cemento), PConc (concreto), Slab (losa), Stone (piedra) y Wood (madera). Dado que en algunos terrenos podría ser mejor contar con cimientos de un tipo y en otros de otro con lo que el orden dependería de circunstancias ajenas a la variable en si dificilmente pueda ser ordenable.=,=br=,=h4>>Análisis de ejemplos=,=p>>En cuanto a la cantidad de ejemplos, hay 1460 datos etiquetados en el train y 1459 sin etiquetar en el test. El caso de estudio se enfocará en trabajar con el dataset de entrenamiento dado que este nos permite medir la performance al desarrollar modelos de predicción comparando los valores obtenidos por los mismos con las etiquetas brindadas. De esta forma todo el análisis que se realizará a continuación refiere a ese conjunto de datos.=,=br=,=h5>>Datos Faltantes=,=p>>Analizando los datos faltantes se puede notar que todos corresponden a la variables continua LotFrontage (28), GarageYrBlt (81) y MasVnrArea (8).=,=img>>7.jpg>>20%>>150 =,=p>>A proósito, es conveniente destacar que en muchas variables categóricas existe la palabra NA como uno de los valores posibles siendo un ejemplo de ellos GarageType en cuyo caso se utiliza para referir a que las propiedades que no tienen garage. Estas observaciones con NA no deben ser confundidas con datos faltantes.=,=br=,=h5>>Correlaciones=,=p>>Es interesante observar la correlación que mantienen tanto cada uno con la variable objetivo como entre sí. =,=p>>Estudiaremos la matriz de correlación generada a partir del coeficiente de Pearson que mide la correlación lineal entre dos variables en una escala entre -1 y 1 interpretándose como cuanto más cerca del cero menor dependencia lineal y cuanto más lejos mayor ya sea directa en el caso de la aproximación al 1 u opuesta en los acercamientos al -1. Este coeficiente solo puede ser calculado para variables numéricas o en su defecto, como se hará a continuación, se pueden agregar también a las ordinales tras convertirlas a una escala numérica. =,=p>>Filtrando entre los atributos que tienen una correlación mayor a 0.5 o menor a -0.5 respecto a la variable objetivo según Pearson se obtienen de forma ordenada los siguientes 13 predictores.=,=img>>ames-1.jpg>>200>>250=,=br=,=br=,=p>>Realizando los cálculos para las correlaciones con la variable objetivo con 55 predictores entre los numéricos y los ordinales tras haber sido codificados para que también lo sean, solamente 13 muestran una predicción superior a 0.5 con esta lo cual es un 23%. Además, estas realaciones todas son de forma directa y ninguna opuesta.=,=p>>A continuación se calculan las correlaciones lineales entre los predictores resultando esta matriz de Pearson.=,=img>>ames-2.jpg>>100%>>400=,=br=,=br=,=p>>En cuanto a las correlaciones entre los propios predictores se obtuvieron valores positivos en todos los casos con lo cual todos tienen una cierta dependencia lineal directa por más mínima que pueda ser. Se consideraron a los valores superiores a >0.66 como correlaciones altas lo cual se da entre los pares GrLivArea y TotRmsAvgGrd con 0.82, GarageArea y GarageCars con 0.88, TotalBsmtSF y 1stFlrSF con 0.81.=,=br=,=h5>>Outliers=,=p>>Estudiando las distribuciones de los atributos que tienen alta correlación con la variable a predecir y baja correlación entre sí se pueden observar outliers para GrLivArea en sus valores sobre 4000 (esto es expresado por el propio Dean De Cock en su paper sobre el problema) y en TotalBsmtSF por sobre 3000 como se muestra en las representaciones a continuación.=,=img>>ames-3.jpg>>400>>300=,=img>>ames-4.jpg>>400>>300=,=br=,=br=,=br=,=h5>>Sesgos=,=p>>Visualizando los histogramas se pudieron ver sesgos hacia la derecha en YearBuilt y YearRemodAdd los cuales corresponden respectivamente al año de construcción y al de la última remodelación de las propiedades. Se puede ver que en el registro hay bastantes más datos de casas hechas o remodeladas hace pocos años con un 40% y un 59% de la muestra con año posterior a 1980 en cada caso.=,=img>>ames-5.jpg>>400>>300=,=img>>ames-6.jpg>>400>>300=,=br=,=br=,=br=,=h5>>Distribuciones de atributos nominales=,=p>>Al realizar el análisis de las variables nominales es útil observar sus distintas categorías en relación a la variable objetivo lo cual es posible con diagramas de caja que nos dejan visualizar cuanta variabilidad hay en el dataset entre las clases de una categoría nominal respecto a la variable objetivo con el rango, la mediana y los datos atípicos que tienen. Si la variabilidad es mucha entonces es una buena variable para aprender de ella.=,=p>>Los diagramas más destacables se encuentran posteriormente. =,=p>>-Neighborhood=,=img>>ames-13.jpg>>600>>400=,=br=,=br=,=p>>-HouseStyle=,=img>>ames-14.jpg>>500>>300 =,=br=,=br=,=p>>-Foundation=,=img>>ames-8.jpg>>500>>300=,=br=,=br=,=h2>>Preparación de datos=,=hr=,=p>>=,=h5>>Conjuntos de entrenamiento generados=,=p>>Se armaron 3 conjuntos de datos para entrenamiento distintas características para aplicar modelos sobre ellos y poder realizar comparaciones.=,=p>>- Datos de entrenamiento 1: Contiene todas los predictores del dataset con las variables ordinales y nominales codificadas y sin datos faltantes.=,=p>>- Datos de entrenamiento 2: Con atributos seleccionados como se explicará a continuación, variables ordinales y nominales codificadas, pero sin que se les haya realizado más pre procesamiento. Su objetivo es probar el feature selection hecho.=,=p>>- Datos de entrenamiento 3: Comprende los mismos atributos seleccionados para el caso anterior, variables ordinales y nominales codificadas pero además sobre este se realizaron otras técnicas de pre procesamiento de datos mostradas debajo. Su finalidad es probar los métodos de pre processing aplicados. =,=br=,=h5>>Carga de datos=,=p>>Los datos fueron cargados y se reemplazaron los valores con NA de las variables categóricas por una palabra que significa que no hay eso de lo que trata el predictor lo cual puede tratarse de garages, sótanos, etc.=,=br=,=h5>>Codificación atributos ordinales (para datos de entrenamiento 1, 2 y 3)=,=p>>Los atributos ordinales se codificaron transformando sus escalas a discretas con el OrdinalEncoder() de SKLearn, lo importante aquí es mantener el ordenamiento y que no pase como en Python que codifica al revés y luego al calcular las correlaciones por ejemplo con la variable objetivo se obtiene el valor opuesto al que debería pero igualmente esto puede arreglarse fácil como se muestra en el Anexo.=,=br=,=h5>>Selección de atributos numéricos y ordinales (para datos de entrenamiento 2 y 3)=,=p>>Nos quedamos con los predictores que mantuvieran una correlación superior a 0.5 o inferior a -0.5 respecto al atributo objetivo y  una correlación con las demás que sea menor a 0.66 o mayor a -0.66 en todos los casos. Obteniendo las siguientes variables.=,=img>>vars.jpg>>100%>>40=,=br=,=br=,=br=,=h5>>Selección de atributos nominales (para datos de entrenamiento 2 y 3) =,=p>>Se observaron diagramas diagramas de caja analizando cuales brindan la mayor variabilidad respecto a la variable objectivo. De esto se consiguieron los predictores HouseStyle, Foundation, CentralAir y Neighborhood. Los demás se descartaron por por no poder justificar que tengan una variabilidad considerable entre sus distintas clases respecto a la variable objetivo que sea determinante para el problema.=,=br=,=h5>>Codificación de atributos nominales (para datos de entrenamiento 1, 2 y 3)=,=p>>Los atributos nominales fueron codificados con one hot encoding que se encuentra en SKLearn y en Pandas. Lo que hace es generar dummy variables que no son otra cosa que nuevos atributos binarios por cada uno de los diferentes valores que pueden serguir las variables nominales seleccionadas lo cual hace que las variable original no pierda su naturaleza nominal de no tener un orden definido.=,=br=,=h5>>Remoción de outliers (para datos de entrenamiento 3)=,=p>>Tras haber seleccionado los atributos con los que se trabajará y fundamentándonos en el análisis hecho con antelación se prosigió a eliminar outliers quitando los valores de GrLivArea<4000 y TotalBsmtSF>3000.=,=h6>>GrLivArea=,=img>>con outliers 1.JPG>>50%>>250=,=img>>sin outliers 1.jpg>>50%>>250=,=br=,=br =,=h6>>TotalBsmtSF=,=img>>con outliers 2.jpg>>50%>>250=,=img>>sin outliers 2.jpg>>50%>>250=,=br=,=br=,=h5>>Tratamiento del sesgo (para datos de entrenamiento 3)=,=p>>Se trató el sesgo en los atributos YearBuilt y YearRemodAdd con transformaciones logarítmicas.=,=h6>>YearBuilt=,=img>>yb sin log.jpg>>50%>>250=,=img>>yb con log.jpg>>50%>>250=,=br=,=br=,=h6>>YearRemodAdd=,=img>>yra sin log.jpg>>50%>>250=,=img>>yra con log.jpg>>50%>>250=,=br=,=br=,=br=,=h5>>Datos faltantes (para datos de entrenamiento 1)=,=p>>Removimos las filas que contenían datos faltantes para el caso 1. Los conjuntos 2 y 3 no tenían missing values.=,=img>>1antes.jpg>>25%>>150=,=img>>1despues.jpg>>25%>>150=,=br=,=br=,=br=,=h5>>Estandarización (para datos de entrenamiento 3)=,=p>> Se estandarizaron las variables numéricas y ordinales para que sus escalas no sean determinantes.=,=h2>>Modelado=,=hr=,=p>>En la decisión de los modelos a desarrollar es fundamental considerar el tipo de problema el cual en este caso consiste en predecir el valor de una propiedad a partir de un conjunto de predictores y por lo tanto se trata de uno supervisado y de regresión. Existen un conjunto de algoritmos que se pueden emplear en estas condiciones entre los cuales se encuentran regresión lineal considerando también sus variaciones Lasso y Ridge, árboles de decisión, random forests, gradient boosting regression, k-nearest neighbors, support vector regression, entre otros. Para decidir entre los modelos aplicables es necesario ver la estructura de los datos que tenemos en este problema en particular.=,=p>>Como se estudió en la sección de análisis de datos con los coeficientes de Pearson y los diagramas, los predictores mantienen una relación lineal con la salida lo cual se acentúa para los atributos seleccionados y trabajados en el data pre processing. Como dicho algoritmo asume una relación lineal entre las entradas y la salida, aplicar regresión lineal para este caso es una buena idea por lo que será empleada a continuación. Además, aplicaremos el algoritmo de ensamble Random Forest para hacer una comparación entre ambos.  =,=p>>Los modelos seleccionados será entrenado con los 3 conjuntos de datos de entrenamiento. Para estos se utilizará cross validation con 10 pliegos de forma tal de evitar el problema en el que la parte de testeo pueda llegar a ser más fácil o más dificil para algún modelo en específico como podría pasar con la técnica hold out.=,=br=,=h2>>Resultados=,=hr=,=p>>Para la medición de resultados se usarán dos coeficientes, el primero es RMSE o root mean squared error que mide la raíz cudrada del promedio de los cuadrados de las diferencias entre las predicciones y los datos etiquetados del dataset train al cuadrado por lo que cuanto menor mejor.=,=p>>Se consiguieron las siguientes cifras.=,=h5>>Regresión lineal=,=p>>- Datos de entrenamiento 1: RMSE = 30850=,=p>>- Datos de entrenamiento 2: RMSE = 25744=,=p>>- Datos de entrenamiento 3: RMSE = 24911=,=br=,=h5>>Random Forest=,=p>>Con 1000 árboles y máximo de atributos a considerar en los splits de 20.=,=p>>- Datos de entrenamiento 1: RMSE = 23755=,=p>>- Datos de entrenamiento 2: RMSE = 27291=,=p>>- Datos de entrenamiento 3: RMSE = 27671=,=br=,=h2>>Conclusiones=,=hr=,=p>>=,=p>>En los resultados se puede visualizar que seleccionar atributos tanto ordinales como nominales cuidadosamente y aplicar otros métodos de pre procesamiento de datos como estandarización, remoción de outliers y transformaciones merece la pena para el caso de la regresión lineal dado que esto optimizó el parámetro utilizados para medir la performance lo cual se puede ver tanto del caso 1 al 2 como del 2 al 3.=,=p>>En cuanto a Random Forest se observa que da mejores resultados con los atributos originales sin realizar seleccion de atributos ni otros métodos de procesamiento de datos. Esto puede deberse a que con estas modificaciones se pierda parte de la buena representación del problema el cual es el único requerimiento exigido por este algoritmo en cuanto a la preparación de datos.=,=p>>Con lo anterior se comprueba en un caso practico que lo que puede ser una buena preparación de datos para un algoritmo de machine learning puede ser mala para otro. De esta forma, para obtener una mejor performance para un problema no solo basta con aplicar distintos algoritmos a un mismo conjunto de prueba preparado y observar cual da mejor sino que también es necesario probar diferentes preparaciones de datos a ser aplicadas a los distintos algoritmos de machine learning de forma tal de sacarles el máximo provecho.=,=p>>Por otra parte, resulta interesante que a partir del desarrollo realizado tanto en el ánálisis de datos por ejemplo al ver la alta correlación que hay entre el tamaño disponible y el precio de la propiedad como en la preparación de datos y modelado siendo un caso conreto la selección de ciertas variables nominales que mejoran mucho el rendimiento de los modelos como es el caso del barrio, se logra ver como afectan en un caso real las características de las propiedades que los expertos mencionan como las más importantes lo que se analizó en un principio en la sección de contexto. Se puede ver como al incluir y procesar correctamente esos factores que ellos más destacan es lo que más termina determinando que al armar nuestros modelos estos hagan mejores predicciones.=,=br=,=h2>>Anexo=,=hr=,=iframe>>"
    },
    {
        "unidad":"Caso",
        "titulo":"Customer Segmentation",
        "descripcion":"",
        "contenido":"img>>customer seg-0.jpg>>100%>>550=,=br=,=hr=,=br=,=h2>>Contexto=,=hr=,=p>>Para cualquier empresa resulta fundamental distinguir cómo son sus clientes de forma tal de que les permita generar estrategias para ciertos públicos en particular como sacar una nueva línea de productos pensando en cierto sector o aprovecharse de poder hacer uso del marketing por microsegmentos que se basa en conocer distintos tipos de clientes y desarrollar campañas específicas para cada uno de estos sectores que muchas veces están motivados a consumir por razones distintas, entre otras razones.=,=p>>Más allá de que una estrategia o campaña sea buena o mala, con alto o bajo presupuesto, resulta impresindible que esté dirigida al público correcto y para ello los datos y su análisis con herramientas de machine learning no supervisado como clustering nos pueden ser de gran ayuda al permitirnos distinguir subgrupos en un conjunto de datos que bien pueden corresponder a los diferentes clases de clientes de una compañía.=,=br=,=h2>>Entendimiento de negocio=,=hr=,=p>>En este caso de estudio se desarrollará el modelo CRISP-DM sobre el problema no supervisado de segmentación de los clientes de un shopping con fines académicos sin tener como fin un deployment o que un producto más grande. Se realizará preparación de datos, modelado y evaluación para encontrar clusters o subgrupos a partir de valores acerca de diferentes cualidades de los compradores ayudándonos de la herramienta de minería de datos RapidMiner.=,=br=,=h2>>Conocimiento de datos=,=hr=,=p>>En este caso de estudio se posee un conjunto de datos obtenidos de Kaggle que serán estudiados con la herramienta RapidMiner. Este cuenta con 200 ejemplos acerca de características de los clientes de una shopping representadas por los atributos dispuestos a continuación junto a sus descripciones e histogramas.=,=p>>- CustomerID (numérico): Simplemente un identificador único del cliente.=,=p>>- Gender (nominal): Si el cliente es masculino o femenino. La proporción es de 56% de mujeres frente a 44% de hombres.=,=img>>customer seg-1.jpg>>35%>>300=,=br=,=br=,=p>>- Age (numérico): Representa la edad del cliente La proprción. El grupo etario que contiene más individuos se sitúa desde en la década de los 26 a los 35 años.=,=img>>customer seg-2.jpg>>70%>>450=,=br=,=br=,=p>>- Annual Income (numérico): Corresponde al salario anual del cliente en miles de dólares. Los datos más repetidos se encuentran en el centro de la distribución yendo desde los 50 hasta los 90 mil dólares al año.=,=img>>customer seg-3.jpg>>70%>>450=,=br=,=br=,=p>>- Spending Score (numérico): Puntaje vinculado al consumo del cliente. Tiene un rango entre (0,100) y los valores más repetidos se encuentran en el centro de la distribución.=,=img>>customer seg-4.jpg>>90%>>450=,=br=,=br=,=p>>*Observación: Entre los datos no se hallan valores faltantes ni outliers.=,=h6>>Descargas:=,=a>>Datos>>Mall_Customers.csv>>d=,=br=,=br=,=h2>>Preparación de datos=,=hr=,=p>>Para algoritmos de clustering como k-means es importante verificar que no hay valores faltantes dado que no los soporta, eliminar outliers por ser muy sensible a ellos, manejar una baja dimensionalidad, solamente utilizar variables numéricas porque medir las distancias que terminan determinando las agrupaciones es más óptimo de esta manera y normalizar para que las escalas de los atributos no sean determinantes en los cálculos.=,=p>>De esta forma quitamos el parámetro CustomerID al no ser de interés para la formación de agrupaciones considerando que es diferente para cada fila, cambiamos el atributo Gender de categórico a numérico con un dummy encoding y quitamos outliers en base a distancias euclideanas. Respecto a los valores faltantes no se realizó ninguna técnica de procesamiento al no ser necesario para este caso particular y no se aplicó estandarización debido a que los predictores numéricos tienen unidades que utilizan rangos rangos similares.=,=br=,=h2>>Modelado=,=hr=,=p>>Empleamos k-means en RapidMiner cambiando la cantidad de clusteres comprendiendo los casos de k=2, k=3, k=4 y k=5 pero manteniendo siempre el mismo número de iteraciones del algoritmo con max runs = 10. Para cada caso se analizó la interpretabilidad del resultado obtenido con scatter plots y la performance según la distancia promedio de los puntos respecto a los centroides de los clusters.=,=h6>>Proceso en RapidMiner=,=img>>customer seg-5.jpg>>100%>>600=,=a>>Descargar proceso>>Customer Segmentation.rmp>>d=,=br=,=br=,=h2>>Resultados=,=hr=,=p>>Observando las gráficas resultantes de los modelos creados se llegó a que las mejores visualizaciones para darle significado a los clusters se obtenían de Spending Score contra Annual Income, Spending Score en función de Age y gráficos de barra de los clusters con colores para diferenciar a Gender.=,=h6>>K=2=,=p>>Avg. within centroid distance: -0.193=,=img>>customer seg-6.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-7.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-8.jpg>>75%>>400=,=br=,=br=,=h6>>K=3=,=p>>Avg. within centroid distance: -0.156=,=img>>customer seg-9.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-10.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-11.jpg>>75%>>400=,=br=,=br=,=h6>>K=4=,=p>>Avg. within centroid distance: -0.119=,=img>>customer seg-12.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-13.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-14.jpg>>75%>>400=,=br=,=br=,=h6>>K=5=,=p>>Avg. within centroid distance: -0.102=,=img>>customer seg-15.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-16.jpg>>75%>>400=,=br=,=br=,=img>>customer seg-17.jpg>>75%>>400=,=br=,=br=,=h6>>Gráfica de Avg. within centroid distance en relación con k=,=img>>customer seg-18.jpg>>75%>>400=,=br=,=br=,=h2>>Conclusiones=,=hr=,=p>>Observado los resultados se puede llegar a que los clusters se van diferenciando visualmente más y más a medida que se aumenta el valor para k hasta llegar a 5. Si uno continúa aumentando esta cifra se empieza a complicar distinguir las agrupaciones según características de los clientes. Igualmente con k=5 para todos los clusteres hay gran cantidad de registros de género masculino y femenino sin diferenciarse largamente entre sí lo que podría mostrar que no existen enormes diferencias en el comportamiento de compra entre ambos casos.=,=p>> Con k=5 a partir de las tres gráficas del caso de pueden distinguir cualidades de cada una de las agrapaciones que aparecen:=,=p>>- Cluster 0 (naranja): Clientes de ingresos medios (40-70), todas las edades (18-70) y un puntaje de gasto medio (40-60).=,=p>>- Cluster 1 (negro): Clientes de altos ingresos (70-110), jóvenes (25-40) y de puntaje de gasto alto (60-100).=,=p>>- Cluster 2 (azul): Clientes de bajos ingresos (15-40), jóvenes (18-35) y de puntaje de gasto alto (60-100).=,=p>>- Cluster 3 (violeta): Clientes de altos ingresos (70-105), mediana edad (30-60) y puntaje de gasto bajo (0-40).=,=p>>- Cluster 4 (verde): Clientes de ingresos bajos (15-40), mediana edad (30-60) y puntaje de gasto bajo (0-40).=,=p>>Por otra parte, se puede ver como disminuye el promedio de distancias de los puntos de los clusteres respecto a sus centroides al subir el valor de k hasta llegar a 5 generándose un comportamiento asintótico que se manifiesta en la última gráfica."
    }
]